Using randaugment for data augmentation
Using cuda device
Epoch 1
-------------------------------
Training loss: 4.155856  [16/14121]
Training loss: 3.435559  [1616/14121]
Training loss: 3.228292  [3216/14121]
Training loss: 3.095074  [4816/14121]
Training loss: 2.648761  [6416/14121]
Training loss: 2.281395  [8016/14121]
Training loss: 2.651944  [9616/14121]
Training loss: 2.086168  [11216/14121]
Training loss: 2.033525  [12816/14121]
Training accuracy: 76.96 %
Validation loss: 2.291512
Validation accuracy: 55.35% 

Epoch 2
-------------------------------
Training loss: 1.911680  [16/14121]
Training loss: 1.649617  [1616/14121]
Training loss: 1.608183  [3216/14121]
Training loss: 1.690046  [4816/14121]
Training loss: 1.761928  [6416/14121]
Training loss: 1.204835  [8016/14121]
Training loss: 1.150776  [9616/14121]
Training loss: 1.264329  [11216/14121]
Training loss: 1.294193  [12816/14121]
Training accuracy: 88.01 %
Validation loss: 1.688891
Validation accuracy: 64.79% 

Epoch 3
-------------------------------
Training loss: 1.153763  [16/14121]
Training loss: 0.822278  [1616/14121]
Training loss: 1.106067  [3216/14121]
Training loss: 0.862865  [4816/14121]
Training loss: 0.980551  [6416/14121]
Training loss: 1.174415  [8016/14121]
Training loss: 0.723084  [9616/14121]
Training loss: 0.898651  [11216/14121]
Training loss: 0.693094  [12816/14121]
Training accuracy: 91.23 %
Validation loss: 1.477903
Validation accuracy: 67.14% 

Epoch 4
-------------------------------
Training loss: 0.752342  [16/14121]
Training loss: 0.935381  [1616/14121]
Training loss: 0.881602  [3216/14121]
Training loss: 0.713450  [4816/14121]
Training loss: 0.704798  [6416/14121]
Training loss: 0.590899  [8016/14121]
Training loss: 0.829212  [9616/14121]
Training loss: 0.943775  [11216/14121]
Training loss: 0.713926  [12816/14121]
Training accuracy: 92.30 %
Validation loss: 1.324332
Validation accuracy: 68.83% 

Epoch 5
-------------------------------
Training loss: 0.549321  [16/14121]
Training loss: 1.204459  [1616/14121]
Training loss: 0.563480  [3216/14121]
Training loss: 0.403752  [4816/14121]
Training loss: 0.821466  [6416/14121]
Training loss: 0.587106  [8016/14121]
Training loss: 0.249912  [9616/14121]
Training loss: 0.401424  [11216/14121]
Training loss: 0.401931  [12816/14121]
Training accuracy: 93.06 %
Validation loss: 1.241602
Validation accuracy: 70.36% 

Epoch 6
-------------------------------
Training loss: 0.577305  [16/14121]
Training loss: 0.292187  [1616/14121]
Training loss: 0.492797  [3216/14121]
Training loss: 0.525898  [4816/14121]
Training loss: 0.367876  [6416/14121]
Training loss: 0.402179  [8016/14121]
Training loss: 0.416932  [9616/14121]
Training loss: 0.284875  [11216/14121]
Training loss: 0.490136  [12816/14121]
Training accuracy: 93.62 %
Validation loss: 1.183256
Validation accuracy: 70.91% 

Epoch 7
-------------------------------
Training loss: 0.412078  [16/14121]
Training loss: 0.469880  [1616/14121]
Training loss: 0.714333  [3216/14121]
Training loss: 0.448941  [4816/14121]
Training loss: 0.721916  [6416/14121]
Training loss: 0.737922  [8016/14121]
Training loss: 0.281158  [9616/14121]
Training loss: 0.381854  [11216/14121]
Training loss: 0.501504  [12816/14121]
Training accuracy: 93.62 %
Validation loss: 1.174040
Validation accuracy: 70.36% 

Epoch 8
-------------------------------
Training loss: 0.257143  [16/14121]
Training loss: 0.374114  [1616/14121]
Training loss: 0.539851  [3216/14121]
Training loss: 0.323150  [4816/14121]
Training loss: 0.307514  [6416/14121]
Training loss: 0.558056  [8016/14121]
Training loss: 0.295941  [9616/14121]
Training loss: 0.547199  [11216/14121]
Training loss: 0.626302  [12816/14121]
Training accuracy: 93.98 %
Validation loss: 1.128619
Validation accuracy: 70.80% 

Epoch 9
-------------------------------
Training loss: 0.413941  [16/14121]
Training loss: 0.641301  [1616/14121]
Training loss: 0.455303  [3216/14121]
Training loss: 0.250947  [4816/14121]
Training loss: 0.237921  [6416/14121]
Training loss: 0.428862  [8016/14121]
Training loss: 0.354847  [9616/14121]
Training loss: 0.268204  [11216/14121]
Training loss: 0.335439  [12816/14121]
Training accuracy: 94.60 %
Validation loss: 1.115853
Validation accuracy: 70.96% 

Epoch 10
-------------------------------
Training loss: 0.525715  [16/14121]
Training loss: 0.561612  [1616/14121]
Training loss: 0.381209  [3216/14121]
Training loss: 0.572965  [4816/14121]
Training loss: 0.330015  [6416/14121]
Training loss: 0.239904  [8016/14121]
Training loss: 0.401594  [9616/14121]
Training loss: 0.591645  [11216/14121]
Training loss: 0.301056  [12816/14121]
Training accuracy: 94.29 %
Validation loss: 1.077015
Validation accuracy: 71.89% 

Epoch 11
-------------------------------
Training loss: 0.196091  [16/14121]
Training loss: 0.176445  [1616/14121]
Training loss: 0.357309  [3216/14121]
Training loss: 0.244926  [4816/14121]
Training loss: 0.150591  [6416/14121]
Training loss: 0.155344  [8016/14121]
Training loss: 0.277767  [9616/14121]
Training loss: 0.260307  [11216/14121]
Training loss: 0.301036  [12816/14121]
Training accuracy: 94.62 %
Validation loss: 1.053036
Validation accuracy: 71.94% 

Epoch 12
-------------------------------
Training loss: 0.257260  [16/14121]
Training loss: 0.295078  [1616/14121]
Training loss: 0.260221  [3216/14121]
Training loss: 0.170782  [4816/14121]
Training loss: 0.299476  [6416/14121]
Training loss: 0.369217  [8016/14121]
Training loss: 0.238190  [9616/14121]
Training loss: 0.234077  [11216/14121]
Training loss: 0.379978  [12816/14121]
Training accuracy: 95.01 %
Validation loss: 1.035602
Validation accuracy: 72.60% 

Epoch 13
-------------------------------
Training loss: 0.277602  [16/14121]
Training loss: 0.316203  [1616/14121]
Training loss: 0.322865  [3216/14121]
Training loss: 0.224748  [4816/14121]
Training loss: 0.241396  [6416/14121]
Training loss: 0.146187  [8016/14121]
Training loss: 0.366784  [9616/14121]
Training loss: 0.308655  [11216/14121]
Training loss: 0.151498  [12816/14121]
Training accuracy: 95.21 %
Validation loss: 1.031912
Validation accuracy: 71.62% 

Epoch 14
-------------------------------
Training loss: 0.381775  [16/14121]
Training loss: 0.166504  [1616/14121]
Training loss: 0.668867  [3216/14121]
Training loss: 0.263635  [4816/14121]
Training loss: 0.346080  [6416/14121]
Training loss: 0.103087  [8016/14121]
Training loss: 0.384713  [9616/14121]
Training loss: 0.152814  [11216/14121]
Training loss: 0.271578  [12816/14121]
Training accuracy: 95.36 %
Validation loss: 1.015184
Validation accuracy: 73.14% 

Epoch 15
-------------------------------
Training loss: 0.222167  [16/14121]
Training loss: 0.149709  [1616/14121]
Training loss: 0.095721  [3216/14121]
Training loss: 0.135505  [4816/14121]
Training loss: 0.211229  [6416/14121]
Training loss: 0.261002  [8016/14121]
Training loss: 0.358481  [9616/14121]
Training loss: 0.291424  [11216/14121]
Training loss: 0.636202  [12816/14121]
Training accuracy: 95.39 %
Validation loss: 1.003169
Validation accuracy: 72.54% 

Epoch 16
-------------------------------
Training loss: 0.339871  [16/14121]
Training loss: 0.172736  [1616/14121]
Training loss: 0.226081  [3216/14121]
Training loss: 0.095618  [4816/14121]
Training loss: 0.285633  [6416/14121]
Training loss: 0.275879  [8016/14121]
Training loss: 0.274469  [9616/14121]
Training loss: 0.189466  [11216/14121]
Training loss: 0.323304  [12816/14121]
Training accuracy: 95.16 %
Validation loss: 0.999775
Validation accuracy: 72.60% 

Epoch 17
-------------------------------
Training loss: 0.123165  [16/14121]
Training loss: 0.173619  [1616/14121]
Training loss: 0.360824  [3216/14121]
Training loss: 0.311452  [4816/14121]
Training loss: 0.227067  [6416/14121]
Training loss: 0.161131  [8016/14121]
Training loss: 0.160308  [9616/14121]
Training loss: 0.344789  [11216/14121]
Training loss: 0.179588  [12816/14121]
Training accuracy: 95.59 %
Validation loss: 1.002772
Validation accuracy: 72.33% 

Epoch 18
-------------------------------
Training loss: 0.148257  [16/14121]
Training loss: 0.602594  [1616/14121]
Training loss: 0.403076  [3216/14121]
Training loss: 0.258365  [4816/14121]
Training loss: 0.110154  [6416/14121]
Training loss: 0.234823  [8016/14121]
Training loss: 0.126971  [9616/14121]
Training loss: 0.367929  [11216/14121]
Training loss: 0.215873  [12816/14121]
Training accuracy: 95.78 %
Validation loss: 0.992687
Validation accuracy: 72.65% 

Epoch 19
-------------------------------
Training loss: 0.256515  [16/14121]
Training loss: 0.156018  [1616/14121]
Training loss: 0.349658  [3216/14121]
Training loss: 0.208394  [4816/14121]
Training loss: 0.251899  [6416/14121]
Training loss: 0.153991  [8016/14121]
Training loss: 0.238465  [9616/14121]
Training loss: 0.447469  [11216/14121]
Training loss: 0.109747  [12816/14121]
Training accuracy: 95.66 %
Validation loss: 1.002237
Validation accuracy: 71.83% 

Early stopping
Done!

Elapsed time: 9581.420265197754 seconds

Current time: 11:42:49
                         precision    recall  f1-score   support

             Abyssinian       0.65      0.65      0.65        49
       American Bulldog       0.69      0.54      0.61        50
  American pitbull terr       0.43      0.72      0.54        50
           Basset hound       0.87      0.68      0.76        50
                 Beagle       0.68      0.78      0.73        50
                 Bengal       0.61      0.82      0.70        50
                 Birman       0.69      0.68      0.69        50
                 Bombay       0.77      0.68      0.72        44
                  Boxer       0.76      0.76      0.76        50
      British Shorthair       0.65      0.48      0.55        50
              Chihuahua       0.64      0.84      0.72        50
           Egyptian Mau       0.86      0.76      0.80        49
 English cocker spaniel       0.93      0.50      0.65        50
         English setter       0.67      0.80      0.73        50
     German shorthaired       0.70      1.00      0.83        50
         Great pyrenees       0.75      0.94      0.83        50
               Havanese       0.65      0.68      0.67        50
          Japanese chin       1.00      0.88      0.94        50
               Keeshond       0.85      1.00      0.92        50
             Leonberger       0.86      0.62      0.72        50
             Maine Coon       0.78      0.64      0.70        50
     Miniature pinscher       0.97      0.68      0.80        50
           Newfoundland       0.69      0.92      0.79        50
                Persian       0.73      0.76      0.75        50
             Pomeranian       0.97      0.68      0.80        50
                    Pug       1.00      0.86      0.92        50
                Ragdoll       0.64      0.58      0.61        50
           Russian blue       0.39      0.72      0.51        50
          Saint bernard       0.94      0.90      0.92        50
                Samoyed       0.87      0.94      0.90        50
       Scottish terrier       0.53      0.96      0.69        50
              Shiba inu       0.84      0.96      0.90        50
                Siamese       0.79      0.76      0.78        50
                 Sphynx       0.00      0.00      0.00        50
Staffordshire bull terr       0.52      0.49      0.51        45
        Wheaten terrier       0.87      0.80      0.83        50
      Yorkshire terrier       0.67      0.16      0.26        50

               accuracy                           0.72      1837
              macro avg       0.73      0.72      0.71      1837
           weighted avg       0.73      0.72      0.71      1837

Test accuracy: 0.7201959716929777
