Not using data augmentation
Using cuda device
Epoch 1
-------------------------------
Training loss: 3.575619  [16/35850]
Training loss: 3.328654  [1616/35850]
Training loss: 2.980582  [3216/35850]
Training loss: 2.710728  [4816/35850]
Training loss: 2.475020  [6416/35850]
Training loss: 2.276710  [8016/35850]
Training loss: 1.854667  [9616/35850]
Training loss: 1.770002  [11216/35850]
Training loss: 1.819863  [12816/35850]
Training loss: 1.462209  [14416/35850]
Training loss: 1.147684  [16016/35850]
Training loss: 1.133352  [17616/35850]
Training loss: 1.379613  [19216/35850]
Training loss: 1.044863  [20816/35850]
Training loss: 1.064121  [22416/35850]
Training loss: 0.959155  [24016/35850]
Training loss: 0.855642  [25616/35850]
Training loss: 0.649632  [27216/35850]
Training loss: 0.664328  [28816/35850]
Training loss: 0.686779  [30416/35850]
Training loss: 0.714223  [32016/35850]
Training loss: 0.568715  [33616/35850]
Training loss: 0.870939  [35216/35850]
Training accuracy: 92.22 %
Validation loss: 1.348018
Validation accuracy: 66.32% 

Epoch 2
-------------------------------
Training loss: 0.590034  [16/35850]
Training loss: 0.411126  [1616/35850]
Training loss: 0.672670  [3216/35850]
Training loss: 0.563562  [4816/35850]
Training loss: 0.761775  [6416/35850]
Training loss: 0.659367  [8016/35850]
Training loss: 0.468684  [9616/35850]
Training loss: 0.634999  [11216/35850]
Training loss: 0.376832  [12816/35850]
Training loss: 0.531277  [14416/35850]
Training loss: 0.369345  [16016/35850]
Training loss: 0.427254  [17616/35850]
Training loss: 0.619497  [19216/35850]
Training loss: 0.625931  [20816/35850]
Training loss: 0.713027  [22416/35850]
Training loss: 0.259827  [24016/35850]
Training loss: 0.253599  [25616/35850]
Training loss: 0.456566  [27216/35850]
Training loss: 0.627218  [28816/35850]
Training loss: 0.380820  [30416/35850]
Training loss: 0.420145  [32016/35850]
Training loss: 0.228081  [33616/35850]
Training loss: 0.442938  [35216/35850]
Training accuracy: 94.47 %
Validation loss: 1.110155
Validation accuracy: 68.01% 

Epoch 3
-------------------------------
Training loss: 0.284915  [16/35850]
Training loss: 0.462240  [1616/35850]
Training loss: 0.354085  [3216/35850]
Training loss: 0.373483  [4816/35850]
Training loss: 0.419135  [6416/35850]
Training loss: 0.395001  [8016/35850]
Training loss: 0.393141  [9616/35850]
Training loss: 0.209259  [11216/35850]
Training loss: 0.230184  [12816/35850]
Training loss: 0.262971  [14416/35850]
Training loss: 0.399457  [16016/35850]
Training loss: 0.412797  [17616/35850]
Training loss: 0.278687  [19216/35850]
Training loss: 0.214634  [20816/35850]
Training loss: 0.209299  [22416/35850]
Training loss: 0.325012  [24016/35850]
Training loss: 0.286400  [25616/35850]
Training loss: 0.330322  [27216/35850]
Training loss: 0.275666  [28816/35850]
Training loss: 0.200712  [30416/35850]
Training loss: 0.328664  [32016/35850]
Training loss: 0.203677  [33616/35850]
Training loss: 0.412119  [35216/35850]
Training accuracy: 95.29 %
Validation loss: 1.081036
Validation accuracy: 66.32% 

Epoch 4
-------------------------------
Training loss: 0.329385  [16/35850]
Training loss: 0.159378  [1616/35850]
Training loss: 0.386109  [3216/35850]
Training loss: 0.540867  [4816/35850]
Training loss: 0.253866  [6416/35850]
Training loss: 0.368133  [8016/35850]
Training loss: 0.338383  [9616/35850]
Training loss: 0.477209  [11216/35850]
Training loss: 0.228500  [12816/35850]
Training loss: 0.285172  [14416/35850]
Training loss: 0.175685  [16016/35850]
Training loss: 0.302275  [17616/35850]
Training loss: 0.273223  [19216/35850]
Training loss: 0.447962  [20816/35850]
Training loss: 0.199244  [22416/35850]
Training loss: 0.219378  [24016/35850]
Training loss: 0.115582  [25616/35850]
Training loss: 0.380632  [27216/35850]
Training loss: 0.176142  [28816/35850]
Training loss: 0.380332  [30416/35850]
Training loss: 0.188715  [32016/35850]
Training loss: 0.258221  [33616/35850]
Training loss: 0.285971  [35216/35850]
Training accuracy: 95.63 %
Validation loss: 1.026758
Validation accuracy: 67.14% 

Epoch 5
-------------------------------
Training loss: 0.126411  [16/35850]
Training loss: 0.142659  [1616/35850]
Training loss: 0.275014  [3216/35850]
Training loss: 0.272915  [4816/35850]
Training loss: 0.193748  [6416/35850]
Training loss: 0.120505  [8016/35850]
Training loss: 0.152176  [9616/35850]
Training loss: 0.502526  [11216/35850]
Training loss: 0.278011  [12816/35850]
Training loss: 0.303686  [14416/35850]
Training loss: 0.107903  [16016/35850]
Training loss: 0.235532  [17616/35850]
Training loss: 0.219194  [19216/35850]
Training loss: 0.138276  [20816/35850]
Training loss: 0.188439  [22416/35850]
Training loss: 0.201201  [24016/35850]
Training loss: 0.106942  [25616/35850]
Training loss: 0.290248  [27216/35850]
Training loss: 0.112080  [28816/35850]
Training loss: 0.419840  [30416/35850]
Training loss: 0.096770  [32016/35850]
Training loss: 0.155137  [33616/35850]
Training loss: 0.359392  [35216/35850]
Training accuracy: 95.91 %
Validation loss: 0.943884
Validation accuracy: 69.98% 

Epoch 6
-------------------------------
Training loss: 0.245878  [16/35850]
Training loss: 0.161853  [1616/35850]
Training loss: 0.161197  [3216/35850]
Training loss: 0.110148  [4816/35850]
Training loss: 0.284715  [6416/35850]
Training loss: 0.143498  [8016/35850]
Training loss: 0.149698  [9616/35850]
Training loss: 0.211379  [11216/35850]
Training loss: 0.156351  [12816/35850]
Training loss: 0.071096  [14416/35850]
Training loss: 0.200248  [16016/35850]
Training loss: 0.117713  [17616/35850]
Training loss: 0.139048  [19216/35850]
Training loss: 0.244191  [20816/35850]
Training loss: 0.315091  [22416/35850]
Training loss: 0.186456  [24016/35850]
Training loss: 0.243332  [25616/35850]
Training loss: 0.160069  [27216/35850]
Training loss: 0.471199  [28816/35850]
Training loss: 0.217124  [30416/35850]
Training loss: 0.151841  [32016/35850]
Training loss: 0.168218  [33616/35850]
Training loss: 0.304531  [35216/35850]
Training accuracy: 96.07 %
Validation loss: 0.975912
Validation accuracy: 69.21% 

Epoch 7
-------------------------------
Training loss: 0.198045  [16/35850]
Training loss: 0.140387  [1616/35850]
Training loss: 0.069448  [3216/35850]
Training loss: 0.250864  [4816/35850]
Training loss: 0.078308  [6416/35850]
Training loss: 0.366007  [8016/35850]
Training loss: 0.152984  [9616/35850]
Training loss: 0.093183  [11216/35850]
Training loss: 0.147508  [12816/35850]
Training loss: 0.190176  [14416/35850]
Training loss: 0.145254  [16016/35850]
Training loss: 0.131234  [17616/35850]
Training loss: 0.150054  [19216/35850]
Training loss: 0.165593  [20816/35850]
Training loss: 0.039876  [22416/35850]
Training loss: 0.239623  [24016/35850]
Training loss: 0.238717  [25616/35850]
Training loss: 0.095377  [27216/35850]
Training loss: 0.138634  [28816/35850]
Training loss: 0.227341  [30416/35850]
Training loss: 0.322713  [32016/35850]
Training loss: 0.302337  [33616/35850]
Training loss: 0.077103  [35216/35850]
Training accuracy: 96.30 %
Validation loss: 1.021179
Validation accuracy: 67.90% 

Epoch 8
-------------------------------
Training loss: 0.103914  [16/35850]
Training loss: 0.129470  [1616/35850]
Training loss: 0.140637  [3216/35850]
Training loss: 0.247042  [4816/35850]
Training loss: 0.079001  [6416/35850]
Training loss: 0.131934  [8016/35850]
Training loss: 0.173591  [9616/35850]
Training loss: 0.270628  [11216/35850]
Training loss: 0.165588  [12816/35850]
Training loss: 0.172747  [14416/35850]
Training loss: 0.120905  [16016/35850]
Training loss: 0.050439  [17616/35850]
Training loss: 0.278221  [19216/35850]
Training loss: 0.109317  [20816/35850]
Training loss: 0.103108  [22416/35850]
Training loss: 0.091234  [24016/35850]
Training loss: 0.264980  [25616/35850]
Training loss: 0.112743  [27216/35850]
Training loss: 0.164150  [28816/35850]
Training loss: 0.597243  [30416/35850]
Training loss: 0.263427  [32016/35850]
Training loss: 0.123849  [33616/35850]
Training loss: 0.196325  [35216/35850]
Training accuracy: 96.48 %
Validation loss: 0.955608
Validation accuracy: 69.71% 

Epoch 9
-------------------------------
Training loss: 0.206885  [16/35850]
Training loss: 0.224437  [1616/35850]
Training loss: 0.146725  [3216/35850]
Training loss: 0.119918  [4816/35850]
Training loss: 0.167873  [6416/35850]
Training loss: 0.062069  [8016/35850]
Training loss: 0.114766  [9616/35850]
Training loss: 0.208314  [11216/35850]
Training loss: 0.357090  [12816/35850]
Training loss: 0.155092  [14416/35850]
Training loss: 0.251343  [16016/35850]
Training loss: 0.084676  [17616/35850]
Training loss: 0.329155  [19216/35850]
Training loss: 0.058701  [20816/35850]
Training loss: 0.114020  [22416/35850]
Training loss: 0.266855  [24016/35850]
Training loss: 0.208005  [25616/35850]
Training loss: 0.088501  [27216/35850]
Training loss: 0.128842  [28816/35850]
Training loss: 0.237788  [30416/35850]
Training loss: 0.147889  [32016/35850]
Training loss: 0.145821  [33616/35850]
Training loss: 0.102206  [35216/35850]
Training accuracy: 96.55 %
Validation loss: 0.960437
Validation accuracy: 68.83% 

Epoch 10
-------------------------------
Training loss: 0.067068  [16/35850]
Training loss: 0.067085  [1616/35850]
Training loss: 0.191842  [3216/35850]
Training loss: 0.052341  [4816/35850]
Training loss: 0.362251  [6416/35850]
Training loss: 0.051862  [8016/35850]
Training loss: 0.069732  [9616/35850]
Training loss: 0.055672  [11216/35850]
Training loss: 0.091220  [12816/35850]
Training loss: 0.204298  [14416/35850]
Training loss: 0.042580  [16016/35850]
Training loss: 0.081036  [17616/35850]
Training loss: 0.115401  [19216/35850]
Training loss: 0.163847  [20816/35850]
Training loss: 0.276226  [22416/35850]
Training loss: 0.196571  [24016/35850]
Training loss: 0.062726  [25616/35850]
Training loss: 0.183907  [27216/35850]
Training loss: 0.074522  [28816/35850]
Training loss: 0.170823  [30416/35850]
Training loss: 0.147502  [32016/35850]
Training loss: 0.132616  [33616/35850]
Training loss: 0.041301  [35216/35850]
Training accuracy: 96.75 %
Validation loss: 1.010869
Validation accuracy: 67.96% 

Early stopping
Done!

Elapsed time: 8829.682744503021 seconds

Current time: 12:23:09
                         precision    recall  f1-score   support

             Abyssinian       0.56      0.29      0.38        49
       American Bulldog       0.62      0.62      0.62        50
  American pitbull terr       0.50      0.74      0.60        50
           Basset hound       0.84      0.92      0.88        50
                 Beagle       0.95      0.76      0.84        50
                 Bengal       0.80      0.08      0.15        50
                 Birman       0.38      0.64      0.47        50
                 Bombay       0.25      0.93      0.39        44
                  Boxer       1.00      0.44      0.61        50
      British Shorthair       0.77      0.48      0.59        50
              Chihuahua       0.97      0.72      0.83        50
           Egyptian Mau       0.42      0.96      0.58        49
 English cocker spaniel       0.94      0.58      0.72        50
         English setter       0.82      0.82      0.82        50
     German shorthaired       0.73      0.98      0.84        50
         Great pyrenees       0.82      0.98      0.89        50
               Havanese       1.00      0.40      0.57        50
          Japanese chin       1.00      0.80      0.89        50
               Keeshond       0.89      1.00      0.94        50
             Leonberger       0.80      0.82      0.81        50
             Maine Coon       0.83      0.40      0.54        50
     Miniature pinscher       1.00      0.74      0.85        50
           Newfoundland       0.68      1.00      0.81        50
                Persian       0.65      0.74      0.69        50
             Pomeranian       0.97      0.58      0.72        50
                    Pug       1.00      0.90      0.95        50
                Ragdoll       0.23      0.10      0.14        50
           Russian blue       0.38      0.20      0.26        50
          Saint bernard       0.96      0.90      0.93        50
                Samoyed       0.82      0.98      0.89        50
       Scottish terrier       0.93      0.82      0.87        50
              Shiba inu       0.91      0.96      0.93        50
                Siamese       0.72      0.58      0.64        50
                 Sphynx       0.72      0.36      0.48        50
Staffordshire bull terr       0.62      0.64      0.63        45
        Wheaten terrier       0.47      0.94      0.63        50
      Yorkshire terrier       1.00      0.62      0.77        50

               accuracy                           0.69      1837
              macro avg       0.76      0.69      0.68      1837
           weighted avg       0.76      0.69      0.68      1837

Test accuracy: 0.6864452912357104
