60600
15150
25250
Not using data augmentation
Using cuda device
Epoch 1
-------------------------------
Training loss: 5.174510  [16/60600]
Training loss: 4.914176  [1616/60600]
Training loss: 4.785415  [3216/60600]
Training loss: 4.703840  [4816/60600]
Training loss: 4.707553  [6416/60600]
Training loss: 4.607775  [8016/60600]
Training loss: 4.513013  [9616/60600]
Training loss: 4.397749  [11216/60600]
Training loss: 4.466791  [12816/60600]
Training loss: 4.512601  [14416/60600]
Training loss: 4.318719  [16016/60600]
Training loss: 4.378403  [17616/60600]
Training loss: 4.512969  [19216/60600]
Training loss: 4.376031  [20816/60600]
Training loss: 4.365955  [22416/60600]
Training loss: 4.431485  [24016/60600]
Training loss: 4.119700  [25616/60600]
Training loss: 4.396336  [27216/60600]
Training loss: 4.464616  [28816/60600]
Training loss: 4.214692  [30416/60600]
Training loss: 4.167368  [32016/60600]
Training loss: 4.160243  [33616/60600]
Training loss: 4.177888  [35216/60600]
Training loss: 4.350499  [36816/60600]
Training loss: 4.321694  [38416/60600]
Training loss: 4.180914  [40016/60600]
Training loss: 4.036088  [41616/60600]
Training loss: 3.947292  [43216/60600]
Training loss: 4.118457  [44816/60600]
Training loss: 3.948426  [46416/60600]
Training loss: 3.844573  [48016/60600]
Training loss: 4.300405  [49616/60600]
Training loss: 4.231440  [51216/60600]
Training loss: 3.832264  [52816/60600]
Training loss: 4.108195  [54416/60600]
Training loss: 3.885499  [56016/60600]
Training loss: 3.854082  [57616/60600]
Training loss: 4.049203  [59216/60600]
Training accuracy: 20.15 %
Validation loss: 3.883207
Validation accuracy: 20.03% 

Epoch 2
-------------------------------
Training loss: 3.936905  [16/60600]
Training loss: 3.823481  [1616/60600]
Training loss: 3.961599  [3216/60600]
Training loss: 3.848033  [4816/60600]
Training loss: 3.633271  [6416/60600]
Training loss: 3.608187  [8016/60600]
Training loss: 3.858229  [9616/60600]
Training loss: 4.091361  [11216/60600]
Training loss: 3.716294  [12816/60600]
Training loss: 4.024452  [14416/60600]
Training loss: 3.786011  [16016/60600]
Training loss: 3.676839  [17616/60600]
Training loss: 3.718691  [19216/60600]
Training loss: 3.902535  [20816/60600]
Training loss: 4.000043  [22416/60600]
Training loss: 3.453985  [24016/60600]
Training loss: 3.704437  [25616/60600]
Training loss: 3.691839  [27216/60600]
Training loss: 3.858613  [28816/60600]
Training loss: 3.613344  [30416/60600]
Training loss: 3.993892  [32016/60600]
Training loss: 3.630400  [33616/60600]
Training loss: 4.241921  [35216/60600]
Training loss: 3.756628  [36816/60600]
Training loss: 3.437813  [38416/60600]
Training loss: 3.296188  [40016/60600]
Training loss: 3.402799  [41616/60600]
Training loss: 3.568202  [43216/60600]
Training loss: 2.984033  [44816/60600]
Training loss: 3.278858  [46416/60600]
Training loss: 3.609379  [48016/60600]
Training loss: 3.625561  [49616/60600]
Training loss: 3.059194  [51216/60600]
Training loss: 2.914547  [52816/60600]
Training loss: 3.526523  [54416/60600]
Training loss: 3.497869  [56016/60600]
Training loss: 3.311581  [57616/60600]
Training loss: 3.206967  [59216/60600]
Training accuracy: 31.49 %
Validation loss: 3.369824
Validation accuracy: 31.47% 

Epoch 3
-------------------------------
Training loss: 3.753310  [16/60600]
Training loss: 3.136016  [1616/60600]
Training loss: 3.923774  [3216/60600]
Training loss: 3.905359  [4816/60600]
Training loss: 3.643580  [6416/60600]
Training loss: 3.611300  [8016/60600]
Training loss: 3.343617  [9616/60600]
Training loss: 3.146785  [11216/60600]
Training loss: 3.258139  [12816/60600]
Training loss: 3.691747  [14416/60600]
Training loss: 3.297509  [16016/60600]
Training loss: 3.597814  [17616/60600]
Training loss: 3.740512  [19216/60600]
Training loss: 3.605797  [20816/60600]
Training loss: 3.344522  [22416/60600]
Training loss: 3.409743  [24016/60600]
Training loss: 3.232787  [25616/60600]
Training loss: 3.103706  [27216/60600]
Training loss: 3.499027  [28816/60600]
Training loss: 3.465386  [30416/60600]
Training loss: 3.789660  [32016/60600]
Training loss: 3.479330  [33616/60600]
Training loss: 3.602642  [35216/60600]
Training loss: 3.250231  [36816/60600]
Training loss: 3.282424  [38416/60600]
Training loss: 3.281032  [40016/60600]
Training loss: 3.156007  [41616/60600]
Training loss: 3.305386  [43216/60600]
Training loss: 2.869370  [44816/60600]
Training loss: 2.904486  [46416/60600]
Training loss: 3.433701  [48016/60600]
Training loss: 3.294877  [49616/60600]
Training loss: 3.124484  [51216/60600]
Training loss: 2.917213  [52816/60600]
Training loss: 3.159640  [54416/60600]
Training loss: 3.127845  [56016/60600]
Training loss: 2.938887  [57616/60600]
Training loss: 3.009518  [59216/60600]
Training accuracy: 36.58 %
Validation loss: 3.045857
Validation accuracy: 36.53% 

Epoch 4
-------------------------------
Training loss: 2.914418  [16/60600]
Training loss: 3.358349  [1616/60600]
Training loss: 2.993308  [3216/60600]
Training loss: 3.385341  [4816/60600]
Training loss: 3.398296  [6416/60600]
Training loss: 3.505266  [8016/60600]
Training loss: 3.207178  [9616/60600]
Training loss: 2.981858  [11216/60600]
Training loss: 2.917782  [12816/60600]
Training loss: 3.241487  [14416/60600]
Training loss: 3.284120  [16016/60600]
Training loss: 3.065283  [17616/60600]
Training loss: 2.869238  [19216/60600]
Training loss: 2.544761  [20816/60600]
Training loss: 2.843983  [22416/60600]
Training loss: 3.546506  [24016/60600]
Training loss: 2.956014  [25616/60600]
Training loss: 3.340483  [27216/60600]
Training loss: 3.008404  [28816/60600]
Training loss: 3.171609  [30416/60600]
Training loss: 3.112725  [32016/60600]
Training loss: 2.423987  [33616/60600]
Training loss: 2.987942  [35216/60600]
Training loss: 3.477865  [36816/60600]
Training loss: 3.470291  [38416/60600]
Training loss: 3.058081  [40016/60600]
Training loss: 2.581427  [41616/60600]
Training loss: 2.866761  [43216/60600]
Training loss: 2.580292  [44816/60600]
Training loss: 3.350931  [46416/60600]
Training loss: 2.780166  [48016/60600]
Training loss: 2.813175  [49616/60600]
Training loss: 2.541515  [51216/60600]
Training loss: 3.411559  [52816/60600]
Training loss: 3.054992  [54416/60600]
Training loss: 2.917853  [56016/60600]
Training loss: 2.807813  [57616/60600]
Training loss: 3.071803  [59216/60600]
Training accuracy: 39.68 %
Validation loss: 2.790309
Validation accuracy: 40.21% 

Epoch 5
-------------------------------
Training loss: 3.058593  [16/60600]
Training loss: 2.733489  [1616/60600]
Training loss: 2.685845  [3216/60600]
Training loss: 3.213758  [4816/60600]
Training loss: 2.760164  [6416/60600]
Training loss: 2.785218  [8016/60600]
Training loss: 2.818475  [9616/60600]
Training loss: 2.817889  [11216/60600]
Training loss: 2.995060  [12816/60600]
Training loss: 3.481197  [14416/60600]
Training loss: 2.765044  [16016/60600]
Training loss: 2.816545  [17616/60600]
Training loss: 2.901628  [19216/60600]
Training loss: 2.608442  [20816/60600]
Training loss: 3.001540  [22416/60600]
Training loss: 2.729391  [24016/60600]
Training loss: 2.695976  [25616/60600]
Training loss: 2.914293  [27216/60600]
Training loss: 2.400219  [28816/60600]
Training loss: 2.814325  [30416/60600]
Training loss: 2.635435  [32016/60600]
Training loss: 3.115102  [33616/60600]
Training loss: 3.274405  [35216/60600]
Training loss: 2.533116  [36816/60600]
Training loss: 3.044447  [38416/60600]
Training loss: 3.218810  [40016/60600]
Training loss: 2.604082  [41616/60600]
Training loss: 2.316413  [43216/60600]
Training loss: 2.355585  [44816/60600]
Training loss: 2.771547  [46416/60600]
Training loss: 3.164409  [48016/60600]
Training loss: 3.314928  [49616/60600]
Training loss: 2.955460  [51216/60600]
Training loss: 2.994093  [52816/60600]
Training loss: 2.492481  [54416/60600]
Training loss: 2.726465  [56016/60600]
Training loss: 2.705104  [57616/60600]
Training loss: 3.064815  [59216/60600]
Training accuracy: 42.21 %
Validation loss: 2.634264
Validation accuracy: 42.15% 

Epoch 6
-------------------------------
Training loss: 2.894029  [16/60600]
Training loss: 2.534886  [1616/60600]
Training loss: 2.987553  [3216/60600]
Training loss: 2.305497  [4816/60600]
Training loss: 3.121412  [6416/60600]
Training loss: 2.887185  [8016/60600]
Training loss: 2.586842  [9616/60600]
Training loss: 2.745086  [11216/60600]
Training loss: 2.627428  [12816/60600]
Training loss: 2.783782  [14416/60600]
Training loss: 2.612289  [16016/60600]
Training loss: 2.961593  [17616/60600]
Training loss: 2.245395  [19216/60600]
Training loss: 2.875039  [20816/60600]
Training loss: 2.754553  [22416/60600]
Training loss: 3.071163  [24016/60600]
Training loss: 2.699638  [25616/60600]
Training loss: 2.979831  [27216/60600]
Training loss: 2.491288  [28816/60600]
Training loss: 2.783631  [30416/60600]
Training loss: 2.763633  [32016/60600]
Training loss: 2.682378  [33616/60600]
Training loss: 1.872413  [35216/60600]
Training loss: 2.373410  [36816/60600]
Training loss: 2.721090  [38416/60600]
Training loss: 1.966598  [40016/60600]
Training loss: 2.676710  [41616/60600]
Training loss: 2.803336  [43216/60600]
Training loss: 3.377563  [44816/60600]
Training loss: 2.934910  [46416/60600]
Training loss: 2.939143  [48016/60600]
Training loss: 2.335660  [49616/60600]
Training loss: 2.314761  [51216/60600]
Training loss: 2.978795  [52816/60600]
Training loss: 2.403585  [54416/60600]
Training loss: 2.025531  [56016/60600]
Training loss: 2.405713  [57616/60600]
Training loss: 2.357508  [59216/60600]
Training accuracy: 43.46 %
Validation loss: 2.517966
Validation accuracy: 43.21% 

Epoch 7
-------------------------------
Training loss: 3.043075  [16/60600]
Training loss: 2.396264  [1616/60600]
Training loss: 3.059385  [3216/60600]
Training loss: 2.587140  [4816/60600]
Training loss: 2.558914  [6416/60600]
Training loss: 2.878798  [8016/60600]
Training loss: 2.793582  [9616/60600]
Training loss: 3.297332  [11216/60600]
Training loss: 2.635216  [12816/60600]
Training loss: 2.580209  [14416/60600]
Training loss: 2.373993  [16016/60600]
Training loss: 3.052638  [17616/60600]
Training loss: 2.667635  [19216/60600]
Training loss: 2.582147  [20816/60600]
Training loss: 2.360312  [22416/60600]
Training loss: 2.237654  [24016/60600]
Training loss: 2.609044  [25616/60600]
Training loss: 1.601433  [27216/60600]
Training loss: 2.162555  [28816/60600]
Training loss: 2.094391  [30416/60600]
Training loss: 3.097921  [32016/60600]
Training loss: 2.510024  [33616/60600]
Training loss: 2.430201  [35216/60600]
Training loss: 2.427909  [36816/60600]
Training loss: 2.290060  [38416/60600]
Training loss: 2.052620  [40016/60600]
Training loss: 2.245099  [41616/60600]
Training loss: 1.757512  [43216/60600]
Training loss: 2.324424  [44816/60600]
Training loss: 2.742658  [46416/60600]
Training loss: 2.245021  [48016/60600]
Training loss: 2.865156  [49616/60600]
Training loss: 3.169933  [51216/60600]
Training loss: 2.598507  [52816/60600]
Training loss: 2.402952  [54416/60600]
Training loss: 3.209723  [56016/60600]
Training loss: 2.445054  [57616/60600]
Training loss: 2.600047  [59216/60600]
Training accuracy: 44.54 %
Validation loss: 2.435927
Validation accuracy: 44.31% 

Epoch 8
-------------------------------
Training loss: 2.995934  [16/60600]
Training loss: 2.187902  [1616/60600]
Training loss: 2.209892  [3216/60600]
Training loss: 2.390963  [4816/60600]
Training loss: 2.267015  [6416/60600]
Training loss: 2.654650  [8016/60600]
Training loss: 2.277417  [9616/60600]
Training loss: 3.055420  [11216/60600]
Training loss: 1.783151  [12816/60600]
Training loss: 2.388091  [14416/60600]
Training loss: 3.289666  [16016/60600]
Training loss: 2.286761  [17616/60600]
Training loss: 2.358807  [19216/60600]
Training loss: 2.628968  [20816/60600]
Training loss: 2.099452  [22416/60600]
Training loss: 2.850074  [24016/60600]
Training loss: 2.052681  [25616/60600]
Training loss: 2.810535  [27216/60600]
Training loss: 2.801154  [28816/60600]
Training loss: 2.862968  [30416/60600]
Training loss: 2.184488  [32016/60600]
Training loss: 2.974047  [33616/60600]
Training loss: 2.236796  [35216/60600]
Training loss: 2.202460  [36816/60600]
Training loss: 1.516077  [38416/60600]
Training loss: 2.569425  [40016/60600]
Training loss: 2.668461  [41616/60600]
Training loss: 2.904844  [43216/60600]
Training loss: 2.283227  [44816/60600]
Training loss: 2.348330  [46416/60600]
Training loss: 2.616775  [48016/60600]
Training loss: 2.508698  [49616/60600]
Training loss: 2.862578  [51216/60600]
Training loss: 2.645111  [52816/60600]
Training loss: 2.699025  [54416/60600]
Training loss: 2.586388  [56016/60600]
Training loss: 2.232152  [57616/60600]
Training loss: 2.659410  [59216/60600]
Training accuracy: 45.46 %
Validation loss: 2.379406
Validation accuracy: 45.64% 

Epoch 9
-------------------------------
Training loss: 1.927600  [16/60600]
Training loss: 1.768927  [1616/60600]
Training loss: 2.531114  [3216/60600]
Training loss: 2.959761  [4816/60600]
Training loss: 2.116693  [6416/60600]
Training loss: 2.227796  [8016/60600]
Training loss: 2.144322  [9616/60600]
Training loss: 1.801189  [11216/60600]
Training loss: 2.654047  [12816/60600]
Training loss: 2.018044  [14416/60600]
Training loss: 2.418234  [16016/60600]
Training loss: 2.173174  [17616/60600]
Training loss: 2.527298  [19216/60600]
Training loss: 2.491852  [20816/60600]
Training loss: 3.084044  [22416/60600]
Training loss: 2.220494  [24016/60600]
Training loss: 1.883758  [25616/60600]
Training loss: 3.259429  [27216/60600]
Training loss: 2.104867  [28816/60600]
Training loss: 2.325384  [30416/60600]
Training loss: 2.433248  [32016/60600]
Training loss: 2.157469  [33616/60600]
Training loss: 1.903324  [35216/60600]
Training loss: 2.345616  [36816/60600]
Training loss: 2.433404  [38416/60600]
Training loss: 2.273073  [40016/60600]
Training loss: 2.522398  [41616/60600]
Training loss: 1.986508  [43216/60600]
Training loss: 2.615894  [44816/60600]
Training loss: 2.764627  [46416/60600]
Training loss: 1.684336  [48016/60600]
Training loss: 1.727432  [49616/60600]
Training loss: 3.194128  [51216/60600]
Training loss: 2.332739  [52816/60600]
Training loss: 2.671612  [54416/60600]
Training loss: 3.008875  [56016/60600]
Training loss: 2.313755  [57616/60600]
Training loss: 3.001178  [59216/60600]
Training accuracy: 46.20 %
Validation loss: 2.306760
Validation accuracy: 46.30% 

Epoch 10
-------------------------------
Training loss: 2.737996  [16/60600]
Training loss: 2.376651  [1616/60600]
Training loss: 2.182808  [3216/60600]
Training loss: 2.038208  [4816/60600]
Training loss: 2.650493  [6416/60600]
Training loss: 2.330666  [8016/60600]
Training loss: 2.191166  [9616/60600]
Training loss: 2.532218  [11216/60600]
Training loss: 2.778329  [12816/60600]
Training loss: 2.182054  [14416/60600]
Training loss: 2.681922  [16016/60600]
Training loss: 2.169603  [17616/60600]
Training loss: 1.948505  [19216/60600]
Training loss: 1.834217  [20816/60600]
Training loss: 1.457713  [22416/60600]
Training loss: 2.915490  [24016/60600]
Training loss: 2.694308  [25616/60600]
Training loss: 3.374248  [27216/60600]
Training loss: 2.655951  [28816/60600]
Training loss: 2.869711  [30416/60600]
Training loss: 2.873678  [32016/60600]
Training loss: 3.006249  [33616/60600]
Training loss: 2.466790  [35216/60600]
Training loss: 2.032849  [36816/60600]
Training loss: 1.880239  [38416/60600]
Training loss: 2.102350  [40016/60600]
Training loss: 2.492727  [41616/60600]
Training loss: 2.068298  [43216/60600]
Training loss: 2.534638  [44816/60600]
Training loss: 1.791448  [46416/60600]
Training loss: 2.239480  [48016/60600]
Training loss: 2.007992  [49616/60600]
Training loss: 1.811473  [51216/60600]
Training loss: 2.784038  [52816/60600]
Training loss: 2.319952  [54416/60600]
Training loss: 2.336195  [56016/60600]
Training loss: 2.822996  [57616/60600]
Training loss: 2.351139  [59216/60600]
Training accuracy: 46.80 %
Validation loss: 2.259893
Validation accuracy: 46.64% 

Epoch 11
-------------------------------
Training loss: 2.356167  [16/60600]
Training loss: 2.276492  [1616/60600]
Training loss: 2.338862  [3216/60600]
Training loss: 2.353919  [4816/60600]
Training loss: 2.370836  [6416/60600]
Training loss: 2.136944  [8016/60600]
Training loss: 2.102855  [9616/60600]
Training loss: 2.263815  [11216/60600]
Training loss: 2.159109  [12816/60600]
Training loss: 2.404623  [14416/60600]
Training loss: 1.560368  [16016/60600]
Training loss: 2.423266  [17616/60600]
Training loss: 2.760621  [19216/60600]
Training loss: 2.055129  [20816/60600]
Training loss: 1.930394  [22416/60600]
Training loss: 2.206210  [24016/60600]
Training loss: 2.771672  [25616/60600]
Training loss: 2.390372  [27216/60600]
Training loss: 2.827882  [28816/60600]
Training loss: 2.745316  [30416/60600]
Training loss: 2.050292  [32016/60600]
Training loss: 2.378905  [33616/60600]
Training loss: 2.695626  [35216/60600]
Training loss: 2.467353  [36816/60600]
Training loss: 2.421812  [38416/60600]
Training loss: 1.588745  [40016/60600]
Training loss: 2.449149  [41616/60600]
Training loss: 2.131756  [43216/60600]
Training loss: 2.302121  [44816/60600]
Training loss: 2.298878  [46416/60600]
Training loss: 2.469559  [48016/60600]
Training loss: 2.583843  [49616/60600]
Training loss: 1.961905  [51216/60600]
Training loss: 3.027945  [52816/60600]
Training loss: 2.295592  [54416/60600]
Training loss: 2.064485  [56016/60600]
Training loss: 1.650079  [57616/60600]
Training loss: 1.729678  [59216/60600]
Training accuracy: 47.34 %
Validation loss: 2.232595
Validation accuracy: 46.99% 

Epoch 12
-------------------------------
Training loss: 3.083663  [16/60600]
Training loss: 2.557566  [1616/60600]
Training loss: 2.340011  [3216/60600]
Training loss: 2.619167  [4816/60600]
Training loss: 2.714998  [6416/60600]
Training loss: 2.836135  [8016/60600]
Training loss: 1.905830  [9616/60600]
Training loss: 2.325144  [11216/60600]
Training loss: 2.268320  [12816/60600]
Training loss: 1.525456  [14416/60600]
Training loss: 1.737158  [16016/60600]
Training loss: 2.882021  [17616/60600]
Training loss: 1.431929  [19216/60600]
Training loss: 1.724369  [20816/60600]
Training loss: 2.946672  [22416/60600]
Training loss: 2.163015  [24016/60600]
Training loss: 2.370291  [25616/60600]
Training loss: 1.265645  [27216/60600]
Training loss: 2.214699  [28816/60600]
Training loss: 2.421883  [30416/60600]
Training loss: 2.248038  [32016/60600]
Training loss: 2.413949  [33616/60600]
Training loss: 2.938682  [35216/60600]
Training loss: 2.681647  [36816/60600]
Training loss: 3.120183  [38416/60600]
Training loss: 2.612863  [40016/60600]
Training loss: 2.888580  [41616/60600]
Training loss: 2.396580  [43216/60600]
Training loss: 2.579467  [44816/60600]
Training loss: 2.404398  [46416/60600]
Training loss: 2.261289  [48016/60600]
Training loss: 1.532814  [49616/60600]
Training loss: 2.989211  [51216/60600]
Training loss: 2.912946  [52816/60600]
Training loss: 2.191216  [54416/60600]
Training loss: 2.075161  [56016/60600]
Training loss: 2.578381  [57616/60600]
Training loss: 2.023994  [59216/60600]
Training accuracy: 47.89 %
Validation loss: 2.192436
Validation accuracy: 47.56% 

Epoch 13
-------------------------------
Training loss: 2.524783  [16/60600]
Training loss: 2.523301  [1616/60600]
Training loss: 2.390718  [3216/60600]
Training loss: 2.319790  [4816/60600]
Training loss: 2.188954  [6416/60600]
Training loss: 2.333483  [8016/60600]
Training loss: 2.039126  [9616/60600]
Training loss: 1.470939  [11216/60600]
Training loss: 1.520233  [12816/60600]
Training loss: 1.882676  [14416/60600]
Training loss: 2.228926  [16016/60600]
Training loss: 2.712461  [17616/60600]
Training loss: 1.947599  [19216/60600]
Training loss: 1.952875  [20816/60600]
Training loss: 1.881570  [22416/60600]
Training loss: 2.053379  [24016/60600]
Training loss: 2.153294  [25616/60600]
Training loss: 2.517344  [27216/60600]
Training loss: 2.294153  [28816/60600]
Training loss: 2.528741  [30416/60600]
Training loss: 2.584441  [32016/60600]
Training loss: 1.775038  [33616/60600]
Training loss: 2.098236  [35216/60600]
Training loss: 2.504071  [36816/60600]
Training loss: 1.983687  [38416/60600]
Training loss: 2.276371  [40016/60600]
Training loss: 2.195991  [41616/60600]
Training loss: 2.009528  [43216/60600]
Training loss: 2.262770  [44816/60600]
Training loss: 2.474133  [46416/60600]
Training loss: 2.376884  [48016/60600]
Training loss: 2.203974  [49616/60600]
Training loss: 2.739754  [51216/60600]
Training loss: 2.824454  [52816/60600]
Training loss: 1.815924  [54416/60600]
Training loss: 2.284970  [56016/60600]
Training loss: 2.025948  [57616/60600]
Training loss: 1.546494  [59216/60600]
Training accuracy: 48.36 %
Validation loss: 2.169851
Validation accuracy: 48.21% 

Epoch 14
-------------------------------
Training loss: 2.586579  [16/60600]
Training loss: 2.941973  [1616/60600]
Training loss: 2.520978  [3216/60600]
Training loss: 1.205577  [4816/60600]
Training loss: 1.792261  [6416/60600]
Training loss: 2.732710  [8016/60600]
Training loss: 2.086826  [9616/60600]
Training loss: 2.022273  [11216/60600]
Training loss: 2.769174  [12816/60600]
Training loss: 2.172211  [14416/60600]
Training loss: 1.956896  [16016/60600]
Training loss: 2.264097  [17616/60600]
Training loss: 2.104540  [19216/60600]
Training loss: 2.052788  [20816/60600]
Training loss: 2.566862  [22416/60600]
Training loss: 2.562089  [24016/60600]
Training loss: 2.224174  [25616/60600]
Training loss: 2.329505  [27216/60600]
Training loss: 2.409657  [28816/60600]
Training loss: 2.073267  [30416/60600]
Training loss: 2.156967  [32016/60600]
Training loss: 2.595801  [33616/60600]
Training loss: 2.498926  [35216/60600]
Training loss: 1.961871  [36816/60600]
Training loss: 2.077578  [38416/60600]
Training loss: 2.820662  [40016/60600]
Training loss: 1.815161  [41616/60600]
Training loss: 2.373539  [43216/60600]
Training loss: 1.934259  [44816/60600]
Training loss: 2.898743  [46416/60600]
Training loss: 3.378647  [48016/60600]
Training loss: 2.172281  [49616/60600]
Training loss: 2.470888  [51216/60600]
Training loss: 2.703270  [52816/60600]
Training loss: 2.043104  [54416/60600]
Training loss: 2.847274  [56016/60600]
Training loss: 2.023189  [57616/60600]
Training loss: 1.804709  [59216/60600]
Training accuracy: 48.85 %
Validation loss: 2.166383
Validation accuracy: 48.09% 

Epoch 15
-------------------------------
Training loss: 2.654174  [16/60600]
Training loss: 1.998077  [1616/60600]
Training loss: 1.625349  [3216/60600]
Training loss: 2.130597  [4816/60600]
Training loss: 2.818331  [6416/60600]
Training loss: 1.893825  [8016/60600]
Training loss: 2.500767  [9616/60600]
Training loss: 2.462703  [11216/60600]
Training loss: 2.418436  [12816/60600]
Training loss: 3.128752  [14416/60600]
Training loss: 2.272097  [16016/60600]
Training loss: 2.298405  [17616/60600]
Training loss: 2.644918  [19216/60600]
Training loss: 1.805368  [20816/60600]
Training loss: 1.946234  [22416/60600]
Training loss: 2.092142  [24016/60600]
Training loss: 2.084005  [25616/60600]
Training loss: 2.491863  [27216/60600]
Training loss: 2.284246  [28816/60600]
Training loss: 2.373807  [30416/60600]
Training loss: 2.824688  [32016/60600]
Training loss: 2.268654  [33616/60600]
Training loss: 2.089681  [35216/60600]
Training loss: 2.322352  [36816/60600]
Training loss: 1.890227  [38416/60600]
Training loss: 2.806367  [40016/60600]
Training loss: 2.827405  [41616/60600]
Training loss: 2.279894  [43216/60600]
Training loss: 1.907742  [44816/60600]
Training loss: 2.167886  [46416/60600]
Training loss: 1.507566  [48016/60600]
Training loss: 2.101721  [49616/60600]
Training loss: 2.368038  [51216/60600]
Training loss: 2.025315  [52816/60600]
Training loss: 2.003763  [54416/60600]
Training loss: 2.373597  [56016/60600]
Training loss: 2.350928  [57616/60600]
Training loss: 2.376896  [59216/60600]
Training accuracy: 49.15 %
Validation loss: 2.133355
Validation accuracy: 48.69% 

Epoch 16
-------------------------------
Training loss: 1.754228  [16/60600]
Training loss: 1.733686  [1616/60600]
Training loss: 1.672377  [3216/60600]
Training loss: 2.356884  [4816/60600]
Training loss: 1.699058  [6416/60600]
Training loss: 2.031422  [8016/60600]
Training loss: 2.092383  [9616/60600]
Training loss: 1.953767  [11216/60600]
Training loss: 1.960784  [12816/60600]
Training loss: 1.558182  [14416/60600]
Training loss: 2.577698  [16016/60600]
Training loss: 2.056760  [17616/60600]
Training loss: 2.319999  [19216/60600]
Training loss: 2.531667  [20816/60600]
Training loss: 1.848383  [22416/60600]
Training loss: 2.138004  [24016/60600]
Training loss: 1.328179  [25616/60600]
Training loss: 2.384630  [27216/60600]
Training loss: 1.818686  [28816/60600]
Training loss: 1.696000  [30416/60600]
Training loss: 1.970272  [32016/60600]
Training loss: 1.251050  [33616/60600]
Training loss: 2.246470  [35216/60600]
Training loss: 2.217969  [36816/60600]
Training loss: 2.328275  [38416/60600]
Training loss: 3.090497  [40016/60600]
Training loss: 1.609350  [41616/60600]
Training loss: 2.458593  [43216/60600]
Training loss: 2.338835  [44816/60600]
Training loss: 2.375121  [46416/60600]
Training loss: 1.889723  [48016/60600]
Training loss: 2.673792  [49616/60600]
Training loss: 2.739179  [51216/60600]
Training loss: 1.789413  [52816/60600]
Training loss: 1.582980  [54416/60600]
Training loss: 2.656372  [56016/60600]
Training loss: 1.886789  [57616/60600]
Training loss: 2.177014  [59216/60600]
Training accuracy: 49.49 %
Validation loss: 2.107167
Validation accuracy: 49.37% 

Epoch 17
-------------------------------
Training loss: 1.918449  [16/60600]
Training loss: 1.579246  [1616/60600]
Training loss: 2.535113  [3216/60600]
Training loss: 1.875031  [4816/60600]
Training loss: 2.039317  [6416/60600]
Training loss: 2.006146  [8016/60600]
Training loss: 2.160443  [9616/60600]
Training loss: 1.645100  [11216/60600]
Training loss: 2.002807  [12816/60600]
Training loss: 1.818625  [14416/60600]
Training loss: 2.139157  [16016/60600]
Training loss: 2.043691  [17616/60600]
Training loss: 1.483280  [19216/60600]
Training loss: 1.959330  [20816/60600]
Training loss: 2.695133  [22416/60600]
Training loss: 1.622742  [24016/60600]
Training loss: 2.090668  [25616/60600]
Training loss: 2.224931  [27216/60600]
Training loss: 2.308890  [28816/60600]
Training loss: 2.128198  [30416/60600]
Training loss: 2.211534  [32016/60600]
Training loss: 1.838183  [33616/60600]
Training loss: 1.989346  [35216/60600]
Training loss: 2.236851  [36816/60600]
Training loss: 2.132150  [38416/60600]
Training loss: 2.238007  [40016/60600]
Training loss: 2.088311  [41616/60600]
Training loss: 2.050830  [43216/60600]
Training loss: 1.798157  [44816/60600]
Training loss: 2.057790  [46416/60600]
Training loss: 2.605732  [48016/60600]
Training loss: 2.191980  [49616/60600]
Training loss: 2.424701  [51216/60600]
Training loss: 1.602243  [52816/60600]
Training loss: 2.168000  [54416/60600]
Training loss: 1.902229  [56016/60600]
Training loss: 1.891506  [57616/60600]
Training loss: 1.727646  [59216/60600]
Training accuracy: 49.81 %
Validation loss: 2.100464
Validation accuracy: 49.37% 

Epoch 18
-------------------------------
Training loss: 1.516477  [16/60600]
Training loss: 2.582434  [1616/60600]
Training loss: 2.645680  [3216/60600]
Training loss: 1.553825  [4816/60600]
Training loss: 2.456689  [6416/60600]
Training loss: 1.906866  [8016/60600]
Training loss: 1.979156  [9616/60600]
Training loss: 2.888294  [11216/60600]
Training loss: 1.822262  [12816/60600]
Training loss: 2.105294  [14416/60600]
Training loss: 1.782620  [16016/60600]
Training loss: 2.141054  [17616/60600]
Training loss: 2.037365  [19216/60600]
Training loss: 2.263833  [20816/60600]
Training loss: 2.061433  [22416/60600]
Training loss: 2.664030  [24016/60600]
Training loss: 1.033109  [25616/60600]
Training loss: 2.514669  [27216/60600]
Training loss: 1.955476  [28816/60600]
Training loss: 1.925722  [30416/60600]
Training loss: 2.168086  [32016/60600]
Training loss: 1.637483  [33616/60600]
Training loss: 1.660767  [35216/60600]
Training loss: 2.708979  [36816/60600]
Training loss: 1.736462  [38416/60600]
Training loss: 2.650401  [40016/60600]
Training loss: 2.264686  [41616/60600]
Training loss: 2.688010  [43216/60600]
Training loss: 2.259112  [44816/60600]
Training loss: 2.762026  [46416/60600]
Training loss: 0.983108  [48016/60600]
Training loss: 1.498180  [49616/60600]
Training loss: 2.272887  [51216/60600]
Training loss: 1.972202  [52816/60600]
Training loss: 3.209091  [54416/60600]
Training loss: 2.522177  [56016/60600]
Training loss: 2.177057  [57616/60600]
Training loss: 1.898604  [59216/60600]
Training accuracy: 50.04 %
Validation loss: 2.078918
Validation accuracy: 49.34% 

Epoch 19
-------------------------------
Training loss: 2.423489  [16/60600]
Training loss: 1.599636  [1616/60600]
Training loss: 1.614892  [3216/60600]
Training loss: 2.179942  [4816/60600]
Training loss: 1.268649  [6416/60600]
Training loss: 2.081164  [8016/60600]
Training loss: 2.414524  [9616/60600]
Training loss: 2.310694  [11216/60600]
Training loss: 2.150827  [12816/60600]
Training loss: 1.745277  [14416/60600]
Training loss: 1.918819  [16016/60600]
Training loss: 1.838759  [17616/60600]
Training loss: 2.217045  [19216/60600]
Training loss: 1.474578  [20816/60600]
Training loss: 1.479808  [22416/60600]
Training loss: 1.794077  [24016/60600]
Training loss: 1.925843  [25616/60600]
Training loss: 1.765139  [27216/60600]
Training loss: 1.803979  [28816/60600]
Training loss: 1.753157  [30416/60600]
Training loss: 2.103971  [32016/60600]
Training loss: 2.344247  [33616/60600]
Training loss: 2.688684  [35216/60600]
Training loss: 2.655894  [36816/60600]
Training loss: 1.668999  [38416/60600]
Training loss: 2.009246  [40016/60600]
Training loss: 2.213912  [41616/60600]
Training loss: 2.573734  [43216/60600]
Training loss: 2.204330  [44816/60600]
Training loss: 1.125340  [46416/60600]
Training loss: 2.878839  [48016/60600]
Training loss: 1.635936  [49616/60600]
Training loss: 2.087417  [51216/60600]
Training loss: 2.898664  [52816/60600]
Training loss: 2.978557  [54416/60600]
Training loss: 1.992094  [56016/60600]
Training loss: 2.625434  [57616/60600]
Training loss: 2.256134  [59216/60600]
Training accuracy: 50.25 %
Validation loss: 2.058779
Validation accuracy: 49.78% 

Epoch 20
-------------------------------
Training loss: 1.790127  [16/60600]
Training loss: 2.406743  [1616/60600]
Training loss: 1.594720  [3216/60600]
Training loss: 2.283645  [4816/60600]
Training loss: 2.556602  [6416/60600]
Training loss: 2.336333  [8016/60600]
Training loss: 1.866492  [9616/60600]
Training loss: 1.901951  [11216/60600]
Training loss: 2.744340  [12816/60600]
Training loss: 2.285756  [14416/60600]
Training loss: 2.186808  [16016/60600]
Training loss: 1.898938  [17616/60600]
Training loss: 2.786134  [19216/60600]
Training loss: 1.640711  [20816/60600]
Training loss: 2.070826  [22416/60600]
Training loss: 2.116038  [24016/60600]
Training loss: 2.673982  [25616/60600]
Training loss: 2.313096  [27216/60600]
Training loss: 1.779581  [28816/60600]
Training loss: 2.106313  [30416/60600]
Training loss: 2.197374  [32016/60600]
Training loss: 1.696450  [33616/60600]
Training loss: 1.225425  [35216/60600]
Training loss: 2.593329  [36816/60600]
Training loss: 2.426019  [38416/60600]
Training loss: 1.916704  [40016/60600]
Training loss: 1.726286  [41616/60600]
Training loss: 1.919560  [43216/60600]
Training loss: 1.862706  [44816/60600]
Training loss: 2.793034  [46416/60600]
Training loss: 1.976909  [48016/60600]
Training loss: 1.709630  [49616/60600]
Training loss: 1.678356  [51216/60600]
Training loss: 2.417898  [52816/60600]
Training loss: 2.289005  [54416/60600]
Training loss: 1.926486  [56016/60600]
Training loss: 2.031645  [57616/60600]
Training loss: 2.012690  [59216/60600]
Training accuracy: 50.60 %
Validation loss: 2.054354
Validation accuracy: 49.95% 

Epoch 21
-------------------------------
Training loss: 1.765363  [16/60600]
Training loss: 2.050537  [1616/60600]
Training loss: 1.704759  [3216/60600]
Training loss: 2.767395  [4816/60600]
Training loss: 2.529095  [6416/60600]
Training loss: 1.386420  [8016/60600]
Training loss: 2.858673  [9616/60600]
Training loss: 1.734445  [11216/60600]
Training loss: 1.773954  [12816/60600]
Training loss: 1.952417  [14416/60600]
Training loss: 2.442935  [16016/60600]
Training loss: 2.137375  [17616/60600]
Training loss: 1.665027  [19216/60600]
Training loss: 1.903046  [20816/60600]
Training loss: 1.553464  [22416/60600]
Training loss: 2.202997  [24016/60600]
Training loss: 2.256429  [25616/60600]
Training loss: 1.483841  [27216/60600]
Training loss: 1.680092  [28816/60600]
Training loss: 1.867195  [30416/60600]
Training loss: 2.786393  [32016/60600]
Training loss: 2.086075  [33616/60600]
Training loss: 2.173849  [35216/60600]
Training loss: 1.403401  [36816/60600]
Training loss: 2.192600  [38416/60600]
Training loss: 1.615434  [40016/60600]
Training loss: 1.611714  [41616/60600]
Training loss: 1.520762  [43216/60600]
Training loss: 1.813014  [44816/60600]
Training loss: 2.379262  [46416/60600]
Training loss: 2.371818  [48016/60600]
Training loss: 2.182848  [49616/60600]
Training loss: 2.703557  [51216/60600]
Training loss: 1.719276  [52816/60600]
Training loss: 2.136097  [54416/60600]
Training loss: 2.198242  [56016/60600]
Training loss: 1.842168  [57616/60600]
Training loss: 1.942553  [59216/60600]
Training accuracy: 51.07 %
Validation loss: 2.049738
Validation accuracy: 49.85% 

Epoch 22
-------------------------------
Training loss: 1.753006  [16/60600]
Training loss: 2.424849  [1616/60600]
Training loss: 2.251801  [3216/60600]
Training loss: 1.999735  [4816/60600]
Training loss: 2.401196  [6416/60600]
Training loss: 1.656042  [8016/60600]
Training loss: 1.806850  [9616/60600]
Training loss: 1.827771  [11216/60600]
Training loss: 1.620243  [12816/60600]
Training loss: 2.283979  [14416/60600]
Training loss: 1.511300  [16016/60600]
Training loss: 1.304405  [17616/60600]
Training loss: 2.246384  [19216/60600]
Training loss: 2.056678  [20816/60600]
Training loss: 2.213574  [22416/60600]
Training loss: 1.464896  [24016/60600]
Training loss: 2.453370  [25616/60600]
Training loss: 2.131136  [27216/60600]
Training loss: 2.342555  [28816/60600]
Training loss: 1.895522  [30416/60600]
Training loss: 2.208550  [32016/60600]
Training loss: 1.436184  [33616/60600]
Training loss: 1.348763  [35216/60600]
Training loss: 1.923580  [36816/60600]
Training loss: 1.813416  [38416/60600]
Training loss: 2.323298  [40016/60600]
Training loss: 1.782078  [41616/60600]
Training loss: 2.305458  [43216/60600]
Training loss: 1.918894  [44816/60600]
Training loss: 2.253306  [46416/60600]
Training loss: 1.555941  [48016/60600]
Training loss: 1.979109  [49616/60600]
Training loss: 2.831688  [51216/60600]
Training loss: 1.968719  [52816/60600]
Training loss: 1.806760  [54416/60600]
Training loss: 2.003536  [56016/60600]
Training loss: 2.044512  [57616/60600]
Training loss: 2.189981  [59216/60600]
Training accuracy: 50.87 %
Validation loss: 2.044138
Validation accuracy: 50.07% 

Epoch 23
-------------------------------
Training loss: 3.051348  [16/60600]
Training loss: 2.236828  [1616/60600]
Training loss: 1.995342  [3216/60600]
Training loss: 1.316835  [4816/60600]
Training loss: 1.779585  [6416/60600]
Training loss: 1.920083  [8016/60600]
Training loss: 1.618219  [9616/60600]
Training loss: 1.778758  [11216/60600]
Training loss: 2.540273  [12816/60600]
Training loss: 2.198611  [14416/60600]
Training loss: 2.441553  [16016/60600]
Training loss: 2.609729  [17616/60600]
Training loss: 2.042942  [19216/60600]
Training loss: 2.195026  [20816/60600]
Training loss: 1.507372  [22416/60600]
Training loss: 1.597132  [24016/60600]
Training loss: 1.357818  [25616/60600]
Training loss: 1.524227  [27216/60600]
Training loss: 2.064844  [28816/60600]
Training loss: 2.095877  [30416/60600]
Training loss: 1.708847  [32016/60600]
Training loss: 1.072429  [33616/60600]
Training loss: 2.400805  [35216/60600]
Training loss: 2.025760  [36816/60600]
Training loss: 1.918848  [38416/60600]
Training loss: 2.729524  [40016/60600]
Training loss: 2.408875  [41616/60600]
Training loss: 2.615007  [43216/60600]
Training loss: 1.784360  [44816/60600]
Training loss: 1.796630  [46416/60600]
Training loss: 1.335429  [48016/60600]
Training loss: 2.440125  [49616/60600]
Training loss: 1.760983  [51216/60600]
Training loss: 1.809702  [52816/60600]
Training loss: 1.978461  [54416/60600]
Training loss: 1.577458  [56016/60600]
Training loss: 2.876262  [57616/60600]
Training loss: 1.235185  [59216/60600]
Training accuracy: 51.07 %
Validation loss: 2.018737
Validation accuracy: 50.61% 

Epoch 24
-------------------------------
Training loss: 1.522668  [16/60600]
Training loss: 2.483430  [1616/60600]
Training loss: 2.629435  [3216/60600]
Training loss: 2.339162  [4816/60600]
Training loss: 2.589370  [6416/60600]
Training loss: 1.297740  [8016/60600]
Training loss: 2.078241  [9616/60600]
Training loss: 2.200451  [11216/60600]
Training loss: 1.355877  [12816/60600]
Training loss: 1.964456  [14416/60600]
Training loss: 2.391849  [16016/60600]
Training loss: 2.328055  [17616/60600]
Training loss: 2.231468  [19216/60600]
Training loss: 2.412758  [20816/60600]
Training loss: 2.728758  [22416/60600]
Training loss: 1.822855  [24016/60600]
Training loss: 1.633393  [25616/60600]
Training loss: 1.995349  [27216/60600]
Training loss: 2.001153  [28816/60600]
Training loss: 1.679595  [30416/60600]
Training loss: 2.099267  [32016/60600]
Training loss: 2.699920  [33616/60600]
Training loss: 1.479383  [35216/60600]
Training loss: 2.403374  [36816/60600]
Training loss: 2.492338  [38416/60600]
Training loss: 2.590496  [40016/60600]
Training loss: 1.540392  [41616/60600]
Training loss: 2.596427  [43216/60600]
Training loss: 2.003957  [44816/60600]
Training loss: 1.669710  [46416/60600]
Training loss: 1.798758  [48016/60600]
Training loss: 2.806417  [49616/60600]
Training loss: 2.313963  [51216/60600]
Training loss: 2.566056  [52816/60600]
Training loss: 2.568653  [54416/60600]
Training loss: 1.837853  [56016/60600]
Training loss: 2.391946  [57616/60600]
Training loss: 2.499192  [59216/60600]
Training accuracy: 51.15 %
Validation loss: 2.006759
Validation accuracy: 50.94% 

Epoch 25
-------------------------------
Training loss: 1.379042  [16/60600]
Training loss: 1.415609  [1616/60600]
Training loss: 1.504418  [3216/60600]
Training loss: 1.609525  [4816/60600]
Training loss: 1.817160  [6416/60600]
Training loss: 2.114908  [8016/60600]
Training loss: 2.071080  [9616/60600]
Training loss: 1.980785  [11216/60600]
Training loss: 2.153811  [12816/60600]
Training loss: 2.379816  [14416/60600]
Training loss: 2.247468  [16016/60600]
Training loss: 1.928867  [17616/60600]
Training loss: 2.500689  [19216/60600]
Training loss: 2.259530  [20816/60600]
Training loss: 2.118732  [22416/60600]
Training loss: 1.616485  [24016/60600]
Training loss: 1.058740  [25616/60600]
Training loss: 2.141809  [27216/60600]
Training loss: 2.997903  [28816/60600]
Training loss: 2.058746  [30416/60600]
Training loss: 2.567871  [32016/60600]
Training loss: 1.942117  [33616/60600]
Training loss: 1.795842  [35216/60600]
Training loss: 1.714080  [36816/60600]
Training loss: 2.287056  [38416/60600]
Training loss: 1.629351  [40016/60600]
Training loss: 1.908295  [41616/60600]
Training loss: 1.670169  [43216/60600]
Training loss: 2.622739  [44816/60600]
Training loss: 1.295715  [46416/60600]
Training loss: 2.117079  [48016/60600]
Training loss: 2.180556  [49616/60600]
Training loss: 2.202267  [51216/60600]
Training loss: 3.152665  [52816/60600]
Training loss: 1.520120  [54416/60600]
Training loss: 1.942902  [56016/60600]
Training loss: 1.137081  [57616/60600]
Training loss: 1.501655  [59216/60600]
Training accuracy: 51.41 %
Validation loss: 2.016907
Validation accuracy: 50.33% 

Epoch 26
-------------------------------
Training loss: 1.815268  [16/60600]
Training loss: 1.640120  [1616/60600]
Training loss: 1.527822  [3216/60600]
Training loss: 2.410648  [4816/60600]
Training loss: 2.687252  [6416/60600]
Training loss: 2.033445  [8016/60600]
Training loss: 1.448027  [9616/60600]
Training loss: 1.600734  [11216/60600]
Training loss: 2.373401  [12816/60600]
Training loss: 2.682314  [14416/60600]
Training loss: 1.947951  [16016/60600]
Training loss: 1.685796  [17616/60600]
Training loss: 2.215212  [19216/60600]
Training loss: 1.883828  [20816/60600]
Training loss: 2.382322  [22416/60600]
Training loss: 1.842275  [24016/60600]
Training loss: 1.974167  [25616/60600]
Training loss: 2.636303  [27216/60600]
Training loss: 1.445279  [28816/60600]
Training loss: 1.855985  [30416/60600]
Training loss: 1.664500  [32016/60600]
Training loss: 2.124905  [33616/60600]
Training loss: 1.674923  [35216/60600]
Training loss: 1.586543  [36816/60600]
Training loss: 2.131265  [38416/60600]
Training loss: 1.621786  [40016/60600]
Training loss: 1.322035  [41616/60600]
Training loss: 1.969985  [43216/60600]
Training loss: 1.597856  [44816/60600]
Training loss: 2.018046  [46416/60600]
Training loss: 1.497770  [48016/60600]
Training loss: 2.063609  [49616/60600]
Training loss: 1.650815  [51216/60600]
Training loss: 2.481041  [52816/60600]
Training loss: 2.114929  [54416/60600]
Training loss: 2.553782  [56016/60600]
Training loss: 1.234843  [57616/60600]
Training loss: 1.807674  [59216/60600]
Training accuracy: 51.73 %
Validation loss: 1.996718
Validation accuracy: 50.85% 

Epoch 27
-------------------------------
Training loss: 1.687508  [16/60600]
Training loss: 1.884203  [1616/60600]
Training loss: 1.719815  [3216/60600]
Training loss: 1.675286  [4816/60600]
Training loss: 1.872905  [6416/60600]
Training loss: 1.797902  [8016/60600]
Training loss: 1.898549  [9616/60600]
Training loss: 2.548910  [11216/60600]
Training loss: 1.804368  [12816/60600]
Training loss: 1.977078  [14416/60600]
Training loss: 1.342680  [16016/60600]
Training loss: 2.577713  [17616/60600]
Training loss: 2.647539  [19216/60600]
Training loss: 1.184065  [20816/60600]
Training loss: 1.745811  [22416/60600]
Training loss: 1.623368  [24016/60600]
Training loss: 1.427287  [25616/60600]
Training loss: 2.002471  [27216/60600]
Training loss: 1.731116  [28816/60600]
Training loss: 2.289747  [30416/60600]
Training loss: 1.571404  [32016/60600]
Training loss: 1.713996  [33616/60600]
Training loss: 2.174632  [35216/60600]
Training loss: 1.438600  [36816/60600]
Training loss: 1.314330  [38416/60600]
Training loss: 1.742845  [40016/60600]
Training loss: 1.365495  [41616/60600]
Training loss: 1.660397  [43216/60600]
Training loss: 1.913198  [44816/60600]
Training loss: 2.216654  [46416/60600]
Training loss: 1.860450  [48016/60600]
Training loss: 2.483176  [49616/60600]
Training loss: 1.808509  [51216/60600]
Training loss: 1.774677  [52816/60600]
Training loss: 1.606388  [54416/60600]
Training loss: 1.730597  [56016/60600]
Training loss: 1.583988  [57616/60600]
Training loss: 1.740270  [59216/60600]
Training accuracy: 51.83 %
Validation loss: 1.989699
Validation accuracy: 51.19% 

Epoch 28
-------------------------------
Training loss: 2.027956  [16/60600]
Training loss: 2.459557  [1616/60600]
Training loss: 2.406867  [3216/60600]
Training loss: 2.349133  [4816/60600]
Training loss: 1.953577  [6416/60600]
Training loss: 1.599209  [8016/60600]
Training loss: 1.467883  [9616/60600]
Training loss: 2.406307  [11216/60600]
Training loss: 1.962305  [12816/60600]
Training loss: 2.343426  [14416/60600]
Training loss: 2.347855  [16016/60600]
Training loss: 1.643211  [17616/60600]
Training loss: 1.651348  [19216/60600]
Training loss: 1.807647  [20816/60600]
Training loss: 2.233762  [22416/60600]
Training loss: 1.838325  [24016/60600]
Training loss: 2.141805  [25616/60600]
Training loss: 2.133197  [27216/60600]
Training loss: 1.707307  [28816/60600]
Training loss: 2.147907  [30416/60600]
Training loss: 1.973284  [32016/60600]
Training loss: 2.198048  [33616/60600]
Training loss: 1.464872  [35216/60600]
Training loss: 1.829379  [36816/60600]
Training loss: 1.391806  [38416/60600]
Training loss: 1.706204  [40016/60600]
Training loss: 1.574174  [41616/60600]
Training loss: 1.817920  [43216/60600]
Training loss: 1.987578  [44816/60600]
Training loss: 2.145523  [46416/60600]
Training loss: 1.572975  [48016/60600]
Training loss: 3.091747  [49616/60600]
Training loss: 2.587616  [51216/60600]
Training loss: 1.873607  [52816/60600]
Training loss: 1.920235  [54416/60600]
Training loss: 1.669936  [56016/60600]
Training loss: 2.849616  [57616/60600]
Training loss: 2.193027  [59216/60600]
Training accuracy: 51.86 %
Validation loss: 1.982654
Validation accuracy: 51.10% 

Epoch 29
-------------------------------
Training loss: 1.735748  [16/60600]
Training loss: 1.653268  [1616/60600]
Training loss: 1.846161  [3216/60600]
Training loss: 2.429702  [4816/60600]
Training loss: 2.636266  [6416/60600]
Training loss: 2.560238  [8016/60600]
Training loss: 1.559499  [9616/60600]
Training loss: 2.030979  [11216/60600]
Training loss: 2.039206  [12816/60600]
Training loss: 2.069054  [14416/60600]
Training loss: 1.189955  [16016/60600]
Training loss: 1.372078  [17616/60600]
Training loss: 1.683696  [19216/60600]
Training loss: 1.912529  [20816/60600]
Training loss: 2.011261  [22416/60600]
Training loss: 2.437675  [24016/60600]
Training loss: 2.105590  [25616/60600]
Training loss: 2.303392  [27216/60600]
Training loss: 2.026994  [28816/60600]
Training loss: 2.530987  [30416/60600]
Training loss: 1.698843  [32016/60600]
Training loss: 2.348557  [33616/60600]
Training loss: 1.571610  [35216/60600]
Training loss: 1.394865  [36816/60600]
Training loss: 1.598837  [38416/60600]
Training loss: 2.389696  [40016/60600]
Training loss: 2.131442  [41616/60600]
Training loss: 1.772536  [43216/60600]
Training loss: 1.945750  [44816/60600]
Training loss: 1.703027  [46416/60600]
Training loss: 1.605814  [48016/60600]
Training loss: 2.136376  [49616/60600]
Training loss: 2.247174  [51216/60600]
Training loss: 2.314181  [52816/60600]
Training loss: 1.651704  [54416/60600]
Training loss: 1.562030  [56016/60600]
Training loss: 1.513170  [57616/60600]
Training loss: 1.658010  [59216/60600]
Training accuracy: 51.88 %
Validation loss: 1.996266
Validation accuracy: 50.73% 

Epoch 30
-------------------------------
Training loss: 2.652439  [16/60600]
Training loss: 1.636436  [1616/60600]
Training loss: 2.216923  [3216/60600]
Training loss: 2.782876  [4816/60600]
Training loss: 2.493853  [6416/60600]
Training loss: 1.481720  [8016/60600]
Training loss: 2.145109  [9616/60600]
Training loss: 2.040408  [11216/60600]
Training loss: 2.404889  [12816/60600]
Training loss: 1.686891  [14416/60600]
Training loss: 1.539883  [16016/60600]
Training loss: 1.751857  [17616/60600]
Training loss: 2.184594  [19216/60600]
Training loss: 2.210669  [20816/60600]
Training loss: 1.522545  [22416/60600]
Training loss: 2.298385  [24016/60600]
Training loss: 1.480031  [25616/60600]
Training loss: 2.144952  [27216/60600]
Training loss: 1.401725  [28816/60600]
Training loss: 2.529331  [30416/60600]
Training loss: 1.895045  [32016/60600]
Training loss: 1.651003  [33616/60600]
Training loss: 2.239663  [35216/60600]
Training loss: 1.954600  [36816/60600]
Training loss: 1.847222  [38416/60600]
Training loss: 1.422457  [40016/60600]
Training loss: 1.963358  [41616/60600]
Training loss: 1.475440  [43216/60600]
Training loss: 1.585042  [44816/60600]
Training loss: 2.342608  [46416/60600]
Training loss: 1.758356  [48016/60600]
Training loss: 1.364381  [49616/60600]
Training loss: 2.263605  [51216/60600]
Training loss: 1.070801  [52816/60600]
Training loss: 1.570362  [54416/60600]
Training loss: 2.170453  [56016/60600]
Training loss: 2.485316  [57616/60600]
Training loss: 2.267717  [59216/60600]
Training accuracy: 52.17 %
Validation loss: 1.978231
Validation accuracy: 51.30% 

Epoch 31
-------------------------------
Training loss: 1.840250  [16/60600]
Training loss: 2.007288  [1616/60600]
Training loss: 2.040187  [3216/60600]
Training loss: 1.293756  [4816/60600]
Training loss: 1.836881  [6416/60600]
Training loss: 2.686768  [8016/60600]
Training loss: 2.823888  [9616/60600]
Training loss: 2.125871  [11216/60600]
Training loss: 2.078327  [12816/60600]
Training loss: 2.524989  [14416/60600]
Training loss: 1.360134  [16016/60600]
Training loss: 1.969549  [17616/60600]
Training loss: 1.567938  [19216/60600]
Training loss: 2.261528  [20816/60600]
Training loss: 1.402917  [22416/60600]
Training loss: 1.575517  [24016/60600]
Training loss: 1.865615  [25616/60600]
Training loss: 1.548824  [27216/60600]
Training loss: 0.664254  [28816/60600]
Training loss: 1.584420  [30416/60600]
Training loss: 1.706503  [32016/60600]
Training loss: 1.809936  [33616/60600]
Training loss: 1.651805  [35216/60600]
Training loss: 1.649634  [36816/60600]
Training loss: 2.295094  [38416/60600]
Training loss: 1.724287  [40016/60600]
Training loss: 1.751066  [41616/60600]
Training loss: 1.428557  [43216/60600]
Training loss: 1.411547  [44816/60600]
Training loss: 2.105168  [46416/60600]
Training loss: 2.387420  [48016/60600]
Training loss: 2.201251  [49616/60600]
Training loss: 1.738183  [51216/60600]
Training loss: 1.384866  [52816/60600]
Training loss: 1.677081  [54416/60600]
Training loss: 1.966084  [56016/60600]
Training loss: 1.902152  [57616/60600]
Training loss: 2.057025  [59216/60600]
Training accuracy: 52.31 %
Validation loss: 1.971889
Validation accuracy: 51.31% 

Epoch 32
-------------------------------
Training loss: 2.862148  [16/60600]
Training loss: 2.214731  [1616/60600]
Training loss: 1.490065  [3216/60600]
Training loss: 2.529684  [4816/60600]
Training loss: 1.997221  [6416/60600]
Training loss: 1.970160  [8016/60600]
Training loss: 1.671505  [9616/60600]
Training loss: 1.335049  [11216/60600]
Training loss: 1.927901  [12816/60600]
Training loss: 1.632277  [14416/60600]
Training loss: 1.867640  [16016/60600]
Training loss: 1.799694  [17616/60600]
Training loss: 2.316730  [19216/60600]
Training loss: 1.594278  [20816/60600]
Training loss: 2.680227  [22416/60600]
Training loss: 1.827158  [24016/60600]
Training loss: 1.988137  [25616/60600]
Training loss: 2.015809  [27216/60600]
Training loss: 2.200050  [28816/60600]
Training loss: 1.933843  [30416/60600]
Training loss: 1.795907  [32016/60600]
Training loss: 1.704356  [33616/60600]
Training loss: 1.854820  [35216/60600]
Training loss: 2.352030  [36816/60600]
Training loss: 1.645260  [38416/60600]
Training loss: 1.661422  [40016/60600]
Training loss: 1.999769  [41616/60600]
Training loss: 2.304241  [43216/60600]
Training loss: 1.651150  [44816/60600]
Training loss: 2.324695  [46416/60600]
Training loss: 1.662320  [48016/60600]
Training loss: 1.745181  [49616/60600]
Training loss: 1.892865  [51216/60600]
Training loss: 2.187000  [52816/60600]
Training loss: 2.159562  [54416/60600]
Training loss: 2.176553  [56016/60600]
Training loss: 1.790877  [57616/60600]
Training loss: 1.645576  [59216/60600]
Training accuracy: 52.49 %
Validation loss: 1.966084
Validation accuracy: 51.48% 

Epoch 33
-------------------------------
Training loss: 1.727416  [16/60600]
Training loss: 1.360275  [1616/60600]
Training loss: 2.708723  [3216/60600]
Training loss: 1.559956  [4816/60600]
Training loss: 1.965054  [6416/60600]
Training loss: 1.497221  [8016/60600]
Training loss: 1.804557  [9616/60600]
Training loss: 2.330934  [11216/60600]
Training loss: 1.376307  [12816/60600]
Training loss: 1.967688  [14416/60600]
Training loss: 1.397808  [16016/60600]
Training loss: 1.526431  [17616/60600]
Training loss: 1.395329  [19216/60600]
Training loss: 2.619768  [20816/60600]
Training loss: 1.796257  [22416/60600]
Training loss: 2.683228  [24016/60600]
Training loss: 2.518198  [25616/60600]
Training loss: 2.239062  [27216/60600]
Training loss: 2.041777  [28816/60600]
Training loss: 1.516769  [30416/60600]
Training loss: 2.305210  [32016/60600]
Training loss: 1.302435  [33616/60600]
Training loss: 2.338151  [35216/60600]
Training loss: 1.789717  [36816/60600]
Training loss: 1.690119  [38416/60600]
Training loss: 1.860986  [40016/60600]
Training loss: 2.799881  [41616/60600]
Training loss: 1.744796  [43216/60600]
Training loss: 2.109268  [44816/60600]
Training loss: 1.589223  [46416/60600]
Training loss: 1.552911  [48016/60600]
Training loss: 1.846374  [49616/60600]
Training loss: 1.737604  [51216/60600]
Training loss: 1.743073  [52816/60600]
Training loss: 2.083416  [54416/60600]
Training loss: 1.749961  [56016/60600]
Training loss: 1.973351  [57616/60600]
Training loss: 2.185489  [59216/60600]
Training accuracy: 52.59 %
Validation loss: 1.958165
Validation accuracy: 51.62% 

Epoch 34
-------------------------------
Training loss: 2.323810  [16/60600]
Training loss: 1.674031  [1616/60600]
Training loss: 1.604811  [3216/60600]
Training loss: 2.457230  [4816/60600]
Training loss: 2.471716  [6416/60600]
Training loss: 1.494685  [8016/60600]
Training loss: 1.752470  [9616/60600]
Training loss: 2.089442  [11216/60600]
Training loss: 1.581505  [12816/60600]
Training loss: 1.934769  [14416/60600]
Training loss: 1.481267  [16016/60600]
Training loss: 2.114317  [17616/60600]
Training loss: 1.679704  [19216/60600]
Training loss: 1.531151  [20816/60600]
Training loss: 1.736174  [22416/60600]
Training loss: 2.012647  [24016/60600]
Training loss: 1.813872  [25616/60600]
Training loss: 1.665505  [27216/60600]
Training loss: 2.167708  [28816/60600]
Training loss: 1.777294  [30416/60600]
Training loss: 1.425643  [32016/60600]
Training loss: 1.876996  [33616/60600]
Training loss: 2.191964  [35216/60600]
Training loss: 2.182169  [36816/60600]
Training loss: 1.708427  [38416/60600]
Training loss: 1.927890  [40016/60600]
Training loss: 1.229484  [41616/60600]
Training loss: 2.159844  [43216/60600]
Training loss: 1.771964  [44816/60600]
Training loss: 1.568488  [46416/60600]
Training loss: 1.996608  [48016/60600]
Training loss: 2.384774  [49616/60600]
Training loss: 1.579330  [51216/60600]
Training loss: 1.440885  [52816/60600]
Training loss: 2.067584  [54416/60600]
Training loss: 1.626691  [56016/60600]
Training loss: 1.708251  [57616/60600]
Training loss: 2.475760  [59216/60600]
Training accuracy: 52.78 %
Validation loss: 1.953306
Validation accuracy: 51.58% 

Epoch 35
-------------------------------
Training loss: 1.192445  [16/60600]
Training loss: 1.893907  [1616/60600]
Training loss: 1.819956  [3216/60600]
Training loss: 1.568411  [4816/60600]
Training loss: 2.369430  [6416/60600]
Training loss: 2.016316  [8016/60600]
Training loss: 2.458971  [9616/60600]
Training loss: 1.834926  [11216/60600]
Training loss: 2.214596  [12816/60600]
Training loss: 2.077308  [14416/60600]
Training loss: 2.125721  [16016/60600]
Training loss: 2.172377  [17616/60600]
Training loss: 1.335929  [19216/60600]
Training loss: 1.280071  [20816/60600]
Training loss: 1.947602  [22416/60600]
Training loss: 2.016247  [24016/60600]
Training loss: 2.458724  [25616/60600]
Training loss: 2.534658  [27216/60600]
Training loss: 1.685041  [28816/60600]
Training loss: 2.488553  [30416/60600]
Training loss: 1.626842  [32016/60600]
Training loss: 1.635503  [33616/60600]
Training loss: 1.856190  [35216/60600]
Training loss: 1.875794  [36816/60600]
Training loss: 2.172752  [38416/60600]
Training loss: 1.836970  [40016/60600]
Training loss: 1.938940  [41616/60600]
Training loss: 2.839119  [43216/60600]
Training loss: 2.397227  [44816/60600]
Training loss: 1.829833  [46416/60600]
Training loss: 2.229581  [48016/60600]
Training loss: 1.859342  [49616/60600]
Training loss: 1.135356  [51216/60600]
Training loss: 2.442443  [52816/60600]
Training loss: 2.337489  [54416/60600]
Training loss: 1.839476  [56016/60600]
Training loss: 2.260280  [57616/60600]
Training loss: 1.544595  [59216/60600]
Training accuracy: 52.76 %
Validation loss: 1.959125
Validation accuracy: 51.66% 

Epoch 36
-------------------------------
Training loss: 1.809722  [16/60600]
Training loss: 1.785958  [1616/60600]
Training loss: 1.882182  [3216/60600]
Training loss: 2.255816  [4816/60600]
Training loss: 2.574438  [6416/60600]
Training loss: 1.824939  [8016/60600]
Training loss: 2.129158  [9616/60600]
Training loss: 1.531679  [11216/60600]
Training loss: 2.394607  [12816/60600]
Training loss: 1.620430  [14416/60600]
Training loss: 1.387571  [16016/60600]
Training loss: 2.127138  [17616/60600]
Training loss: 1.989307  [19216/60600]
Training loss: 2.148347  [20816/60600]
Training loss: 2.072303  [22416/60600]
Training loss: 1.722168  [24016/60600]
Training loss: 2.496838  [25616/60600]
Training loss: 1.520177  [27216/60600]
Training loss: 1.514981  [28816/60600]
Training loss: 1.225336  [30416/60600]
Training loss: 2.164944  [32016/60600]
Training loss: 2.837011  [33616/60600]
Training loss: 2.219993  [35216/60600]
Training loss: 1.768410  [36816/60600]
Training loss: 2.252452  [38416/60600]
Training loss: 2.563422  [40016/60600]
Training loss: 2.158549  [41616/60600]
Training loss: 2.513985  [43216/60600]
Training loss: 1.586321  [44816/60600]
Training loss: 2.216045  [46416/60600]
Training loss: 1.530146  [48016/60600]
Training loss: 1.497757  [49616/60600]
Training loss: 2.434519  [51216/60600]
Training loss: 2.071419  [52816/60600]
Training loss: 1.566041  [54416/60600]
Training loss: 2.468339  [56016/60600]
Training loss: 2.371101  [57616/60600]
Training loss: 1.684707  [59216/60600]
Training accuracy: 52.89 %
Validation loss: 1.946109
Validation accuracy: 51.98% 

Epoch 37
-------------------------------
Training loss: 2.234530  [16/60600]
Training loss: 1.832725  [1616/60600]
Training loss: 2.419041  [3216/60600]
Training loss: 1.353225  [4816/60600]
Training loss: 2.358636  [6416/60600]
Training loss: 1.364909  [8016/60600]
Training loss: 1.992363  [9616/60600]
Training loss: 3.449696  [11216/60600]
Training loss: 1.794210  [12816/60600]
Training loss: 1.451747  [14416/60600]
Training loss: 1.894335  [16016/60600]
Training loss: 2.295871  [17616/60600]
Training loss: 1.587924  [19216/60600]
Training loss: 1.964635  [20816/60600]
Training loss: 1.671354  [22416/60600]
Training loss: 1.867641  [24016/60600]
Training loss: 1.913684  [25616/60600]
Training loss: 1.771134  [27216/60600]
Training loss: 1.477031  [28816/60600]
Training loss: 1.641886  [30416/60600]
Training loss: 1.836091  [32016/60600]
Training loss: 1.359932  [33616/60600]
Training loss: 2.527873  [35216/60600]
Training loss: 2.050964  [36816/60600]
Training loss: 2.029211  [38416/60600]
Training loss: 1.656604  [40016/60600]
Training loss: 2.817400  [41616/60600]
Training loss: 1.597621  [43216/60600]
Training loss: 2.117810  [44816/60600]
Training loss: 2.150411  [46416/60600]
Training loss: 2.388299  [48016/60600]
Training loss: 1.613736  [49616/60600]
Training loss: 1.493230  [51216/60600]
Training loss: 2.211037  [52816/60600]
Training loss: 1.803772  [54416/60600]
Training loss: 1.885857  [56016/60600]
Training loss: 1.836720  [57616/60600]
Training loss: 2.202124  [59216/60600]
Training accuracy: 53.04 %
Validation loss: 1.958926
Validation accuracy: 51.56% 

Epoch 38
-------------------------------
Training loss: 1.648746  [16/60600]
Training loss: 2.228799  [1616/60600]
Training loss: 1.568493  [3216/60600]
Training loss: 1.865716  [4816/60600]
Training loss: 2.457577  [6416/60600]
Training loss: 1.295612  [8016/60600]
Training loss: 1.925603  [9616/60600]
Training loss: 1.558769  [11216/60600]
Training loss: 1.837136  [12816/60600]
Training loss: 1.090384  [14416/60600]
Training loss: 2.643553  [16016/60600]
Training loss: 2.246476  [17616/60600]
Training loss: 1.956496  [19216/60600]
Training loss: 1.484964  [20816/60600]
Training loss: 1.651336  [22416/60600]
Training loss: 1.601213  [24016/60600]
Training loss: 1.726103  [25616/60600]
Training loss: 2.553643  [27216/60600]
Training loss: 1.950170  [28816/60600]
Training loss: 1.639125  [30416/60600]
Training loss: 2.673219  [32016/60600]
Training loss: 2.196169  [33616/60600]
Training loss: 2.192693  [35216/60600]
Training loss: 1.483039  [36816/60600]
Training loss: 1.591177  [38416/60600]
Training loss: 1.924736  [40016/60600]
Training loss: 2.312495  [41616/60600]
Training loss: 3.201698  [43216/60600]
Training loss: 1.266340  [44816/60600]
Training loss: 1.871530  [46416/60600]
Training loss: 3.008218  [48016/60600]
Training loss: 2.102362  [49616/60600]
Training loss: 1.568591  [51216/60600]
Training loss: 1.477675  [52816/60600]
Training loss: 2.020168  [54416/60600]
Training loss: 2.396587  [56016/60600]
Training loss: 2.405307  [57616/60600]
Training loss: 1.892239  [59216/60600]
Training accuracy: 53.07 %
Validation loss: 1.960237
Validation accuracy: 51.45% 

Epoch 39
-------------------------------
Training loss: 1.670976  [16/60600]
Training loss: 1.722742  [1616/60600]
Training loss: 1.710543  [3216/60600]
Training loss: 1.837416  [4816/60600]
Training loss: 1.190645  [6416/60600]
Training loss: 2.644262  [8016/60600]
Training loss: 1.781779  [9616/60600]
Training loss: 2.935191  [11216/60600]
Training loss: 1.952707  [12816/60600]
Training loss: 1.636374  [14416/60600]
Training loss: 1.896130  [16016/60600]
Training loss: 1.905292  [17616/60600]
Training loss: 1.497283  [19216/60600]
Training loss: 1.134543  [20816/60600]
Training loss: 2.165861  [22416/60600]
Training loss: 1.790916  [24016/60600]
Training loss: 1.940580  [25616/60600]
Training loss: 1.383225  [27216/60600]
Training loss: 1.711319  [28816/60600]
Training loss: 1.789002  [30416/60600]
Training loss: 2.530239  [32016/60600]
Training loss: 2.539232  [33616/60600]
Training loss: 2.203758  [35216/60600]
Training loss: 2.162812  [36816/60600]
Training loss: 1.673903  [38416/60600]
Training loss: 2.063940  [40016/60600]
Training loss: 1.310610  [41616/60600]
Training loss: 2.714533  [43216/60600]
Training loss: 1.715953  [44816/60600]
Training loss: 1.350113  [46416/60600]
Training loss: 1.093157  [48016/60600]
Training loss: 1.994243  [49616/60600]
Training loss: 1.968370  [51216/60600]
Training loss: 2.035364  [52816/60600]
Training loss: 3.561018  [54416/60600]
Training loss: 1.772539  [56016/60600]
Training loss: 2.285144  [57616/60600]
Training loss: 1.686238  [59216/60600]
Training accuracy: 53.44 %
Validation loss: 1.931030
Validation accuracy: 52.07% 

Epoch 40
-------------------------------
Training loss: 1.659115  [16/60600]
Training loss: 2.495415  [1616/60600]
Training loss: 1.619410  [3216/60600]
Training loss: 1.706728  [4816/60600]
Training loss: 1.131570  [6416/60600]
Training loss: 1.559610  [8016/60600]
Training loss: 2.385256  [9616/60600]
Training loss: 1.753599  [11216/60600]
Training loss: 1.466489  [12816/60600]
Training loss: 1.250388  [14416/60600]
Training loss: 1.473380  [16016/60600]
Training loss: 1.253687  [17616/60600]
Training loss: 2.138239  [19216/60600]
Training loss: 2.337712  [20816/60600]
Training loss: 2.405293  [22416/60600]
Training loss: 2.218369  [24016/60600]
Training loss: 1.467091  [25616/60600]
Training loss: 2.487885  [27216/60600]
Training loss: 1.481181  [28816/60600]
Training loss: 1.454038  [30416/60600]
Training loss: 2.284791  [32016/60600]
Training loss: 1.459858  [33616/60600]
Training loss: 1.648327  [35216/60600]
Training loss: 1.733667  [36816/60600]
Training loss: 1.808550  [38416/60600]
Training loss: 1.507672  [40016/60600]
Training loss: 1.648272  [41616/60600]
Training loss: 1.521211  [43216/60600]
Training loss: 1.510812  [44816/60600]
Training loss: 1.868100  [46416/60600]
Training loss: 2.162153  [48016/60600]
Training loss: 2.432804  [49616/60600]
Training loss: 1.631840  [51216/60600]
Training loss: 1.800384  [52816/60600]
Training loss: 1.805586  [54416/60600]
Training loss: 2.292987  [56016/60600]
Training loss: 1.250578  [57616/60600]
Training loss: 2.098500  [59216/60600]
Training accuracy: 53.39 %
Validation loss: 1.940872
Validation accuracy: 52.03% 

Epoch 41
-------------------------------
Training loss: 2.354492  [16/60600]
Training loss: 2.700934  [1616/60600]
Training loss: 2.056760  [3216/60600]
Training loss: 1.632670  [4816/60600]
Training loss: 1.834622  [6416/60600]
Training loss: 1.853900  [8016/60600]
Training loss: 1.552974  [9616/60600]
Training loss: 2.338754  [11216/60600]
Training loss: 1.739968  [12816/60600]
Training loss: 1.791648  [14416/60600]
Training loss: 2.072744  [16016/60600]
Training loss: 1.615560  [17616/60600]
Training loss: 1.963424  [19216/60600]
Training loss: 1.745985  [20816/60600]
Training loss: 1.942549  [22416/60600]
Training loss: 1.161033  [24016/60600]
Training loss: 1.463524  [25616/60600]
Training loss: 2.525523  [27216/60600]
Training loss: 1.604024  [28816/60600]
Training loss: 1.740048  [30416/60600]
Training loss: 2.494276  [32016/60600]
Training loss: 1.957803  [33616/60600]
Training loss: 2.213955  [35216/60600]
Training loss: 3.016471  [36816/60600]
Training loss: 2.595162  [38416/60600]
Training loss: 1.612633  [40016/60600]
Training loss: 1.742849  [41616/60600]
Training loss: 2.184099  [43216/60600]
Training loss: 2.148306  [44816/60600]
Training loss: 1.420845  [46416/60600]
Training loss: 1.506349  [48016/60600]
Training loss: 1.533295  [49616/60600]
Training loss: 1.263398  [51216/60600]
Training loss: 1.614813  [52816/60600]
Training loss: 1.561083  [54416/60600]
Training loss: 2.518106  [56016/60600]
Training loss: 2.098398  [57616/60600]
Training loss: 1.744914  [59216/60600]
Training accuracy: 53.41 %
Validation loss: 1.937080
Validation accuracy: 52.06% 

Epoch 42
-------------------------------
Training loss: 2.317849  [16/60600]
Training loss: 1.367246  [1616/60600]
Training loss: 2.290430  [3216/60600]
Training loss: 1.971040  [4816/60600]
Training loss: 2.568207  [6416/60600]
Training loss: 1.736770  [8016/60600]
Training loss: 1.998726  [9616/60600]
Training loss: 2.329315  [11216/60600]
Training loss: 1.520582  [12816/60600]
Training loss: 1.929911  [14416/60600]
Training loss: 2.008054  [16016/60600]
Training loss: 1.720081  [17616/60600]
Training loss: 2.385930  [19216/60600]
Training loss: 2.080044  [20816/60600]
Training loss: 1.810112  [22416/60600]
Training loss: 2.184642  [24016/60600]
Training loss: 1.785953  [25616/60600]
Training loss: 1.693238  [27216/60600]
Training loss: 1.806041  [28816/60600]
Training loss: 1.779592  [30416/60600]
Training loss: 1.824138  [32016/60600]
Training loss: 1.763914  [33616/60600]
Training loss: 1.428023  [35216/60600]
Training loss: 2.779873  [36816/60600]
Training loss: 2.895370  [38416/60600]
Training loss: 2.790163  [40016/60600]
Training loss: 1.632925  [41616/60600]
Training loss: 2.151685  [43216/60600]
Training loss: 1.779794  [44816/60600]
Training loss: 1.892884  [46416/60600]
Training loss: 1.267860  [48016/60600]
Training loss: 1.951482  [49616/60600]
Training loss: 1.713764  [51216/60600]
Training loss: 2.784174  [52816/60600]
Training loss: 1.959348  [54416/60600]
Training loss: 2.238872  [56016/60600]
Training loss: 2.155652  [57616/60600]
Training loss: 2.078655  [59216/60600]
Training accuracy: 53.51 %
Validation loss: 1.929229
Validation accuracy: 52.20% 

Epoch 43
-------------------------------
Training loss: 1.874715  [16/60600]
Training loss: 1.643488  [1616/60600]
Training loss: 1.838586  [3216/60600]
Training loss: 1.890837  [4816/60600]
Training loss: 2.431679  [6416/60600]
Training loss: 1.958697  [8016/60600]
Training loss: 1.729113  [9616/60600]
Training loss: 1.434640  [11216/60600]
Training loss: 1.457391  [12816/60600]
Training loss: 1.901878  [14416/60600]
Training loss: 2.155187  [16016/60600]
Training loss: 2.422595  [17616/60600]
Training loss: 2.654237  [19216/60600]
Training loss: 2.111351  [20816/60600]
Training loss: 1.794014  [22416/60600]
Training loss: 1.258881  [24016/60600]
Training loss: 1.165616  [25616/60600]
Training loss: 1.614492  [27216/60600]
Training loss: 2.339442  [28816/60600]
Training loss: 1.787342  [30416/60600]
Training loss: 2.165925  [32016/60600]
Training loss: 1.018625  [33616/60600]
Training loss: 2.199816  [35216/60600]
Training loss: 1.810440  [36816/60600]
Training loss: 1.734642  [38416/60600]
Training loss: 2.219190  [40016/60600]
Training loss: 1.941711  [41616/60600]
Training loss: 1.330155  [43216/60600]
Training loss: 1.523344  [44816/60600]
Training loss: 2.231152  [46416/60600]
Training loss: 1.341268  [48016/60600]
Training loss: 2.227150  [49616/60600]
Training loss: 2.173986  [51216/60600]
Training loss: 1.924964  [52816/60600]
Training loss: 1.470694  [54416/60600]
Training loss: 1.895821  [56016/60600]
Training loss: 2.160951  [57616/60600]
Training loss: 1.801722  [59216/60600]
Training accuracy: 53.40 %
Validation loss: 1.935920
Validation accuracy: 52.19% 

Epoch 44
-------------------------------
Training loss: 2.423493  [16/60600]
Training loss: 1.563498  [1616/60600]
Training loss: 1.480087  [3216/60600]
Training loss: 1.220070  [4816/60600]
Training loss: 1.319046  [6416/60600]
Training loss: 1.793747  [8016/60600]
Training loss: 2.537918  [9616/60600]
Training loss: 2.276826  [11216/60600]
Training loss: 1.940734  [12816/60600]
Training loss: 1.684653  [14416/60600]
Training loss: 2.127651  [16016/60600]
Training loss: 1.212408  [17616/60600]
Training loss: 1.140834  [19216/60600]
Training loss: 1.730613  [20816/60600]
Training loss: 2.039991  [22416/60600]
Training loss: 1.961315  [24016/60600]
Training loss: 2.387612  [25616/60600]
Training loss: 2.407412  [27216/60600]
Training loss: 2.465674  [28816/60600]
Training loss: 2.335124  [30416/60600]
Training loss: 1.299729  [32016/60600]
Training loss: 2.685862  [33616/60600]
Training loss: 1.964834  [35216/60600]
Training loss: 1.808868  [36816/60600]
Training loss: 1.898107  [38416/60600]
Training loss: 2.060517  [40016/60600]
Training loss: 1.657007  [41616/60600]
Training loss: 2.429994  [43216/60600]
Training loss: 1.133483  [44816/60600]
Training loss: 2.446195  [46416/60600]
Training loss: 2.247818  [48016/60600]
Training loss: 2.379706  [49616/60600]
Training loss: 2.662629  [51216/60600]
Training loss: 2.095871  [52816/60600]
Training loss: 2.019454  [54416/60600]
Training loss: 1.777567  [56016/60600]
Training loss: 1.519311  [57616/60600]
Training loss: 1.869218  [59216/60600]
Training accuracy: 53.57 %
Validation loss: 1.921688
Validation accuracy: 52.45% 

Epoch 45
-------------------------------
Training loss: 1.991420  [16/60600]
Training loss: 1.920304  [1616/60600]
Training loss: 1.746050  [3216/60600]
Training loss: 1.763746  [4816/60600]
Training loss: 1.248772  [6416/60600]
Training loss: 2.293004  [8016/60600]
Training loss: 1.846875  [9616/60600]
Training loss: 2.437183  [11216/60600]
Training loss: 1.876290  [12816/60600]
Training loss: 1.547977  [14416/60600]
Training loss: 2.973155  [16016/60600]
Training loss: 1.452050  [17616/60600]
Training loss: 1.954322  [19216/60600]
Training loss: 1.682221  [20816/60600]
Training loss: 1.010866  [22416/60600]
Training loss: 2.641767  [24016/60600]
Training loss: 2.416274  [25616/60600]
Training loss: 2.054059  [27216/60600]
Training loss: 1.242531  [28816/60600]
Training loss: 1.601434  [30416/60600]
Training loss: 1.160937  [32016/60600]
Training loss: 1.960655  [33616/60600]
Training loss: 2.026032  [35216/60600]
Training loss: 2.447723  [36816/60600]
Training loss: 2.010693  [38416/60600]
Training loss: 1.700160  [40016/60600]
Training loss: 1.904551  [41616/60600]
Training loss: 1.535326  [43216/60600]
Training loss: 2.095709  [44816/60600]
Training loss: 1.891963  [46416/60600]
Training loss: 1.507420  [48016/60600]
Training loss: 1.786883  [49616/60600]
Training loss: 1.981199  [51216/60600]
Training loss: 2.286636  [52816/60600]
Training loss: 1.153704  [54416/60600]
Training loss: 1.493702  [56016/60600]
Training loss: 2.258152  [57616/60600]
Training loss: 2.154438  [59216/60600]
Training accuracy: 53.71 %
Validation loss: 1.931807
Validation accuracy: 52.01% 

Epoch 46
-------------------------------
Training loss: 1.514054  [16/60600]
Training loss: 1.189719  [1616/60600]
Training loss: 1.151745  [3216/60600]
Training loss: 1.486360  [4816/60600]
Training loss: 1.196389  [6416/60600]
Training loss: 1.781075  [8016/60600]
Training loss: 1.508190  [9616/60600]
Training loss: 2.836035  [11216/60600]
Training loss: 2.659643  [12816/60600]
Training loss: 1.108482  [14416/60600]
Training loss: 2.006055  [16016/60600]
Training loss: 2.170012  [17616/60600]
Training loss: 2.165788  [19216/60600]
Training loss: 2.250556  [20816/60600]
Training loss: 1.754596  [22416/60600]
Training loss: 2.061222  [24016/60600]
Training loss: 1.655536  [25616/60600]
Training loss: 2.096738  [27216/60600]
Training loss: 1.641938  [28816/60600]
Training loss: 1.842629  [30416/60600]
Training loss: 1.571374  [32016/60600]
Training loss: 1.472326  [33616/60600]
Training loss: 2.151057  [35216/60600]
Training loss: 1.370144  [36816/60600]
Training loss: 2.317498  [38416/60600]
Training loss: 2.098268  [40016/60600]
Training loss: 1.676416  [41616/60600]
Training loss: 1.713115  [43216/60600]
Training loss: 1.545862  [44816/60600]
Training loss: 2.685589  [46416/60600]
Training loss: 1.847912  [48016/60600]
Training loss: 1.951439  [49616/60600]
Training loss: 1.946094  [51216/60600]
Training loss: 2.165431  [52816/60600]
Training loss: 1.027993  [54416/60600]
Training loss: 1.262234  [56016/60600]
Training loss: 2.581026  [57616/60600]
Training loss: 2.934530  [59216/60600]
Training accuracy: 53.64 %
Validation loss: 1.936805
Validation accuracy: 51.62% 

Epoch 47
-------------------------------
Training loss: 2.306231  [16/60600]
Training loss: 1.677102  [1616/60600]
Training loss: 1.595735  [3216/60600]
Training loss: 2.155498  [4816/60600]
Training loss: 1.472807  [6416/60600]
Training loss: 1.538111  [8016/60600]
Training loss: 1.880863  [9616/60600]
Training loss: 2.246648  [11216/60600]
Training loss: 2.386714  [12816/60600]
Training loss: 2.035299  [14416/60600]
Training loss: 1.814578  [16016/60600]
Training loss: 2.375983  [17616/60600]
Training loss: 1.715445  [19216/60600]
Training loss: 1.732361  [20816/60600]
Training loss: 2.240104  [22416/60600]
Training loss: 2.485608  [24016/60600]
Training loss: 1.528036  [25616/60600]
Training loss: 2.161370  [27216/60600]
Training loss: 2.091347  [28816/60600]
Training loss: 2.332461  [30416/60600]
Training loss: 1.826812  [32016/60600]
Training loss: 1.717724  [33616/60600]
Training loss: 1.243560  [35216/60600]
Training loss: 2.048321  [36816/60600]
Training loss: 2.283855  [38416/60600]
Training loss: 1.247059  [40016/60600]
Training loss: 2.051683  [41616/60600]
Training loss: 1.540895  [43216/60600]
Training loss: 1.814463  [44816/60600]
Training loss: 1.937756  [46416/60600]
Training loss: 1.933831  [48016/60600]
Training loss: 1.827432  [49616/60600]
Training loss: 2.223962  [51216/60600]
Training loss: 1.917163  [52816/60600]
Training loss: 2.208862  [54416/60600]
Training loss: 2.277673  [56016/60600]
Training loss: 1.336681  [57616/60600]
Training loss: 1.669727  [59216/60600]
Training accuracy: 53.84 %
Validation loss: 1.923801
Validation accuracy: 52.28% 

Epoch 48
-------------------------------
Training loss: 1.804334  [16/60600]
Training loss: 1.536197  [1616/60600]
Training loss: 2.108003  [3216/60600]
Training loss: 1.830538  [4816/60600]
Training loss: 1.912663  [6416/60600]
Training loss: 2.503130  [8016/60600]
Training loss: 2.146010  [9616/60600]
Training loss: 1.462675  [11216/60600]
Training loss: 1.989700  [12816/60600]
Training loss: 1.410134  [14416/60600]
Training loss: 1.375446  [16016/60600]
Training loss: 2.035841  [17616/60600]
Training loss: 1.632393  [19216/60600]
Training loss: 2.458434  [20816/60600]
Training loss: 1.710128  [22416/60600]
Training loss: 1.212548  [24016/60600]
Training loss: 1.884217  [25616/60600]
Training loss: 1.606631  [27216/60600]
Training loss: 2.339534  [28816/60600]
Training loss: 1.603970  [30416/60600]
Training loss: 1.363892  [32016/60600]
Training loss: 2.436815  [33616/60600]
Training loss: 1.946688  [35216/60600]
Training loss: 1.496076  [36816/60600]
Training loss: 1.799152  [38416/60600]
Training loss: 1.110285  [40016/60600]
Training loss: 1.482749  [41616/60600]
Training loss: 2.371918  [43216/60600]
Training loss: 1.909920  [44816/60600]
Training loss: 1.675298  [46416/60600]
Training loss: 1.446602  [48016/60600]
Training loss: 1.724422  [49616/60600]
Training loss: 1.840318  [51216/60600]
Training loss: 1.865536  [52816/60600]
Training loss: 2.088923  [54416/60600]
Training loss: 1.903365  [56016/60600]
Training loss: 1.059638  [57616/60600]
Training loss: 1.988233  [59216/60600]
Training accuracy: 54.13 %
Validation loss: 1.908019
Validation accuracy: 52.35% 

Epoch 49
-------------------------------
Training loss: 2.205351  [16/60600]
Training loss: 1.315389  [1616/60600]
Training loss: 2.545296  [3216/60600]
Training loss: 2.028831  [4816/60600]
Training loss: 1.070222  [6416/60600]
Training loss: 2.069154  [8016/60600]
Training loss: 2.145623  [9616/60600]
Training loss: 1.471188  [11216/60600]
Training loss: 1.660129  [12816/60600]
Training loss: 1.586036  [14416/60600]
Training loss: 2.020823  [16016/60600]
Training loss: 1.760232  [17616/60600]
Training loss: 1.916582  [19216/60600]
Training loss: 2.110679  [20816/60600]
Training loss: 1.582278  [22416/60600]
Training loss: 1.920577  [24016/60600]
Training loss: 2.052599  [25616/60600]
Training loss: 2.329331  [27216/60600]
Training loss: 2.056062  [28816/60600]
Training loss: 1.807405  [30416/60600]
Training loss: 1.497224  [32016/60600]
Training loss: 2.363737  [33616/60600]
Training loss: 2.449171  [35216/60600]
Training loss: 1.571916  [36816/60600]
Training loss: 2.040181  [38416/60600]
Training loss: 1.461068  [40016/60600]
Training loss: 1.662810  [41616/60600]
Training loss: 1.554528  [43216/60600]
Training loss: 1.446146  [44816/60600]
Training loss: 1.916901  [46416/60600]
Training loss: 1.467424  [48016/60600]
Training loss: 2.296358  [49616/60600]
Training loss: 2.044493  [51216/60600]
Training loss: 2.783810  [52816/60600]
Training loss: 2.619654  [54416/60600]
Training loss: 1.833638  [56016/60600]
Training loss: 2.245156  [57616/60600]
Training loss: 1.462991  [59216/60600]
Training accuracy: 53.86 %
Validation loss: 1.921946
Validation accuracy: 52.37% 

Early stopping
Done!

Elapsed time: 35527.15813803673 seconds

Current time: 19:00:14
                         precision    recall  f1-score   support

              apple_pie       0.39      0.17      0.24       250
         baby_back_ribs       0.58      0.58      0.58       250
                baklava       0.49      0.55      0.52       250
         beef_carpaccio       0.57      0.57      0.57       250
           beef_tartare       0.40      0.40      0.40       250
             beet_salad       0.43      0.41      0.42       250
               beignets       0.65      0.73      0.68       250
               bibimbap       0.66      0.77      0.71       250
          bread_pudding       0.39      0.26      0.32       250
      breakfast_burrito       0.47      0.44      0.45       250
             bruschetta       0.44      0.38      0.41       250
           caesar_salad       0.52      0.52      0.52       250
                cannoli       0.58      0.61      0.59       250
          caprese_salad       0.48      0.55      0.51       250
            carrot_cake       0.57      0.50      0.53       250
                ceviche       0.46      0.18      0.26       250
             cheesecake       0.52      0.52      0.52       250
           cheese_plate       0.48      0.38      0.43       250
          chicken_curry       0.46      0.39      0.42       250
     chicken_quesadilla       0.49      0.61      0.54       250
          chicken_wings       0.64      0.65      0.65       250
         chocolate_cake       0.56      0.46      0.50       250
       chocolate_mousse       0.45      0.42      0.44       250
                churros       0.65      0.72      0.68       250
           clam_chowder       0.64      0.76      0.69       250
          club_sandwich       0.59      0.60      0.60       250
             crab_cakes       0.33      0.40      0.36       250
           creme_brulee       0.64      0.73      0.68       250
          croque_madame       0.59      0.56      0.57       250
              cup_cakes       0.73      0.78      0.75       250
           deviled_eggs       0.61      0.71      0.66       250
                 donuts       0.60      0.70      0.64       250
              dumplings       0.91      0.82      0.86       250
                edamame       0.92      0.94      0.93       250
          eggs_benedict       0.57      0.70      0.62       250
              escargots       0.62      0.74      0.68       250
                falafel       0.45      0.46      0.46       250
           filet_mignon       0.37      0.29      0.33       250
         fish_and_chips       0.61      0.66      0.63       250
              foie_gras       0.38      0.27      0.32       250
           french_fries       0.64      0.82      0.72       250
      french_onion_soup       0.61      0.67      0.64       250
           french_toast       0.54      0.32      0.40       250
         fried_calamari       0.53      0.60      0.56       250
             fried_rice       0.56      0.64      0.60       250
          frozen_yogurt       0.75      0.74      0.74       250
           garlic_bread       0.44      0.58      0.50       250
                gnocchi       0.48      0.30      0.37       250
            greek_salad       0.44      0.64      0.52       250
grilled_cheese_sandwich       0.38      0.42      0.40       250
         grilled_salmon       0.51      0.35      0.42       250
              guacamole       0.71      0.76      0.73       250
                  gyoza       0.56      0.68      0.61       250
              hamburger       0.58      0.59      0.58       250
      hot_and_sour_soup       0.77      0.82      0.79       250
                hot_dog       0.69      0.68      0.69       250
       huevos_rancheros       0.43      0.27      0.33       250
                 hummus       0.55      0.32      0.40       250
              ice_cream       0.67      0.60      0.63       250
                lasagna       0.47      0.48      0.48       250
         lobster_bisque       0.66      0.76      0.71       250
  lobster_roll_sandwich       0.58      0.56      0.57       250
    macaroni_and_cheese       0.51      0.57      0.54       250
               macarons       0.79      0.90      0.84       250
              miso_soup       0.75      0.77      0.76       250
                mussels       0.74      0.80      0.77       250
                 nachos       0.47      0.43      0.45       250
               omelette       0.43      0.37      0.40       250
            onion_rings       0.70      0.79      0.74       250
                oysters       0.71      0.86      0.78       250
               pad_thai       0.56      0.79      0.66       250
                 paella       0.59      0.54      0.56       250
               pancakes       0.60      0.57      0.59       250
            panna_cotta       0.49      0.52      0.50       250
            peking_duck       0.61      0.58      0.59       250
                    pho       0.77      0.79      0.78       250
                  pizza       0.58      0.82      0.68       250
              pork_chop       0.45      0.24      0.31       250
                poutine       0.75      0.64      0.69       250
              prime_rib       0.55      0.62      0.58       250
   pulled_pork_sandwich       0.48      0.50      0.49       250
                  ramen       0.58      0.57      0.58       250
                ravioli       0.45      0.31      0.36       250
        red_velvet_cake       0.57      0.76      0.65       250
                risotto       0.47      0.43      0.45       250
                 samosa       0.60      0.53      0.56       250
                sashimi       0.68      0.75      0.71       250
               scallops       0.40      0.37      0.39       250
          seaweed_salad       0.69      0.75      0.72       250
       shrimp_and_grits       0.47      0.48      0.48       250
    spaghetti_bolognese       0.70      0.67      0.69       250
    spaghetti_carbonara       0.79      0.78      0.79       250
           spring_rolls       0.64      0.63      0.63       250
                  steak       0.37      0.26      0.31       250
   strawberry_shortcake       0.55      0.60      0.57       250
                  sushi       0.51      0.60      0.55       250
                  tacos       0.32      0.36      0.34       250
               takoyaki       0.47      0.61      0.53       250
               tiramisu       0.45      0.45      0.45       250
           tuna_tartare       0.53      0.24      0.33       250
                waffles       0.63      0.67      0.65       250

               accuracy                           0.57     25250
              macro avg       0.56      0.57      0.56     25250
           weighted avg       0.56      0.57      0.56     25250

Test accuracy: 0.5688712871287128
The metadata of the previous execution is...
{'training_images_percentage': 1, 'epochs': 55, 'learning_rate': 0.001, 'batch_size': 16, 'data_augmentation': 'no', 'subject_driven_technique': 'dreambooth', 'number_of_samples': 5, 'images_to_generate': 0, 'path_to_dataset': '../../../../../../work3/s226536/datasets/food-101', 'DATA_DIR': '../../../../../../work3/s226536/datasets'}

Preparing next execution...
Finished preparing next execution...
