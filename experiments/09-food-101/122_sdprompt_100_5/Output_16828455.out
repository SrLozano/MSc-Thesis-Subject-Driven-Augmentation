60600
15150
25250
-------------------------------------
Generating 5 images for food apple_pie...

Not using data augmentation
Using cuda device
Epoch 1
-------------------------------
Training loss: 4.699374  [16/60600]
Training loss: 4.707386  [1616/60600]
Training loss: 4.804182  [3216/60600]
Training loss: 4.731719  [4816/60600]
Training loss: 4.614886  [6416/60600]
Training loss: 4.682510  [8016/60600]
Training loss: 4.625140  [9616/60600]
Training loss: 4.634161  [11216/60600]
Training loss: 4.474886  [12816/60600]
Training loss: 4.345427  [14416/60600]
Training loss: 4.578359  [16016/60600]
Training loss: 4.530307  [17616/60600]
Training loss: 4.442148  [19216/60600]
Training loss: 4.335681  [20816/60600]
Training loss: 4.321664  [22416/60600]
Training loss: 4.506327  [24016/60600]
Training loss: 4.405070  [25616/60600]
Training loss: 4.357219  [27216/60600]
Training loss: 4.229008  [28816/60600]
Training loss: 4.356940  [30416/60600]
Training loss: 4.329016  [32016/60600]
Training loss: 4.162342  [33616/60600]
Training loss: 4.187919  [35216/60600]
Training loss: 4.187721  [36816/60600]
Training loss: 4.102970  [38416/60600]
Training loss: 4.013962  [40016/60600]
Training loss: 3.969772  [41616/60600]
Training loss: 4.045770  [43216/60600]
Training loss: 4.080130  [44816/60600]
Training loss: 3.934729  [46416/60600]
Training loss: 4.035223  [48016/60600]
Training loss: 3.602160  [49616/60600]
Training loss: 4.118863  [51216/60600]
Training loss: 3.847446  [52816/60600]
Training loss: 4.072036  [54416/60600]
Training loss: 3.916498  [56016/60600]
Training loss: 4.038065  [57616/60600]
Training loss: 3.739721  [59216/60600]
Training accuracy: 20.00 %
Validation loss: 3.887791
Validation accuracy: 21.12% 

Epoch 2
-------------------------------
Training loss: 3.734714  [16/60600]
Training loss: 3.908501  [1616/60600]
Training loss: 3.535322  [3216/60600]
Training loss: 3.903622  [4816/60600]
Training loss: 3.770724  [6416/60600]
Training loss: 4.023569  [8016/60600]
Training loss: 3.796625  [9616/60600]
Training loss: 4.012518  [11216/60600]
Training loss: 3.582933  [12816/60600]
Training loss: 3.634475  [14416/60600]
Training loss: 4.088883  [16016/60600]
Training loss: 3.809052  [17616/60600]
Training loss: 3.576135  [19216/60600]
Training loss: 3.903585  [20816/60600]
Training loss: 3.858628  [22416/60600]
Training loss: 3.753915  [24016/60600]
Training loss: 3.827956  [25616/60600]
Training loss: 3.753946  [27216/60600]
Training loss: 3.297542  [28816/60600]
Training loss: 3.701051  [30416/60600]
Training loss: 3.931975  [32016/60600]
Training loss: 3.718106  [33616/60600]
Training loss: 3.461332  [35216/60600]
Training loss: 3.490996  [36816/60600]
Training loss: 3.705232  [38416/60600]
Training loss: 3.576368  [40016/60600]
Training loss: 3.527673  [41616/60600]
Training loss: 3.495034  [43216/60600]
Training loss: 3.449598  [44816/60600]
Training loss: 3.305688  [46416/60600]
Training loss: 3.574651  [48016/60600]
Training loss: 3.230689  [49616/60600]
Training loss: 3.869762  [51216/60600]
Training loss: 3.030434  [52816/60600]
Training loss: 3.192815  [54416/60600]
Training loss: 3.289753  [56016/60600]
Training loss: 3.594196  [57616/60600]
Training loss: 3.624839  [59216/60600]
Training accuracy: 31.05 %
Validation loss: 3.354583
Validation accuracy: 32.10% 

Epoch 3
-------------------------------
Training loss: 3.210893  [16/60600]
Training loss: 3.196749  [1616/60600]
Training loss: 3.455868  [3216/60600]
Training loss: 3.074085  [4816/60600]
Training loss: 3.176894  [6416/60600]
Training loss: 3.054150  [8016/60600]
Training loss: 3.238728  [9616/60600]
Training loss: 3.727318  [11216/60600]
Training loss: 3.775219  [12816/60600]
Training loss: 3.628854  [14416/60600]
Training loss: 3.296077  [16016/60600]
Training loss: 3.283391  [17616/60600]
Training loss: 3.426252  [19216/60600]
Training loss: 3.401287  [20816/60600]
Training loss: 3.279218  [22416/60600]
Training loss: 2.638242  [24016/60600]
Training loss: 3.481667  [25616/60600]
Training loss: 3.279752  [27216/60600]
Training loss: 3.345487  [28816/60600]
Training loss: 2.966398  [30416/60600]
Training loss: 3.115353  [32016/60600]
Training loss: 3.381871  [33616/60600]
Training loss: 3.168547  [35216/60600]
Training loss: 3.413727  [36816/60600]
Training loss: 3.521913  [38416/60600]
Training loss: 3.163160  [40016/60600]
Training loss: 3.318184  [41616/60600]
Training loss: 2.696784  [43216/60600]
Training loss: 3.392546  [44816/60600]
Training loss: 3.085187  [46416/60600]
Training loss: 3.330485  [48016/60600]
Training loss: 3.715230  [49616/60600]
Training loss: 3.385640  [51216/60600]
Training loss: 3.301295  [52816/60600]
Training loss: 2.922278  [54416/60600]
Training loss: 2.637653  [56016/60600]
Training loss: 3.168514  [57616/60600]
Training loss: 2.821383  [59216/60600]
Training accuracy: 36.78 %
Validation loss: 3.016396
Validation accuracy: 37.27% 

Epoch 4
-------------------------------
Training loss: 2.970798  [16/60600]
Training loss: 3.365759  [1616/60600]
Training loss: 3.247102  [3216/60600]
Training loss: 2.879866  [4816/60600]
Training loss: 3.267914  [6416/60600]
Training loss: 3.637324  [8016/60600]
Training loss: 3.610135  [9616/60600]
Training loss: 2.720497  [11216/60600]
Training loss: 3.882730  [12816/60600]
Training loss: 2.833276  [14416/60600]
Training loss: 3.198764  [16016/60600]
Training loss: 2.992506  [17616/60600]
Training loss: 2.923234  [19216/60600]
Training loss: 2.933322  [20816/60600]
Training loss: 3.349230  [22416/60600]
Training loss: 3.245847  [24016/60600]
Training loss: 2.602256  [25616/60600]
Training loss: 3.042583  [27216/60600]
Training loss: 3.495589  [28816/60600]
Training loss: 2.545231  [30416/60600]
Training loss: 2.787262  [32016/60600]
Training loss: 2.655949  [33616/60600]
Training loss: 3.105274  [35216/60600]
Training loss: 3.018623  [36816/60600]
Training loss: 2.671143  [38416/60600]
Training loss: 2.694372  [40016/60600]
Training loss: 2.770494  [41616/60600]
Training loss: 3.194736  [43216/60600]
Training loss: 2.672961  [44816/60600]
Training loss: 3.562806  [46416/60600]
Training loss: 2.721839  [48016/60600]
Training loss: 2.881699  [49616/60600]
Training loss: 3.104998  [51216/60600]
Training loss: 3.378448  [52816/60600]
Training loss: 2.661622  [54416/60600]
Training loss: 2.651634  [56016/60600]
Training loss: 2.728659  [57616/60600]
Training loss: 2.459518  [59216/60600]
Training accuracy: 39.80 %
Validation loss: 2.793280
Validation accuracy: 40.26% 

Epoch 5
-------------------------------
Training loss: 2.573628  [16/60600]
Training loss: 2.948823  [1616/60600]
Training loss: 2.471519  [3216/60600]
Training loss: 2.861178  [4816/60600]
Training loss: 2.473471  [6416/60600]
Training loss: 3.166163  [8016/60600]
Training loss: 3.045461  [9616/60600]
Training loss: 3.329941  [11216/60600]
Training loss: 3.241635  [12816/60600]
Training loss: 2.951770  [14416/60600]
Training loss: 2.611144  [16016/60600]
Training loss: 2.751007  [17616/60600]
Training loss: 2.857590  [19216/60600]
Training loss: 2.444769  [20816/60600]
Training loss: 2.595782  [22416/60600]
Training loss: 2.879723  [24016/60600]
Training loss: 2.420087  [25616/60600]
Training loss: 2.461164  [27216/60600]
Training loss: 3.003430  [28816/60600]
Training loss: 2.807798  [30416/60600]
Training loss: 3.232199  [32016/60600]
Training loss: 3.230675  [33616/60600]
Training loss: 2.719053  [35216/60600]
Training loss: 2.588513  [36816/60600]
Training loss: 3.132008  [38416/60600]
Training loss: 3.127273  [40016/60600]
Training loss: 3.034324  [41616/60600]
Training loss: 2.933191  [43216/60600]
Training loss: 2.612571  [44816/60600]
Training loss: 2.533880  [46416/60600]
Training loss: 2.653445  [48016/60600]
Training loss: 2.396647  [49616/60600]
Training loss: 2.763100  [51216/60600]
Training loss: 2.435959  [52816/60600]
Training loss: 2.561955  [54416/60600]
Training loss: 2.158142  [56016/60600]
Training loss: 3.291966  [57616/60600]
Training loss: 2.888599  [59216/60600]
Training accuracy: 41.89 %
Validation loss: 2.636087
Validation accuracy: 42.36% 

Epoch 6
-------------------------------
Training loss: 2.562319  [16/60600]
Training loss: 2.481377  [1616/60600]
Training loss: 2.598826  [3216/60600]
Training loss: 2.126061  [4816/60600]
Training loss: 2.875612  [6416/60600]
Training loss: 2.878364  [8016/60600]
Training loss: 2.836843  [9616/60600]
Training loss: 2.868187  [11216/60600]
Training loss: 3.000757  [12816/60600]
Training loss: 2.056064  [14416/60600]
Training loss: 2.397842  [16016/60600]
Training loss: 2.526162  [17616/60600]
Training loss: 2.548990  [19216/60600]
Training loss: 2.416862  [20816/60600]
Training loss: 2.756027  [22416/60600]
Training loss: 2.870803  [24016/60600]
Training loss: 2.312413  [25616/60600]
Training loss: 2.303099  [27216/60600]
Training loss: 3.498565  [28816/60600]
Training loss: 2.729065  [30416/60600]
Training loss: 3.115148  [32016/60600]
Training loss: 2.406083  [33616/60600]
Training loss: 2.875677  [35216/60600]
Training loss: 2.181942  [36816/60600]
Training loss: 2.338716  [38416/60600]
Training loss: 2.229306  [40016/60600]
Training loss: 2.757306  [41616/60600]
Training loss: 2.695072  [43216/60600]
Training loss: 2.340745  [44816/60600]
Training loss: 2.329463  [46416/60600]
Training loss: 2.390924  [48016/60600]
Training loss: 2.945894  [49616/60600]
Training loss: 3.283522  [51216/60600]
Training loss: 2.746211  [52816/60600]
Training loss: 2.947942  [54416/60600]
Training loss: 2.380770  [56016/60600]
Training loss: 2.946183  [57616/60600]
Training loss: 2.500504  [59216/60600]
Training accuracy: 43.30 %
Validation loss: 2.515064
Validation accuracy: 43.87% 

Epoch 7
-------------------------------
Training loss: 2.637029  [16/60600]
Training loss: 2.202098  [1616/60600]
Training loss: 2.341794  [3216/60600]
Training loss: 2.542856  [4816/60600]
Training loss: 2.564613  [6416/60600]
Training loss: 2.662736  [8016/60600]
Training loss: 2.347754  [9616/60600]
Training loss: 2.289458  [11216/60600]
Training loss: 3.005072  [12816/60600]
Training loss: 2.544608  [14416/60600]
Training loss: 2.978873  [16016/60600]
Training loss: 2.406579  [17616/60600]
Training loss: 2.570388  [19216/60600]
Training loss: 1.616147  [20816/60600]
Training loss: 2.675111  [22416/60600]
Training loss: 3.221099  [24016/60600]
Training loss: 1.907820  [25616/60600]
Training loss: 3.020713  [27216/60600]
Training loss: 3.011321  [28816/60600]
Training loss: 2.286526  [30416/60600]
Training loss: 3.098124  [32016/60600]
Training loss: 2.487961  [33616/60600]
Training loss: 2.447969  [35216/60600]
Training loss: 2.144398  [36816/60600]
Training loss: 2.577599  [38416/60600]
Training loss: 2.546795  [40016/60600]
Training loss: 2.595168  [41616/60600]
Training loss: 2.018925  [43216/60600]
Training loss: 2.622454  [44816/60600]
Training loss: 3.130249  [46416/60600]
Training loss: 2.404073  [48016/60600]
Training loss: 2.285127  [49616/60600]
Training loss: 2.469153  [51216/60600]
Training loss: 2.067663  [52816/60600]
Training loss: 2.742553  [54416/60600]
Training loss: 3.034244  [56016/60600]
Training loss: 1.927770  [57616/60600]
Training loss: 2.342301  [59216/60600]
Training accuracy: 44.77 %
Validation loss: 2.446592
Validation accuracy: 44.50% 

Epoch 8
-------------------------------
Training loss: 2.195228  [16/60600]
Training loss: 1.984523  [1616/60600]
Training loss: 2.905634  [3216/60600]
Training loss: 2.429231  [4816/60600]
Training loss: 2.338289  [6416/60600]
Training loss: 2.084485  [8016/60600]
Training loss: 1.277119  [9616/60600]
Training loss: 3.103840  [11216/60600]
Training loss: 2.128699  [12816/60600]
Training loss: 2.222431  [14416/60600]
Training loss: 2.608769  [16016/60600]
Training loss: 2.061367  [17616/60600]
Training loss: 2.835538  [19216/60600]
Training loss: 3.061175  [20816/60600]
Training loss: 2.711598  [22416/60600]
Training loss: 2.906742  [24016/60600]
Training loss: 2.481374  [25616/60600]
Training loss: 2.097387  [27216/60600]
Training loss: 1.816019  [28816/60600]
Training loss: 2.327090  [30416/60600]
Training loss: 3.369823  [32016/60600]
Training loss: 2.146486  [33616/60600]
Training loss: 2.617025  [35216/60600]
Training loss: 2.933840  [36816/60600]
Training loss: 2.496893  [38416/60600]
Training loss: 2.372617  [40016/60600]
Training loss: 3.084885  [41616/60600]
Training loss: 1.857509  [43216/60600]
Training loss: 2.348088  [44816/60600]
Training loss: 2.372732  [46416/60600]
Training loss: 2.034105  [48016/60600]
Training loss: 2.383579  [49616/60600]
Training loss: 2.262261  [51216/60600]
Training loss: 2.102715  [52816/60600]
Training loss: 2.801780  [54416/60600]
Training loss: 2.481498  [56016/60600]
Training loss: 2.763555  [57616/60600]
Training loss: 2.596176  [59216/60600]
Training accuracy: 45.27 %
Validation loss: 2.368315
Validation accuracy: 45.48% 

Epoch 9
-------------------------------
Training loss: 2.391848  [16/60600]
Training loss: 2.293140  [1616/60600]
Training loss: 2.316028  [3216/60600]
Training loss: 2.451861  [4816/60600]
Training loss: 2.127494  [6416/60600]
Training loss: 2.536164  [8016/60600]
Training loss: 2.409362  [9616/60600]
Training loss: 2.140768  [11216/60600]
Training loss: 2.515957  [12816/60600]
Training loss: 2.521343  [14416/60600]
Training loss: 2.408261  [16016/60600]
Training loss: 2.157705  [17616/60600]
Training loss: 2.480617  [19216/60600]
Training loss: 1.918105  [20816/60600]
Training loss: 2.212188  [22416/60600]
Training loss: 3.049615  [24016/60600]
Training loss: 2.633897  [25616/60600]
Training loss: 2.642735  [27216/60600]
Training loss: 2.351578  [28816/60600]
Training loss: 2.731747  [30416/60600]
Training loss: 2.022927  [32016/60600]
Training loss: 3.166913  [33616/60600]
Training loss: 2.084167  [35216/60600]
Training loss: 2.015039  [36816/60600]
Training loss: 2.157301  [38416/60600]
Training loss: 3.377629  [40016/60600]
Training loss: 2.964324  [41616/60600]
Training loss: 2.223635  [43216/60600]
Training loss: 3.356285  [44816/60600]
Training loss: 1.952634  [46416/60600]
Training loss: 2.021832  [48016/60600]
Training loss: 2.929167  [49616/60600]
Training loss: 2.276786  [51216/60600]
Training loss: 2.329636  [52816/60600]
Training loss: 2.543127  [54416/60600]
Training loss: 1.948593  [56016/60600]
Training loss: 3.018312  [57616/60600]
Training loss: 2.521578  [59216/60600]
Training accuracy: 46.35 %
Validation loss: 2.304786
Validation accuracy: 46.38% 

Epoch 10
-------------------------------
Training loss: 2.265472  [16/60600]
Training loss: 1.783213  [1616/60600]
Training loss: 2.597794  [3216/60600]
Training loss: 2.574833  [4816/60600]
Training loss: 2.422566  [6416/60600]
Training loss: 2.579497  [8016/60600]
Training loss: 1.775605  [9616/60600]
Training loss: 2.089546  [11216/60600]
Training loss: 2.056193  [12816/60600]
Training loss: 1.766229  [14416/60600]
Training loss: 1.697356  [16016/60600]
Training loss: 1.625175  [17616/60600]
Training loss: 2.181852  [19216/60600]
Training loss: 2.353806  [20816/60600]
Training loss: 1.738030  [22416/60600]
Training loss: 2.699915  [24016/60600]
Training loss: 2.516907  [25616/60600]
Training loss: 2.353051  [27216/60600]
Training loss: 2.207308  [28816/60600]
Training loss: 2.375837  [30416/60600]
Training loss: 2.429050  [32016/60600]
Training loss: 2.319579  [33616/60600]
Training loss: 1.756386  [35216/60600]
Training loss: 2.125859  [36816/60600]
Training loss: 2.459923  [38416/60600]
Training loss: 1.702688  [40016/60600]
Training loss: 1.646126  [41616/60600]
Training loss: 1.894584  [43216/60600]
Training loss: 2.170777  [44816/60600]
Training loss: 2.737281  [46416/60600]
Training loss: 1.491039  [48016/60600]
Training loss: 1.961168  [49616/60600]
Training loss: 2.132119  [51216/60600]
Training loss: 2.084487  [52816/60600]
Training loss: 1.879087  [54416/60600]
Training loss: 2.469403  [56016/60600]
Training loss: 2.416044  [57616/60600]
Training loss: 1.909999  [59216/60600]
Training accuracy: 46.98 %
Validation loss: 2.273025
Validation accuracy: 46.63% 

Epoch 11
-------------------------------
Training loss: 2.238342  [16/60600]
Training loss: 2.385206  [1616/60600]
Training loss: 2.782349  [3216/60600]
Training loss: 2.710226  [4816/60600]
Training loss: 1.909812  [6416/60600]
Training loss: 3.077163  [8016/60600]
Training loss: 2.123884  [9616/60600]
Training loss: 1.924713  [11216/60600]
Training loss: 2.289184  [12816/60600]
Training loss: 2.319985  [14416/60600]
Training loss: 2.624230  [16016/60600]
Training loss: 1.475526  [17616/60600]
Training loss: 2.125617  [19216/60600]
Training loss: 3.279225  [20816/60600]
Training loss: 1.617684  [22416/60600]
Training loss: 1.975521  [24016/60600]
Training loss: 1.989693  [25616/60600]
Training loss: 2.412105  [27216/60600]
Training loss: 2.025873  [28816/60600]
Training loss: 1.862799  [30416/60600]
Training loss: 3.388072  [32016/60600]
Training loss: 1.519392  [33616/60600]
Training loss: 1.905570  [35216/60600]
Training loss: 2.265151  [36816/60600]
Training loss: 2.751010  [38416/60600]
Training loss: 2.401397  [40016/60600]
Training loss: 1.792502  [41616/60600]
Training loss: 2.517571  [43216/60600]
Training loss: 2.946750  [44816/60600]
Training loss: 2.048003  [46416/60600]
Training loss: 2.547516  [48016/60600]
Training loss: 1.639983  [49616/60600]
Training loss: 2.384921  [51216/60600]
Training loss: 2.425733  [52816/60600]
Training loss: 2.153539  [54416/60600]
Training loss: 2.769920  [56016/60600]
Training loss: 2.051845  [57616/60600]
Training loss: 2.445978  [59216/60600]
Training accuracy: 47.03 %
Validation loss: 2.216460
Validation accuracy: 47.63% 

Epoch 12
-------------------------------
Training loss: 2.036546  [16/60600]
Training loss: 1.367326  [1616/60600]
Training loss: 2.482743  [3216/60600]
Training loss: 2.227988  [4816/60600]
Training loss: 2.414969  [6416/60600]
Training loss: 2.324406  [8016/60600]
Training loss: 2.093652  [9616/60600]
Training loss: 2.992767  [11216/60600]
Training loss: 2.026320  [12816/60600]
Training loss: 1.796716  [14416/60600]
Training loss: 2.043499  [16016/60600]
Training loss: 2.603210  [17616/60600]
Training loss: 2.815279  [19216/60600]
Training loss: 1.953552  [20816/60600]
Training loss: 2.275879  [22416/60600]
Training loss: 2.173187  [24016/60600]
Training loss: 2.235220  [25616/60600]
Training loss: 2.297153  [27216/60600]
Training loss: 1.870530  [28816/60600]
Training loss: 2.254577  [30416/60600]
Training loss: 2.705292  [32016/60600]
Training loss: 2.133994  [33616/60600]
Training loss: 2.437479  [35216/60600]
Training loss: 2.695355  [36816/60600]
Training loss: 2.835703  [38416/60600]
Training loss: 2.413939  [40016/60600]
Training loss: 1.926900  [41616/60600]
Training loss: 1.846041  [43216/60600]
Training loss: 2.632031  [44816/60600]
Training loss: 2.792509  [46416/60600]
Training loss: 3.117626  [48016/60600]
Training loss: 2.422203  [49616/60600]
Training loss: 2.228867  [51216/60600]
Training loss: 2.318797  [52816/60600]
Training loss: 3.408461  [54416/60600]
Training loss: 2.007371  [56016/60600]
Training loss: 2.697783  [57616/60600]
Training loss: 2.686887  [59216/60600]
Training accuracy: 47.83 %
Validation loss: 2.190934
Validation accuracy: 47.89% 

Epoch 13
-------------------------------
Training loss: 2.710608  [16/60600]
Training loss: 1.326239  [1616/60600]
Training loss: 1.837800  [3216/60600]
Training loss: 1.660927  [4816/60600]
Training loss: 2.243576  [6416/60600]
Training loss: 1.790348  [8016/60600]
Training loss: 2.008058  [9616/60600]
Training loss: 2.062204  [11216/60600]
Training loss: 2.339350  [12816/60600]
Training loss: 2.156057  [14416/60600]
Training loss: 2.348765  [16016/60600]
Training loss: 2.104652  [17616/60600]
Training loss: 1.839604  [19216/60600]
Training loss: 2.184424  [20816/60600]
Training loss: 1.948971  [22416/60600]
Training loss: 1.768839  [24016/60600]
Training loss: 2.015080  [25616/60600]
Training loss: 2.251453  [27216/60600]
Training loss: 2.716968  [28816/60600]
Training loss: 1.573791  [30416/60600]
Training loss: 2.355024  [32016/60600]
Training loss: 2.795020  [33616/60600]
Training loss: 2.658686  [35216/60600]
Training loss: 2.469787  [36816/60600]
Training loss: 2.347102  [38416/60600]
Training loss: 2.175603  [40016/60600]
Training loss: 2.146525  [41616/60600]
Training loss: 2.299538  [43216/60600]
Training loss: 2.751331  [44816/60600]
Training loss: 2.678237  [46416/60600]
Training loss: 2.467343  [48016/60600]
Training loss: 2.028936  [49616/60600]
Training loss: 2.300499  [51216/60600]
Training loss: 2.807661  [52816/60600]
Training loss: 2.406282  [54416/60600]
Training loss: 1.704175  [56016/60600]
Training loss: 2.487888  [57616/60600]
Training loss: 1.791491  [59216/60600]
Training accuracy: 48.18 %
Validation loss: 2.166915
Validation accuracy: 48.55% 

Epoch 14
-------------------------------
Training loss: 1.330179  [16/60600]
Training loss: 2.547013  [1616/60600]
Training loss: 2.052411  [3216/60600]
Training loss: 2.155372  [4816/60600]
Training loss: 2.235454  [6416/60600]
Training loss: 2.015449  [8016/60600]
Training loss: 2.135288  [9616/60600]
Training loss: 2.801121  [11216/60600]
Training loss: 1.977946  [12816/60600]
Training loss: 2.189280  [14416/60600]
Training loss: 2.350419  [16016/60600]
Training loss: 1.893597  [17616/60600]
Training loss: 3.245662  [19216/60600]
Training loss: 2.462636  [20816/60600]
Training loss: 2.801705  [22416/60600]
Training loss: 2.281234  [24016/60600]
Training loss: 1.955589  [25616/60600]
Training loss: 2.288530  [27216/60600]
Training loss: 2.194171  [28816/60600]
Training loss: 1.955563  [30416/60600]
Training loss: 1.670835  [32016/60600]
Training loss: 3.006012  [33616/60600]
Training loss: 2.017285  [35216/60600]
Training loss: 2.679488  [36816/60600]
Training loss: 2.368786  [38416/60600]
Training loss: 2.608190  [40016/60600]
Training loss: 2.410002  [41616/60600]
Training loss: 1.851378  [43216/60600]
Training loss: 2.140734  [44816/60600]
Training loss: 2.004599  [46416/60600]
Training loss: 2.032409  [48016/60600]
Training loss: 2.092096  [49616/60600]
Training loss: 2.711377  [51216/60600]
Training loss: 2.210467  [52816/60600]
Training loss: 2.453251  [54416/60600]
Training loss: 1.547177  [56016/60600]
Training loss: 1.887608  [57616/60600]
Training loss: 2.525774  [59216/60600]
Training accuracy: 48.77 %
Validation loss: 2.154487
Validation accuracy: 48.39% 

Epoch 15
-------------------------------
Training loss: 1.651048  [16/60600]
Training loss: 1.697864  [1616/60600]
Training loss: 1.567878  [3216/60600]
Training loss: 1.985865  [4816/60600]
Training loss: 1.502285  [6416/60600]
Training loss: 2.053958  [8016/60600]
Training loss: 1.962011  [9616/60600]
Training loss: 1.989048  [11216/60600]
Training loss: 2.068836  [12816/60600]
Training loss: 1.962905  [14416/60600]
Training loss: 2.896907  [16016/60600]
Training loss: 2.674971  [17616/60600]
Training loss: 2.308962  [19216/60600]
Training loss: 2.227027  [20816/60600]
Training loss: 1.770970  [22416/60600]
Training loss: 2.147479  [24016/60600]
Training loss: 1.719310  [25616/60600]
Training loss: 2.024187  [27216/60600]
Training loss: 2.744019  [28816/60600]
Training loss: 3.052994  [30416/60600]
Training loss: 2.666083  [32016/60600]
Training loss: 2.111651  [33616/60600]
Training loss: 1.895374  [35216/60600]
Training loss: 3.146189  [36816/60600]
Training loss: 2.680537  [38416/60600]
Training loss: 1.624122  [40016/60600]
Training loss: 2.572645  [41616/60600]
Training loss: 2.174610  [43216/60600]
Training loss: 1.987035  [44816/60600]
Training loss: 2.096693  [46416/60600]
Training loss: 2.326314  [48016/60600]
Training loss: 2.952078  [49616/60600]
Training loss: 1.668067  [51216/60600]
Training loss: 2.497690  [52816/60600]
Training loss: 2.228516  [54416/60600]
Training loss: 2.045828  [56016/60600]
Training loss: 2.444239  [57616/60600]
Training loss: 2.456031  [59216/60600]
Training accuracy: 48.96 %
Validation loss: 2.136942
Validation accuracy: 48.71% 

Epoch 16
-------------------------------
Training loss: 2.291924  [16/60600]
Training loss: 1.539026  [1616/60600]
Training loss: 1.748445  [3216/60600]
Training loss: 2.424748  [4816/60600]
Training loss: 2.610508  [6416/60600]
Training loss: 2.056504  [8016/60600]
Training loss: 1.931337  [9616/60600]
Training loss: 1.453786  [11216/60600]
Training loss: 2.593262  [12816/60600]
Training loss: 2.480755  [14416/60600]
Training loss: 1.934946  [16016/60600]
Training loss: 2.796798  [17616/60600]
Training loss: 2.108048  [19216/60600]
Training loss: 1.334524  [20816/60600]
Training loss: 2.985478  [22416/60600]
Training loss: 1.956722  [24016/60600]
Training loss: 1.994952  [25616/60600]
Training loss: 2.080889  [27216/60600]
Training loss: 2.830214  [28816/60600]
Training loss: 2.027020  [30416/60600]
Training loss: 2.692298  [32016/60600]
Training loss: 2.672315  [33616/60600]
Training loss: 2.058841  [35216/60600]
Training loss: 2.534959  [36816/60600]
Training loss: 2.243457  [38416/60600]
Training loss: 2.127227  [40016/60600]
Training loss: 2.349966  [41616/60600]
Training loss: 2.419502  [43216/60600]
Training loss: 2.074471  [44816/60600]
Training loss: 2.684814  [46416/60600]
Training loss: 2.296919  [48016/60600]
Training loss: 2.628773  [49616/60600]
Training loss: 2.159013  [51216/60600]
Training loss: 2.131364  [52816/60600]
Training loss: 1.909345  [54416/60600]
Training loss: 2.198994  [56016/60600]
Training loss: 2.006637  [57616/60600]
Training loss: 3.242801  [59216/60600]
Training accuracy: 49.43 %
Validation loss: 2.118808
Validation accuracy: 48.84% 

Epoch 17
-------------------------------
Training loss: 1.889573  [16/60600]
Training loss: 1.254404  [1616/60600]
Training loss: 2.036322  [3216/60600]
Training loss: 1.889584  [4816/60600]
Training loss: 2.333395  [6416/60600]
Training loss: 1.642431  [8016/60600]
Training loss: 1.816661  [9616/60600]
Training loss: 2.712686  [11216/60600]
Training loss: 2.314905  [12816/60600]
Training loss: 2.762877  [14416/60600]
Training loss: 1.691241  [16016/60600]
Training loss: 2.248013  [17616/60600]
Training loss: 1.428602  [19216/60600]
Training loss: 2.069611  [20816/60600]
Training loss: 2.128778  [22416/60600]
Training loss: 3.091944  [24016/60600]
Training loss: 2.157270  [25616/60600]
Training loss: 2.114475  [27216/60600]
Training loss: 2.277407  [28816/60600]
Training loss: 1.810153  [30416/60600]
Training loss: 2.017303  [32016/60600]
Training loss: 1.367938  [33616/60600]
Training loss: 3.210109  [35216/60600]
Training loss: 1.862889  [36816/60600]
Training loss: 3.013935  [38416/60600]
Training loss: 2.182269  [40016/60600]
Training loss: 2.554976  [41616/60600]
Training loss: 1.529883  [43216/60600]
Training loss: 1.275900  [44816/60600]
Training loss: 3.040140  [46416/60600]
Training loss: 1.489670  [48016/60600]
Training loss: 2.548010  [49616/60600]
Training loss: 1.931517  [51216/60600]
Training loss: 2.032169  [52816/60600]
Training loss: 2.847010  [54416/60600]
Training loss: 2.559493  [56016/60600]
Training loss: 2.424189  [57616/60600]
Training loss: 1.233562  [59216/60600]
Training accuracy: 49.68 %
Validation loss: 2.096063
Validation accuracy: 49.29% 

Epoch 18
-------------------------------
Training loss: 1.702281  [16/60600]
Training loss: 1.702069  [1616/60600]
Training loss: 1.214092  [3216/60600]
Training loss: 1.503043  [4816/60600]
Training loss: 2.134475  [6416/60600]
Training loss: 2.361408  [8016/60600]
Training loss: 2.431726  [9616/60600]
Training loss: 1.768277  [11216/60600]
Training loss: 2.456813  [12816/60600]
Training loss: 2.114936  [14416/60600]
Training loss: 2.453636  [16016/60600]
Training loss: 2.109095  [17616/60600]
Training loss: 2.655519  [19216/60600]
Training loss: 1.590337  [20816/60600]
Training loss: 1.320301  [22416/60600]
Training loss: 2.433070  [24016/60600]
Training loss: 1.791662  [25616/60600]
Training loss: 1.531352  [27216/60600]
Training loss: 2.200419  [28816/60600]
Training loss: 2.100082  [30416/60600]
Training loss: 1.726120  [32016/60600]
Training loss: 1.761117  [33616/60600]
Training loss: 1.895885  [35216/60600]
Training loss: 2.143913  [36816/60600]
Training loss: 3.018480  [38416/60600]
Training loss: 2.130049  [40016/60600]
Training loss: 2.989367  [41616/60600]
Training loss: 2.084069  [43216/60600]
Training loss: 2.392824  [44816/60600]
Training loss: 2.886942  [46416/60600]
Training loss: 2.731092  [48016/60600]
Training loss: 2.092034  [49616/60600]
Training loss: 1.955372  [51216/60600]
Training loss: 2.289266  [52816/60600]
Training loss: 2.092278  [54416/60600]
Training loss: 2.184226  [56016/60600]
Training loss: 1.628517  [57616/60600]
Training loss: 1.921837  [59216/60600]
Training accuracy: 49.91 %
Validation loss: 2.071320
Validation accuracy: 49.66% 

Epoch 19
-------------------------------
Training loss: 2.205913  [16/60600]
Training loss: 2.450906  [1616/60600]
Training loss: 1.857190  [3216/60600]
Training loss: 2.139479  [4816/60600]
Training loss: 2.301819  [6416/60600]
Training loss: 2.447387  [8016/60600]
Training loss: 2.409305  [9616/60600]
Training loss: 1.485582  [11216/60600]
Training loss: 2.188443  [12816/60600]
Training loss: 1.999674  [14416/60600]
Training loss: 2.146700  [16016/60600]
Training loss: 1.667868  [17616/60600]
Training loss: 1.034730  [19216/60600]
Training loss: 2.213949  [20816/60600]
Training loss: 2.465363  [22416/60600]
Training loss: 1.673621  [24016/60600]
Training loss: 1.835618  [25616/60600]
Training loss: 1.735451  [27216/60600]
Training loss: 2.761105  [28816/60600]
Training loss: 2.396574  [30416/60600]
Training loss: 2.204736  [32016/60600]
Training loss: 2.367961  [33616/60600]
Training loss: 1.741420  [35216/60600]
Training loss: 2.007802  [36816/60600]
Training loss: 2.126556  [38416/60600]
Training loss: 2.408111  [40016/60600]
Training loss: 2.161433  [41616/60600]
Training loss: 2.124259  [43216/60600]
Training loss: 2.324264  [44816/60600]
Training loss: 1.811556  [46416/60600]
Training loss: 1.876086  [48016/60600]
Training loss: 2.238064  [49616/60600]
Training loss: 2.215301  [51216/60600]
Training loss: 2.962711  [52816/60600]
Training loss: 2.055636  [54416/60600]
Training loss: 1.513867  [56016/60600]
Training loss: 1.602764  [57616/60600]
Training loss: 1.982632  [59216/60600]
Training accuracy: 50.37 %
Validation loss: 2.090494
Validation accuracy: 49.33% 

Epoch 20
-------------------------------
Training loss: 1.434214  [16/60600]
Training loss: 2.515495  [1616/60600]
Training loss: 2.508276  [3216/60600]
Training loss: 1.384494  [4816/60600]
Training loss: 2.059088  [6416/60600]
Training loss: 2.467099  [8016/60600]
Training loss: 1.490631  [9616/60600]
Training loss: 1.691649  [11216/60600]
Training loss: 2.038851  [12816/60600]
Training loss: 2.033241  [14416/60600]
Training loss: 1.938440  [16016/60600]
Training loss: 2.881518  [17616/60600]
Training loss: 2.715800  [19216/60600]
Training loss: 2.008303  [20816/60600]
Training loss: 2.132179  [22416/60600]
Training loss: 2.847299  [24016/60600]
Training loss: 2.083299  [25616/60600]
Training loss: 2.005450  [27216/60600]
Training loss: 2.384747  [28816/60600]
Training loss: 2.160156  [30416/60600]
Training loss: 2.699035  [32016/60600]
Training loss: 1.784168  [33616/60600]
Training loss: 1.697487  [35216/60600]
Training loss: 1.985660  [36816/60600]
Training loss: 1.618310  [38416/60600]
Training loss: 1.651411  [40016/60600]
Training loss: 1.516464  [41616/60600]
Training loss: 1.848786  [43216/60600]
Training loss: 2.390348  [44816/60600]
Training loss: 2.408061  [46416/60600]
Training loss: 1.265465  [48016/60600]
Training loss: 1.911715  [49616/60600]
Training loss: 2.089525  [51216/60600]
Training loss: 2.066726  [52816/60600]
Training loss: 1.882231  [54416/60600]
Training loss: 1.740342  [56016/60600]
Training loss: 1.741253  [57616/60600]
Training loss: 1.658979  [59216/60600]
Training accuracy: 50.45 %
Validation loss: 2.047207
Validation accuracy: 50.14% 

Epoch 21
-------------------------------
Training loss: 1.915647  [16/60600]
Training loss: 1.511033  [1616/60600]
Training loss: 1.763645  [3216/60600]
Training loss: 2.256841  [4816/60600]
Training loss: 1.707979  [6416/60600]
Training loss: 1.701303  [8016/60600]
Training loss: 2.025090  [9616/60600]
Training loss: 2.407487  [11216/60600]
Training loss: 1.608835  [12816/60600]
Training loss: 1.335569  [14416/60600]
Training loss: 2.445791  [16016/60600]
Training loss: 1.741809  [17616/60600]
Training loss: 2.590719  [19216/60600]
Training loss: 1.077680  [20816/60600]
Training loss: 2.196183  [22416/60600]
Training loss: 1.360951  [24016/60600]
Training loss: 1.621835  [25616/60600]
Training loss: 2.323081  [27216/60600]
Training loss: 1.685012  [28816/60600]
Training loss: 1.879506  [30416/60600]
Training loss: 1.117579  [32016/60600]
Training loss: 2.033260  [33616/60600]
Training loss: 1.471089  [35216/60600]
Training loss: 2.164701  [36816/60600]
Training loss: 2.140171  [38416/60600]
Training loss: 2.353046  [40016/60600]
Training loss: 2.366426  [41616/60600]
Training loss: 2.604691  [43216/60600]
Training loss: 2.476260  [44816/60600]
Training loss: 1.667706  [46416/60600]
Training loss: 2.543546  [48016/60600]
Training loss: 1.981502  [49616/60600]
Training loss: 1.876443  [51216/60600]
Training loss: 2.614169  [52816/60600]
Training loss: 2.136805  [54416/60600]
Training loss: 2.533733  [56016/60600]
Training loss: 2.249410  [57616/60600]
Training loss: 1.802616  [59216/60600]
Training accuracy: 50.58 %
Validation loss: 2.056545
Validation accuracy: 49.73% 

Epoch 22
-------------------------------
Training loss: 1.344294  [16/60600]
Training loss: 1.179093  [1616/60600]
Training loss: 1.763856  [3216/60600]
Training loss: 2.083612  [4816/60600]
Training loss: 2.745024  [6416/60600]
Training loss: 1.887834  [8016/60600]
Training loss: 1.941445  [9616/60600]
Training loss: 1.501181  [11216/60600]
Training loss: 2.258142  [12816/60600]
Training loss: 1.628165  [14416/60600]
Training loss: 1.682640  [16016/60600]
Training loss: 2.046022  [17616/60600]
Training loss: 2.144471  [19216/60600]
Training loss: 1.478413  [20816/60600]
Training loss: 1.939023  [22416/60600]
Training loss: 1.488032  [24016/60600]
Training loss: 2.640567  [25616/60600]
Training loss: 1.998558  [27216/60600]
Training loss: 1.252694  [28816/60600]
Training loss: 2.141296  [30416/60600]
Training loss: 1.615209  [32016/60600]
Training loss: 1.754209  [33616/60600]
Training loss: 2.073089  [35216/60600]
Training loss: 1.266452  [36816/60600]
Training loss: 2.112644  [38416/60600]
Training loss: 2.173290  [40016/60600]
Training loss: 2.369643  [41616/60600]
Training loss: 1.717108  [43216/60600]
Training loss: 2.710398  [44816/60600]
Training loss: 2.963992  [46416/60600]
Training loss: 1.571240  [48016/60600]
Training loss: 2.241086  [49616/60600]
Training loss: 1.598175  [51216/60600]
Training loss: 1.485967  [52816/60600]
Training loss: 1.620892  [54416/60600]
Training loss: 2.057145  [56016/60600]
Training loss: 2.589352  [57616/60600]
Training loss: 1.490580  [59216/60600]
Training accuracy: 50.72 %
Validation loss: 2.054007
Validation accuracy: 49.95% 

Epoch 23
-------------------------------
Training loss: 2.731485  [16/60600]
Training loss: 1.668372  [1616/60600]
Training loss: 1.689365  [3216/60600]
Training loss: 2.433677  [4816/60600]
Training loss: 2.163229  [6416/60600]
Training loss: 2.373090  [8016/60600]
Training loss: 1.905401  [9616/60600]
Training loss: 1.747141  [11216/60600]
Training loss: 1.871314  [12816/60600]
Training loss: 1.537713  [14416/60600]
Training loss: 2.402207  [16016/60600]
Training loss: 2.298339  [17616/60600]
Training loss: 2.179435  [19216/60600]
Training loss: 1.907050  [20816/60600]
Training loss: 2.015103  [22416/60600]
Training loss: 2.096933  [24016/60600]
Training loss: 2.268438  [25616/60600]
Training loss: 1.775814  [27216/60600]
Training loss: 1.855379  [28816/60600]
Training loss: 1.742924  [30416/60600]
Training loss: 1.191421  [32016/60600]
Training loss: 1.761936  [33616/60600]
Training loss: 1.971806  [35216/60600]
Training loss: 2.232980  [36816/60600]
Training loss: 1.957162  [38416/60600]
Training loss: 2.003633  [40016/60600]
Training loss: 2.664140  [41616/60600]
Training loss: 2.242836  [43216/60600]
Training loss: 1.633253  [44816/60600]
Training loss: 1.439586  [46416/60600]
Training loss: 1.696458  [48016/60600]
Training loss: 1.348141  [49616/60600]
Training loss: 1.975187  [51216/60600]
Training loss: 2.438219  [52816/60600]
Training loss: 2.609173  [54416/60600]
Training loss: 1.517092  [56016/60600]
Training loss: 1.867697  [57616/60600]
Training loss: 2.336322  [59216/60600]
Training accuracy: 51.00 %
Validation loss: 2.034240
Validation accuracy: 50.56% 

Epoch 24
-------------------------------
Training loss: 1.951289  [16/60600]
Training loss: 1.628074  [1616/60600]
Training loss: 2.023936  [3216/60600]
Training loss: 2.059603  [4816/60600]
Training loss: 1.413229  [6416/60600]
Training loss: 2.113548  [8016/60600]
Training loss: 1.812815  [9616/60600]
Training loss: 1.309811  [11216/60600]
Training loss: 1.864018  [12816/60600]
Training loss: 1.509488  [14416/60600]
Training loss: 1.792751  [16016/60600]
Training loss: 2.663561  [17616/60600]
Training loss: 1.449324  [19216/60600]
Training loss: 2.353827  [20816/60600]
Training loss: 1.510792  [22416/60600]
Training loss: 1.580935  [24016/60600]
Training loss: 3.121179  [25616/60600]
Training loss: 2.403315  [27216/60600]
Training loss: 2.778273  [28816/60600]
Training loss: 2.105829  [30416/60600]
Training loss: 1.953432  [32016/60600]
Training loss: 1.816966  [33616/60600]
Training loss: 1.847767  [35216/60600]
Training loss: 2.167804  [36816/60600]
Training loss: 2.145214  [38416/60600]
Training loss: 1.700908  [40016/60600]
Training loss: 1.693112  [41616/60600]
Training loss: 1.648970  [43216/60600]
Training loss: 2.273703  [44816/60600]
Training loss: 1.925972  [46416/60600]
Training loss: 1.897536  [48016/60600]
Training loss: 1.569747  [49616/60600]
Training loss: 1.737655  [51216/60600]
Training loss: 1.537485  [52816/60600]
Training loss: 2.021498  [54416/60600]
Training loss: 2.369120  [56016/60600]
Training loss: 1.664506  [57616/60600]
Training loss: 1.782932  [59216/60600]
Training accuracy: 51.29 %
Validation loss: 2.028245
Validation accuracy: 50.55% 

Epoch 25
-------------------------------
Training loss: 2.015522  [16/60600]
Training loss: 2.534521  [1616/60600]
Training loss: 2.388222  [3216/60600]
Training loss: 1.963123  [4816/60600]
Training loss: 1.854639  [6416/60600]
Training loss: 1.546082  [8016/60600]
Training loss: 2.452271  [9616/60600]
Training loss: 1.158249  [11216/60600]
Training loss: 1.894991  [12816/60600]
Training loss: 1.932250  [14416/60600]
Training loss: 2.025909  [16016/60600]
Training loss: 2.888744  [17616/60600]
Training loss: 1.961751  [19216/60600]
Training loss: 2.071215  [20816/60600]
Training loss: 2.769993  [22416/60600]
Training loss: 1.913443  [24016/60600]
Training loss: 1.524933  [25616/60600]
Training loss: 2.179766  [27216/60600]
Training loss: 2.066812  [28816/60600]
Training loss: 1.439959  [30416/60600]
Training loss: 2.333214  [32016/60600]
Training loss: 2.089576  [33616/60600]
Training loss: 2.148500  [35216/60600]
Training loss: 2.056614  [36816/60600]
Training loss: 1.995281  [38416/60600]
Training loss: 1.774791  [40016/60600]
Training loss: 1.659101  [41616/60600]
Training loss: 2.290810  [43216/60600]
Training loss: 2.447028  [44816/60600]
Training loss: 1.387256  [46416/60600]
Training loss: 1.870889  [48016/60600]
Training loss: 1.658097  [49616/60600]
Training loss: 2.022357  [51216/60600]
Training loss: 1.811277  [52816/60600]
Training loss: 2.349141  [54416/60600]
Training loss: 2.419906  [56016/60600]
Training loss: 2.865848  [57616/60600]
Training loss: 1.848935  [59216/60600]
Training accuracy: 51.53 %
Validation loss: 2.009794
Validation accuracy: 51.24% 

Epoch 26
-------------------------------
Training loss: 2.046548  [16/60600]
Training loss: 2.641206  [1616/60600]
Training loss: 2.338103  [3216/60600]
Training loss: 1.901019  [4816/60600]
Training loss: 1.999606  [6416/60600]
Training loss: 1.730524  [8016/60600]
Training loss: 2.168554  [9616/60600]
Training loss: 2.671520  [11216/60600]
Training loss: 1.278354  [12816/60600]
Training loss: 2.289396  [14416/60600]
Training loss: 2.228816  [16016/60600]
Training loss: 2.214369  [17616/60600]
Training loss: 1.720935  [19216/60600]
Training loss: 2.108142  [20816/60600]
Training loss: 1.721673  [22416/60600]
Training loss: 2.460451  [24016/60600]
Training loss: 1.994578  [25616/60600]
Training loss: 2.336171  [27216/60600]
Training loss: 1.890184  [28816/60600]
Training loss: 1.956367  [30416/60600]
Training loss: 1.887774  [32016/60600]
Training loss: 1.619128  [33616/60600]
Training loss: 2.439346  [35216/60600]
Training loss: 2.066272  [36816/60600]
Training loss: 2.213357  [38416/60600]
Training loss: 2.354630  [40016/60600]
Training loss: 2.214457  [41616/60600]
Training loss: 1.544714  [43216/60600]
Training loss: 2.134118  [44816/60600]
Training loss: 2.170510  [46416/60600]
Training loss: 1.536272  [48016/60600]
Training loss: 1.795625  [49616/60600]
Training loss: 1.815379  [51216/60600]
Training loss: 1.008653  [52816/60600]
Training loss: 1.694003  [54416/60600]
Training loss: 1.401952  [56016/60600]
Training loss: 1.620712  [57616/60600]
Training loss: 2.332792  [59216/60600]
Training accuracy: 51.68 %
Validation loss: 2.010616
Validation accuracy: 50.86% 

Epoch 27
-------------------------------
Training loss: 2.065953  [16/60600]
Training loss: 1.656098  [1616/60600]
Training loss: 2.096963  [3216/60600]
Training loss: 1.373558  [4816/60600]
Training loss: 1.639174  [6416/60600]
Training loss: 2.358605  [8016/60600]
Training loss: 2.043977  [9616/60600]
Training loss: 2.263256  [11216/60600]
Training loss: 1.802608  [12816/60600]
Training loss: 1.609851  [14416/60600]
Training loss: 1.696602  [16016/60600]
Training loss: 1.986738  [17616/60600]
Training loss: 2.131949  [19216/60600]
Training loss: 2.116400  [20816/60600]
Training loss: 2.147963  [22416/60600]
Training loss: 1.657328  [24016/60600]
Training loss: 1.786622  [25616/60600]
Training loss: 2.235449  [27216/60600]
Training loss: 2.194289  [28816/60600]
Training loss: 1.460035  [30416/60600]
Training loss: 1.960546  [32016/60600]
Training loss: 1.965975  [33616/60600]
Training loss: 2.395916  [35216/60600]
Training loss: 2.709232  [36816/60600]
Training loss: 1.977025  [38416/60600]
Training loss: 2.054584  [40016/60600]
Training loss: 1.905108  [41616/60600]
Training loss: 1.941081  [43216/60600]
Training loss: 2.333398  [44816/60600]
Training loss: 1.668507  [46416/60600]
Training loss: 1.676316  [48016/60600]
Training loss: 1.853126  [49616/60600]
Training loss: 2.535528  [51216/60600]
Training loss: 1.484619  [52816/60600]
Training loss: 2.319430  [54416/60600]
Training loss: 2.048918  [56016/60600]
Training loss: 1.382348  [57616/60600]
Training loss: 2.406675  [59216/60600]
Training accuracy: 51.92 %
Validation loss: 2.011073
Validation accuracy: 50.66% 

Epoch 28
-------------------------------
Training loss: 1.669301  [16/60600]
Training loss: 1.938789  [1616/60600]
Training loss: 2.107711  [3216/60600]
Training loss: 1.679048  [4816/60600]
Training loss: 1.421728  [6416/60600]
Training loss: 1.935668  [8016/60600]
Training loss: 1.552910  [9616/60600]
Training loss: 1.689996  [11216/60600]
Training loss: 2.047917  [12816/60600]
Training loss: 2.359104  [14416/60600]
Training loss: 1.987908  [16016/60600]
Training loss: 1.482907  [17616/60600]
Training loss: 1.882410  [19216/60600]
Training loss: 1.735404  [20816/60600]
Training loss: 1.958149  [22416/60600]
Training loss: 1.750579  [24016/60600]
Training loss: 2.335190  [25616/60600]
Training loss: 2.308186  [27216/60600]
Training loss: 2.657981  [28816/60600]
Training loss: 2.516432  [30416/60600]
Training loss: 2.887285  [32016/60600]
Training loss: 1.608352  [33616/60600]
Training loss: 1.688180  [35216/60600]
Training loss: 2.770727  [36816/60600]
Training loss: 2.631891  [38416/60600]
Training loss: 1.733379  [40016/60600]
Training loss: 1.611232  [41616/60600]
Training loss: 1.940338  [43216/60600]
Training loss: 2.492327  [44816/60600]
Training loss: 1.990056  [46416/60600]
Training loss: 2.209003  [48016/60600]
Training loss: 1.715428  [49616/60600]
Training loss: 2.049573  [51216/60600]
Training loss: 1.736028  [52816/60600]
Training loss: 2.873249  [54416/60600]
Training loss: 2.310297  [56016/60600]
Training loss: 2.327949  [57616/60600]
Training loss: 1.929707  [59216/60600]
Training accuracy: 52.01 %
Validation loss: 1.991639
Validation accuracy: 51.14% 

Epoch 29
-------------------------------
Training loss: 1.708395  [16/60600]
Training loss: 2.135214  [1616/60600]
Training loss: 1.994375  [3216/60600]
Training loss: 2.093634  [4816/60600]
Training loss: 2.751698  [6416/60600]
Training loss: 1.139284  [8016/60600]
Training loss: 2.040611  [9616/60600]
Training loss: 1.505107  [11216/60600]
Training loss: 1.636091  [12816/60600]
Training loss: 2.128412  [14416/60600]
Training loss: 2.700841  [16016/60600]
Training loss: 1.983151  [17616/60600]
Training loss: 2.935501  [19216/60600]
Training loss: 1.654233  [20816/60600]
Training loss: 1.391638  [22416/60600]
Training loss: 1.916263  [24016/60600]
Training loss: 2.820608  [25616/60600]
Training loss: 1.514750  [27216/60600]
Training loss: 1.126824  [28816/60600]
Training loss: 2.049606  [30416/60600]
Training loss: 2.234037  [32016/60600]
Training loss: 2.359216  [33616/60600]
Training loss: 1.864120  [35216/60600]
Training loss: 2.043097  [36816/60600]
Training loss: 1.637731  [38416/60600]
Training loss: 1.651737  [40016/60600]
Training loss: 1.535723  [41616/60600]
Training loss: 2.291461  [43216/60600]
Training loss: 1.508474  [44816/60600]
Training loss: 2.210628  [46416/60600]
Training loss: 2.128129  [48016/60600]
Training loss: 1.970244  [49616/60600]
Training loss: 2.565193  [51216/60600]
Training loss: 1.870152  [52816/60600]
Training loss: 1.777084  [54416/60600]
Training loss: 1.855526  [56016/60600]
Training loss: 2.075295  [57616/60600]
Training loss: 1.426756  [59216/60600]
Training accuracy: 52.15 %
Validation loss: 1.986102
Validation accuracy: 51.15% 

Epoch 30
-------------------------------
Training loss: 1.841555  [16/60600]
Training loss: 1.913869  [1616/60600]
Training loss: 1.955319  [3216/60600]
Training loss: 1.454241  [4816/60600]
Training loss: 1.371305  [6416/60600]
Training loss: 2.015112  [8016/60600]
Training loss: 1.470106  [9616/60600]
Training loss: 2.140664  [11216/60600]
Training loss: 2.190955  [12816/60600]
Training loss: 1.429414  [14416/60600]
Training loss: 1.405968  [16016/60600]
Training loss: 2.062893  [17616/60600]
Training loss: 2.265791  [19216/60600]
Training loss: 1.740124  [20816/60600]
Training loss: 2.142397  [22416/60600]
Training loss: 1.092409  [24016/60600]
Training loss: 1.814004  [25616/60600]
Training loss: 1.907035  [27216/60600]
Training loss: 2.202394  [28816/60600]
Training loss: 2.007671  [30416/60600]
Training loss: 2.035524  [32016/60600]
Training loss: 1.434073  [33616/60600]
Training loss: 1.741789  [35216/60600]
Training loss: 2.578740  [36816/60600]
Training loss: 1.909165  [38416/60600]
Training loss: 1.445262  [40016/60600]
Training loss: 2.487821  [41616/60600]
Training loss: 2.530083  [43216/60600]
Training loss: 1.948571  [44816/60600]
Training loss: 1.880174  [46416/60600]
Training loss: 2.811295  [48016/60600]
Training loss: 1.137601  [49616/60600]
Training loss: 2.048754  [51216/60600]
Training loss: 1.625273  [52816/60600]
Training loss: 1.291517  [54416/60600]
Training loss: 2.018806  [56016/60600]
Training loss: 1.845227  [57616/60600]
Training loss: 1.723149  [59216/60600]
Training accuracy: 52.18 %
Validation loss: 1.983064
Validation accuracy: 51.38% 

Epoch 31
-------------------------------
Training loss: 2.820164  [16/60600]
Training loss: 2.192765  [1616/60600]
Training loss: 1.754676  [3216/60600]
Training loss: 2.393740  [4816/60600]
Training loss: 1.890771  [6416/60600]
Training loss: 2.149272  [8016/60600]
Training loss: 1.880693  [9616/60600]
Training loss: 1.100142  [11216/60600]
Training loss: 2.296174  [12816/60600]
Training loss: 2.413629  [14416/60600]
Training loss: 1.876262  [16016/60600]
Training loss: 2.162942  [17616/60600]
Training loss: 1.709246  [19216/60600]
Training loss: 1.984252  [20816/60600]
Training loss: 2.432433  [22416/60600]
Training loss: 0.977916  [24016/60600]
Training loss: 2.394691  [25616/60600]
Training loss: 1.676984  [27216/60600]
Training loss: 2.368781  [28816/60600]
Training loss: 2.063835  [30416/60600]
Training loss: 2.243225  [32016/60600]
Training loss: 1.415586  [33616/60600]
Training loss: 2.245077  [35216/60600]
Training loss: 1.902136  [36816/60600]
Training loss: 2.151165  [38416/60600]
Training loss: 1.408683  [40016/60600]
Training loss: 2.708042  [41616/60600]
Training loss: 1.721867  [43216/60600]
Training loss: 1.518515  [44816/60600]
Training loss: 2.811024  [46416/60600]
Training loss: 2.209661  [48016/60600]
Training loss: 2.124486  [49616/60600]
Training loss: 1.907238  [51216/60600]
Training loss: 1.498327  [52816/60600]
Training loss: 2.172584  [54416/60600]
Training loss: 2.035934  [56016/60600]
Training loss: 2.521860  [57616/60600]
Training loss: 1.644144  [59216/60600]
Training accuracy: 52.30 %
Validation loss: 1.990861
Validation accuracy: 51.26% 

Epoch 32
-------------------------------
Training loss: 1.866226  [16/60600]
Training loss: 1.785680  [1616/60600]
Training loss: 1.787898  [3216/60600]
Training loss: 1.811926  [4816/60600]
Training loss: 1.982451  [6416/60600]
Training loss: 3.114882  [8016/60600]
Training loss: 2.970945  [9616/60600]
Training loss: 2.105082  [11216/60600]
Training loss: 2.143921  [12816/60600]
Training loss: 2.757422  [14416/60600]
Training loss: 1.946793  [16016/60600]
Training loss: 2.080635  [17616/60600]
Training loss: 2.024219  [19216/60600]
Training loss: 1.624763  [20816/60600]
Training loss: 1.847086  [22416/60600]
Training loss: 2.241794  [24016/60600]
Training loss: 2.230194  [25616/60600]
Training loss: 2.184057  [27216/60600]
Training loss: 1.767678  [28816/60600]
Training loss: 1.420938  [30416/60600]
Training loss: 2.242431  [32016/60600]
Training loss: 1.318665  [33616/60600]
Training loss: 2.382179  [35216/60600]
Training loss: 2.889931  [36816/60600]
Training loss: 1.394904  [38416/60600]
Training loss: 1.854952  [40016/60600]
Training loss: 1.275198  [41616/60600]
Training loss: 2.415481  [43216/60600]
Training loss: 1.992714  [44816/60600]
Training loss: 1.644781  [46416/60600]
Training loss: 1.878396  [48016/60600]
Training loss: 1.292444  [49616/60600]
Training loss: 1.685469  [51216/60600]
Training loss: 1.278375  [52816/60600]
Training loss: 2.295744  [54416/60600]
Training loss: 1.930454  [56016/60600]
Training loss: 1.754997  [57616/60600]
Training loss: 1.510248  [59216/60600]
Training accuracy: 52.62 %
Validation loss: 1.983638
Validation accuracy: 51.41% 

Epoch 33
-------------------------------
Training loss: 2.663044  [16/60600]
Training loss: 2.271020  [1616/60600]
Training loss: 1.771249  [3216/60600]
Training loss: 1.541010  [4816/60600]
Training loss: 1.973264  [6416/60600]
Training loss: 2.596180  [8016/60600]
Training loss: 1.682825  [9616/60600]
Training loss: 2.529408  [11216/60600]
Training loss: 2.391331  [12816/60600]
Training loss: 1.902430  [14416/60600]
Training loss: 1.714210  [16016/60600]
Training loss: 2.112711  [17616/60600]
Training loss: 1.630450  [19216/60600]
Training loss: 1.965264  [20816/60600]
Training loss: 1.997849  [22416/60600]
Training loss: 2.921923  [24016/60600]
Training loss: 1.959547  [25616/60600]
Training loss: 1.090383  [27216/60600]
Training loss: 2.196237  [28816/60600]
Training loss: 2.730508  [30416/60600]
Training loss: 2.193196  [32016/60600]
Training loss: 2.465489  [33616/60600]
Training loss: 1.986848  [35216/60600]
Training loss: 1.498778  [36816/60600]
Training loss: 1.309715  [38416/60600]
Training loss: 2.043670  [40016/60600]
Training loss: 1.953524  [41616/60600]
Training loss: 2.655034  [43216/60600]
Training loss: 1.805150  [44816/60600]
Training loss: 1.620034  [46416/60600]
Training loss: 2.544562  [48016/60600]
Training loss: 1.749269  [49616/60600]
Training loss: 1.519375  [51216/60600]
Training loss: 2.505068  [52816/60600]
Training loss: 2.430437  [54416/60600]
Training loss: 2.114832  [56016/60600]
Training loss: 1.694448  [57616/60600]
Training loss: 2.271543  [59216/60600]
Training accuracy: 52.43 %
Validation loss: 1.975878
Validation accuracy: 51.68% 

Epoch 34
-------------------------------
Training loss: 1.650540  [16/60600]
Training loss: 1.560495  [1616/60600]
Training loss: 2.244905  [3216/60600]
Training loss: 2.458203  [4816/60600]
Training loss: 1.539024  [6416/60600]
Training loss: 2.030346  [8016/60600]
Training loss: 1.738465  [9616/60600]
Training loss: 2.328078  [11216/60600]
Training loss: 1.811169  [12816/60600]
Training loss: 1.414243  [14416/60600]
Training loss: 1.910209  [16016/60600]
Training loss: 0.992388  [17616/60600]
Training loss: 2.000494  [19216/60600]
Training loss: 1.678167  [20816/60600]
Training loss: 2.059603  [22416/60600]
Training loss: 1.772648  [24016/60600]
Training loss: 1.572538  [25616/60600]
Training loss: 1.606742  [27216/60600]
Training loss: 2.349500  [28816/60600]
Training loss: 1.833393  [30416/60600]
Training loss: 2.356593  [32016/60600]
Training loss: 1.567011  [33616/60600]
Training loss: 1.858600  [35216/60600]
Training loss: 2.206461  [36816/60600]
Training loss: 1.612343  [38416/60600]
Training loss: 1.346767  [40016/60600]
Training loss: 2.141672  [41616/60600]
Training loss: 1.680309  [43216/60600]
Training loss: 1.870449  [44816/60600]
Training loss: 1.541500  [46416/60600]
Training loss: 1.741439  [48016/60600]
Training loss: 2.181895  [49616/60600]
Training loss: 2.019500  [51216/60600]
Training loss: 1.994473  [52816/60600]
Training loss: 2.189835  [54416/60600]
Training loss: 2.131073  [56016/60600]
Training loss: 1.337356  [57616/60600]
Training loss: 2.257780  [59216/60600]
Training accuracy: 52.77 %
Validation loss: 1.971258
Validation accuracy: 51.58% 

Epoch 35
-------------------------------
Training loss: 1.625461  [16/60600]
Training loss: 1.886053  [1616/60600]
Training loss: 1.251276  [3216/60600]
Training loss: 2.012095  [4816/60600]
Training loss: 1.950072  [6416/60600]
Training loss: 1.570495  [8016/60600]
Training loss: 2.010210  [9616/60600]
Training loss: 1.804791  [11216/60600]
Training loss: 1.555269  [12816/60600]
Training loss: 1.456024  [14416/60600]
Training loss: 0.980734  [16016/60600]
Training loss: 1.540793  [17616/60600]
Training loss: 2.451886  [19216/60600]
Training loss: 1.666043  [20816/60600]
Training loss: 2.103209  [22416/60600]
Training loss: 2.110571  [24016/60600]
Training loss: 1.487579  [25616/60600]
Training loss: 1.340957  [27216/60600]
Training loss: 1.042226  [28816/60600]
Training loss: 1.812871  [30416/60600]
Training loss: 2.714813  [32016/60600]
Training loss: 1.784940  [33616/60600]
Training loss: 2.464274  [35216/60600]
Training loss: 1.644994  [36816/60600]
Training loss: 2.089991  [38416/60600]
Training loss: 2.647458  [40016/60600]
Training loss: 2.340478  [41616/60600]
Training loss: 1.135992  [43216/60600]
Training loss: 1.930974  [44816/60600]
Training loss: 2.060228  [46416/60600]
Training loss: 1.942826  [48016/60600]
Training loss: 1.771462  [49616/60600]
Training loss: 2.046266  [51216/60600]
Training loss: 1.932401  [52816/60600]
Training loss: 2.126259  [54416/60600]
Training loss: 1.352715  [56016/60600]
Training loss: 1.821718  [57616/60600]
Training loss: 1.495164  [59216/60600]
Training accuracy: 52.95 %
Validation loss: 1.958689
Validation accuracy: 51.71% 

Epoch 36
-------------------------------
Training loss: 1.878326  [16/60600]
Training loss: 2.136620  [1616/60600]
Training loss: 1.696970  [3216/60600]
Training loss: 1.834657  [4816/60600]
Training loss: 1.113999  [6416/60600]
Training loss: 1.532534  [8016/60600]
Training loss: 2.221065  [9616/60600]
Training loss: 1.999829  [11216/60600]
Training loss: 1.814199  [12816/60600]
Training loss: 1.533764  [14416/60600]
Training loss: 1.435848  [16016/60600]
Training loss: 2.117064  [17616/60600]
Training loss: 2.061665  [19216/60600]
Training loss: 1.746595  [20816/60600]
Training loss: 2.352502  [22416/60600]
Training loss: 2.056220  [24016/60600]
Training loss: 1.607482  [25616/60600]
Training loss: 2.476417  [27216/60600]
Training loss: 1.478400  [28816/60600]
Training loss: 1.970348  [30416/60600]
Training loss: 1.935237  [32016/60600]
Training loss: 1.723323  [33616/60600]
Training loss: 1.655338  [35216/60600]
Training loss: 1.845991  [36816/60600]
Training loss: 2.071078  [38416/60600]
Training loss: 1.642354  [40016/60600]
Training loss: 1.664474  [41616/60600]
Training loss: 1.840527  [43216/60600]
Training loss: 2.138493  [44816/60600]
Training loss: 1.540329  [46416/60600]
Training loss: 1.453325  [48016/60600]
Training loss: 2.167489  [49616/60600]
Training loss: 2.378646  [51216/60600]
Training loss: 2.140302  [52816/60600]
Training loss: 1.663648  [54416/60600]
Training loss: 1.633032  [56016/60600]
Training loss: 1.886244  [57616/60600]
Training loss: 2.052680  [59216/60600]
Training accuracy: 52.97 %
Validation loss: 1.965107
Validation accuracy: 51.56% 

Epoch 37
-------------------------------
Training loss: 1.913949  [16/60600]
Training loss: 2.681881  [1616/60600]
Training loss: 2.306194  [3216/60600]
Training loss: 1.970463  [4816/60600]
Training loss: 2.247490  [6416/60600]
Training loss: 2.374530  [8016/60600]
Training loss: 1.597759  [9616/60600]
Training loss: 2.287913  [11216/60600]
Training loss: 2.097091  [12816/60600]
Training loss: 1.387843  [14416/60600]
Training loss: 1.461315  [16016/60600]
Training loss: 2.510441  [17616/60600]
Training loss: 1.728142  [19216/60600]
Training loss: 1.787097  [20816/60600]
Training loss: 1.834248  [22416/60600]
Training loss: 1.412384  [24016/60600]
Training loss: 2.508529  [25616/60600]
Training loss: 1.948945  [27216/60600]
Training loss: 1.747427  [28816/60600]
Training loss: 2.171129  [30416/60600]
Training loss: 1.758400  [32016/60600]
Training loss: 1.419419  [33616/60600]
Training loss: 2.435456  [35216/60600]
Training loss: 1.866432  [36816/60600]
Training loss: 1.649411  [38416/60600]
Training loss: 1.850413  [40016/60600]
Training loss: 1.530116  [41616/60600]
Training loss: 2.920018  [43216/60600]
Training loss: 3.221462  [44816/60600]
Training loss: 2.294528  [46416/60600]
Training loss: 1.309699  [48016/60600]
Training loss: 1.575769  [49616/60600]
Training loss: 2.224277  [51216/60600]
Training loss: 1.596363  [52816/60600]
Training loss: 1.986298  [54416/60600]
Training loss: 1.591425  [56016/60600]
Training loss: 2.182457  [57616/60600]
Training loss: 2.485514  [59216/60600]
Training accuracy: 53.10 %
Validation loss: 1.958788
Validation accuracy: 51.94% 

Epoch 38
-------------------------------
Training loss: 1.819062  [16/60600]
Training loss: 2.172899  [1616/60600]
Training loss: 2.003479  [3216/60600]
Training loss: 1.289765  [4816/60600]
Training loss: 1.337724  [6416/60600]
Training loss: 1.400302  [8016/60600]
Training loss: 1.592401  [9616/60600]
Training loss: 1.249355  [11216/60600]
Training loss: 1.573626  [12816/60600]
Training loss: 2.146126  [14416/60600]
Training loss: 1.972131  [16016/60600]
Training loss: 2.155935  [17616/60600]
Training loss: 1.789917  [19216/60600]
Training loss: 1.349496  [20816/60600]
Training loss: 1.654631  [22416/60600]
Training loss: 1.988318  [24016/60600]
Training loss: 2.241454  [25616/60600]
Training loss: 2.846049  [27216/60600]
Training loss: 1.700523  [28816/60600]
Training loss: 1.760487  [30416/60600]
Training loss: 2.264858  [32016/60600]
Training loss: 1.743637  [33616/60600]
Training loss: 1.491305  [35216/60600]
Training loss: 1.977393  [36816/60600]
Training loss: 1.816414  [38416/60600]
Training loss: 1.303915  [40016/60600]
Training loss: 1.977298  [41616/60600]
Training loss: 1.867622  [43216/60600]
Training loss: 2.285757  [44816/60600]
Training loss: 1.677742  [46416/60600]
Training loss: 1.790936  [48016/60600]
Training loss: 1.404518  [49616/60600]
Training loss: 1.637069  [51216/60600]
Training loss: 1.840998  [52816/60600]
Training loss: 2.549738  [54416/60600]
Training loss: 2.140693  [56016/60600]
Training loss: 1.871196  [57616/60600]
Training loss: 2.073825  [59216/60600]
Training accuracy: 52.96 %
Validation loss: 1.952844
Validation accuracy: 51.93% 

Epoch 39
-------------------------------
Training loss: 2.189724  [16/60600]
Training loss: 1.932251  [1616/60600]
Training loss: 1.882817  [3216/60600]
Training loss: 1.061336  [4816/60600]
Training loss: 2.750647  [6416/60600]
Training loss: 2.041429  [8016/60600]
Training loss: 1.498847  [9616/60600]
Training loss: 2.703888  [11216/60600]
Training loss: 2.599513  [12816/60600]
Training loss: 1.850240  [14416/60600]
Training loss: 1.437929  [16016/60600]
Training loss: 2.226448  [17616/60600]
Training loss: 1.471720  [19216/60600]
Training loss: 2.517414  [20816/60600]
Training loss: 1.545151  [22416/60600]
Training loss: 2.701588  [24016/60600]
Training loss: 2.194231  [25616/60600]
Training loss: 2.072436  [27216/60600]
Training loss: 2.590792  [28816/60600]
Training loss: 1.590500  [30416/60600]
Training loss: 1.709571  [32016/60600]
Training loss: 2.225443  [33616/60600]
Training loss: 2.058682  [35216/60600]
Training loss: 2.573739  [36816/60600]
Training loss: 2.397245  [38416/60600]
Training loss: 1.739978  [40016/60600]
Training loss: 2.199238  [41616/60600]
Training loss: 1.359520  [43216/60600]
Training loss: 2.548834  [44816/60600]
Training loss: 1.119700  [46416/60600]
Training loss: 2.290658  [48016/60600]
Training loss: 1.640659  [49616/60600]
Training loss: 2.452407  [51216/60600]
Training loss: 1.376012  [52816/60600]
Training loss: 2.185898  [54416/60600]
Training loss: 1.421515  [56016/60600]
Training loss: 2.392494  [57616/60600]
Training loss: 1.508875  [59216/60600]
Training accuracy: 53.16 %
Validation loss: 1.946111
Validation accuracy: 51.83% 

Epoch 40
-------------------------------
Training loss: 1.487822  [16/60600]
Training loss: 1.787492  [1616/60600]
Training loss: 2.171242  [3216/60600]
Training loss: 1.275576  [4816/60600]
Training loss: 1.532678  [6416/60600]
Training loss: 1.983710  [8016/60600]
Training loss: 1.999847  [9616/60600]
Training loss: 2.587626  [11216/60600]
Training loss: 1.900169  [12816/60600]
Training loss: 1.750322  [14416/60600]
Training loss: 1.585307  [16016/60600]
Training loss: 2.148534  [17616/60600]
Training loss: 1.793515  [19216/60600]
Training loss: 1.581195  [20816/60600]
Training loss: 1.973814  [22416/60600]
Training loss: 1.470702  [24016/60600]
Training loss: 1.301926  [25616/60600]
Training loss: 1.682104  [27216/60600]
Training loss: 1.918491  [28816/60600]
Training loss: 1.890957  [30416/60600]
Training loss: 2.423056  [32016/60600]
Training loss: 1.509659  [33616/60600]
Training loss: 1.985755  [35216/60600]
Training loss: 2.149173  [36816/60600]
Training loss: 1.799572  [38416/60600]
Training loss: 1.852970  [40016/60600]
Training loss: 2.082870  [41616/60600]
Training loss: 1.756072  [43216/60600]
Training loss: 1.810305  [44816/60600]
Training loss: 2.562380  [46416/60600]
Training loss: 1.029582  [48016/60600]
Training loss: 2.306470  [49616/60600]
Training loss: 1.794684  [51216/60600]
Training loss: 2.575331  [52816/60600]
Training loss: 1.434502  [54416/60600]
Training loss: 2.756936  [56016/60600]
Training loss: 2.042402  [57616/60600]
Training loss: 1.572318  [59216/60600]
Training accuracy: 53.33 %
Validation loss: 1.964151
Validation accuracy: 51.54% 

Epoch 41
-------------------------------
Training loss: 2.433362  [16/60600]
Training loss: 2.410635  [1616/60600]
Training loss: 1.460697  [3216/60600]
Training loss: 1.856946  [4816/60600]
Training loss: 1.879951  [6416/60600]
Training loss: 1.480574  [8016/60600]
Training loss: 1.010876  [9616/60600]
Training loss: 1.210616  [11216/60600]
Training loss: 2.065338  [12816/60600]
Training loss: 1.485220  [14416/60600]
Training loss: 2.169591  [16016/60600]
Training loss: 1.466275  [17616/60600]
Training loss: 1.812095  [19216/60600]
Training loss: 1.998169  [20816/60600]
Training loss: 1.773034  [22416/60600]
Training loss: 2.133569  [24016/60600]
Training loss: 2.020895  [25616/60600]
Training loss: 1.299897  [27216/60600]
Training loss: 1.683300  [28816/60600]
Training loss: 1.626561  [30416/60600]
Training loss: 1.348054  [32016/60600]
Training loss: 1.268994  [33616/60600]
Training loss: 2.091160  [35216/60600]
Training loss: 2.203462  [36816/60600]
Training loss: 1.735262  [38416/60600]
Training loss: 3.160949  [40016/60600]
Training loss: 2.236139  [41616/60600]
Training loss: 1.582490  [43216/60600]
Training loss: 1.583105  [44816/60600]
Training loss: 2.090639  [46416/60600]
Training loss: 1.677379  [48016/60600]
Training loss: 2.139975  [49616/60600]
Training loss: 2.077387  [51216/60600]
Training loss: 2.153947  [52816/60600]
Training loss: 2.600450  [54416/60600]
Training loss: 2.382311  [56016/60600]
Training loss: 1.559397  [57616/60600]
Training loss: 2.187412  [59216/60600]
Training accuracy: 53.37 %
Validation loss: 1.942050
Validation accuracy: 52.17% 

Epoch 42
-------------------------------
Training loss: 2.743770  [16/60600]
Training loss: 2.021917  [1616/60600]
Training loss: 1.851244  [3216/60600]
Training loss: 2.168456  [4816/60600]
Training loss: 1.860398  [6416/60600]
Training loss: 2.511283  [8016/60600]
Training loss: 2.577606  [9616/60600]
Training loss: 1.707519  [11216/60600]
Training loss: 1.871283  [12816/60600]
Training loss: 1.874123  [14416/60600]
Training loss: 2.134916  [16016/60600]
Training loss: 2.415772  [17616/60600]
Training loss: 1.939583  [19216/60600]
Training loss: 1.556586  [20816/60600]
Training loss: 1.986416  [22416/60600]
Training loss: 2.014496  [24016/60600]
Training loss: 2.615564  [25616/60600]
Training loss: 1.433834  [27216/60600]
Training loss: 2.767348  [28816/60600]
Training loss: 2.073150  [30416/60600]
Training loss: 1.809430  [32016/60600]
Training loss: 2.080055  [33616/60600]
Training loss: 2.114350  [35216/60600]
Training loss: 1.623231  [36816/60600]
Training loss: 2.337096  [38416/60600]
Training loss: 1.931418  [40016/60600]
Training loss: 1.383646  [41616/60600]
Training loss: 1.620195  [43216/60600]
Training loss: 1.900199  [44816/60600]
Training loss: 2.143475  [46416/60600]
Training loss: 2.035470  [48016/60600]
Training loss: 2.467340  [49616/60600]
Training loss: 2.073559  [51216/60600]
Training loss: 1.574629  [52816/60600]
Training loss: 1.529518  [54416/60600]
Training loss: 2.167871  [56016/60600]
Training loss: 2.370746  [57616/60600]
Training loss: 2.128065  [59216/60600]
Training accuracy: 53.55 %
Validation loss: 1.964885
Validation accuracy: 51.60% 

Epoch 43
-------------------------------
Training loss: 1.720631  [16/60600]
Training loss: 2.473685  [1616/60600]
Training loss: 1.942980  [3216/60600]
Training loss: 2.075915  [4816/60600]
Training loss: 1.550879  [6416/60600]
Training loss: 2.024841  [8016/60600]
Training loss: 1.924432  [9616/60600]
Training loss: 2.107489  [11216/60600]
Training loss: 1.896900  [12816/60600]
Training loss: 1.508041  [14416/60600]
Training loss: 2.255241  [16016/60600]
Training loss: 1.475055  [17616/60600]
Training loss: 1.556511  [19216/60600]
Training loss: 2.139877  [20816/60600]
Training loss: 1.243845  [22416/60600]
Training loss: 1.594257  [24016/60600]
Training loss: 2.274344  [25616/60600]
Training loss: 1.650280  [27216/60600]
Training loss: 1.468556  [28816/60600]
Training loss: 1.265194  [30416/60600]
Training loss: 2.576492  [32016/60600]
Training loss: 2.084022  [33616/60600]
Training loss: 1.472773  [35216/60600]
Training loss: 2.301515  [36816/60600]
Training loss: 1.542941  [38416/60600]
Training loss: 1.460505  [40016/60600]
Training loss: 2.186365  [41616/60600]
Training loss: 1.415868  [43216/60600]
Training loss: 2.896439  [44816/60600]
Training loss: 1.563928  [46416/60600]
Training loss: 1.302236  [48016/60600]
Training loss: 0.929656  [49616/60600]
Training loss: 2.192938  [51216/60600]
Training loss: 1.872230  [52816/60600]
Training loss: 1.616815  [54416/60600]
Training loss: 1.306994  [56016/60600]
Training loss: 2.303966  [57616/60600]
Training loss: 1.726264  [59216/60600]
Training accuracy: 53.76 %
Validation loss: 1.944414
Validation accuracy: 51.87% 

Epoch 44
-------------------------------
Training loss: 1.319256  [16/60600]
Training loss: 1.620786  [1616/60600]
Training loss: 1.534146  [3216/60600]
Training loss: 1.867339  [4816/60600]
Training loss: 2.498099  [6416/60600]
Training loss: 1.764876  [8016/60600]
Training loss: 1.904392  [9616/60600]
Training loss: 1.250460  [11216/60600]
Training loss: 1.401487  [12816/60600]
Training loss: 2.835047  [14416/60600]
Training loss: 1.952789  [16016/60600]
Training loss: 1.632734  [17616/60600]
Training loss: 2.260693  [19216/60600]
Training loss: 2.506916  [20816/60600]
Training loss: 2.312868  [22416/60600]
Training loss: 1.794601  [24016/60600]
Training loss: 2.014655  [25616/60600]
Training loss: 2.479191  [27216/60600]
Training loss: 2.418824  [28816/60600]
Training loss: 1.065341  [30416/60600]
Training loss: 1.893801  [32016/60600]
Training loss: 2.790221  [33616/60600]
Training loss: 1.606421  [35216/60600]
Training loss: 2.128143  [36816/60600]
Training loss: 2.256083  [38416/60600]
Training loss: 1.455716  [40016/60600]
Training loss: 1.709383  [41616/60600]
Training loss: 1.709362  [43216/60600]
Training loss: 1.915409  [44816/60600]
Training loss: 2.274736  [46416/60600]
Training loss: 1.489659  [48016/60600]
Training loss: 2.173368  [49616/60600]
Training loss: 1.560757  [51216/60600]
Training loss: 1.630901  [52816/60600]
Training loss: 1.079305  [54416/60600]
Training loss: 1.392005  [56016/60600]
Training loss: 1.752199  [57616/60600]
Training loss: 1.310471  [59216/60600]
Training accuracy: 53.76 %
Validation loss: 1.925230
Validation accuracy: 52.55% 

Epoch 45
-------------------------------
Training loss: 1.575613  [16/60600]
Training loss: 1.802955  [1616/60600]
Training loss: 1.674505  [3216/60600]
Training loss: 1.900287  [4816/60600]
Training loss: 1.964505  [6416/60600]
Training loss: 2.285313  [8016/60600]
Training loss: 2.768858  [9616/60600]
Training loss: 2.267638  [11216/60600]
Training loss: 1.601907  [12816/60600]
Training loss: 2.474437  [14416/60600]
Training loss: 2.266247  [16016/60600]
Training loss: 2.047651  [17616/60600]
Training loss: 1.940283  [19216/60600]
Training loss: 2.291538  [20816/60600]
Training loss: 2.132317  [22416/60600]
Training loss: 2.079882  [24016/60600]
Training loss: 1.795284  [25616/60600]
Training loss: 1.426863  [27216/60600]
Training loss: 1.466896  [28816/60600]
Training loss: 1.780447  [30416/60600]
Training loss: 1.782329  [32016/60600]
Training loss: 2.312355  [33616/60600]
Training loss: 1.199388  [35216/60600]
Training loss: 1.662580  [36816/60600]
Training loss: 1.534218  [38416/60600]
Training loss: 1.880504  [40016/60600]
Training loss: 2.248305  [41616/60600]
Training loss: 2.283580  [43216/60600]
Training loss: 2.631773  [44816/60600]
Training loss: 1.563241  [46416/60600]
Training loss: 2.198849  [48016/60600]
Training loss: 1.718914  [49616/60600]
Training loss: 1.807985  [51216/60600]
Training loss: 1.480760  [52816/60600]
Training loss: 1.803530  [54416/60600]
Training loss: 2.550429  [56016/60600]
Training loss: 2.028109  [57616/60600]
Training loss: 1.781850  [59216/60600]
Training accuracy: 53.79 %
Validation loss: 1.939432
Validation accuracy: 52.07% 

Epoch 46
-------------------------------
Training loss: 2.168854  [16/60600]
Training loss: 1.889562  [1616/60600]
Training loss: 3.107286  [3216/60600]
Training loss: 1.621330  [4816/60600]
Training loss: 1.933779  [6416/60600]
Training loss: 2.098189  [8016/60600]
Training loss: 1.020919  [9616/60600]
Training loss: 2.304469  [11216/60600]
Training loss: 1.517971  [12816/60600]
Training loss: 1.294394  [14416/60600]
Training loss: 2.400826  [16016/60600]
Training loss: 1.853897  [17616/60600]
Training loss: 2.413867  [19216/60600]
Training loss: 2.082235  [20816/60600]
Training loss: 2.370688  [22416/60600]
Training loss: 2.218046  [24016/60600]
Training loss: 2.527006  [25616/60600]
Training loss: 2.026289  [27216/60600]
Training loss: 2.000864  [28816/60600]
Training loss: 2.315240  [30416/60600]
Training loss: 2.354096  [32016/60600]
Training loss: 1.783028  [33616/60600]
Training loss: 2.153184  [35216/60600]
Training loss: 1.851741  [36816/60600]
Training loss: 1.922092  [38416/60600]
Training loss: 2.043441  [40016/60600]
Training loss: 2.036887  [41616/60600]
Training loss: 1.529476  [43216/60600]
Training loss: 2.731061  [44816/60600]
Training loss: 2.563934  [46416/60600]
Training loss: 1.481522  [48016/60600]
Training loss: 1.209650  [49616/60600]
Training loss: 1.472864  [51216/60600]
Training loss: 1.411789  [52816/60600]
Training loss: 1.951881  [54416/60600]
Training loss: 1.530713  [56016/60600]
Training loss: 1.227643  [57616/60600]
Training loss: 1.535775  [59216/60600]
Training accuracy: 53.99 %
Validation loss: 1.938017
Validation accuracy: 52.26% 

Epoch 47
-------------------------------
Training loss: 2.544744  [16/60600]
Training loss: 1.773257  [1616/60600]
Training loss: 1.640666  [3216/60600]
Training loss: 1.697502  [4816/60600]
Training loss: 2.040352  [6416/60600]
Training loss: 1.824451  [8016/60600]
Training loss: 1.706241  [9616/60600]
Training loss: 1.609147  [11216/60600]
Training loss: 2.216596  [12816/60600]
Training loss: 1.869209  [14416/60600]
Training loss: 1.933965  [16016/60600]
Training loss: 1.660120  [17616/60600]
Training loss: 2.180192  [19216/60600]
Training loss: 1.959724  [20816/60600]
Training loss: 1.883327  [22416/60600]
Training loss: 2.169267  [24016/60600]
Training loss: 1.089318  [25616/60600]
Training loss: 1.778263  [27216/60600]
Training loss: 2.450032  [28816/60600]
Training loss: 2.055028  [30416/60600]
Training loss: 1.303857  [32016/60600]
Training loss: 1.641250  [33616/60600]
Training loss: 1.736322  [35216/60600]
Training loss: 1.634122  [36816/60600]
Training loss: 2.265909  [38416/60600]
Training loss: 1.890507  [40016/60600]
Training loss: 2.103503  [41616/60600]
Training loss: 1.301107  [43216/60600]
Training loss: 1.701960  [44816/60600]
Training loss: 2.096870  [46416/60600]
Training loss: 1.862973  [48016/60600]
Training loss: 1.728987  [49616/60600]
Training loss: 2.431816  [51216/60600]
Training loss: 1.257348  [52816/60600]
Training loss: 1.914757  [54416/60600]
Training loss: 1.831794  [56016/60600]
Training loss: 1.726362  [57616/60600]
Training loss: 1.677332  [59216/60600]
Training accuracy: 53.85 %
Validation loss: 1.949760
Validation accuracy: 51.80% 

Epoch 48
-------------------------------
Training loss: 2.201190  [16/60600]
Training loss: 2.241783  [1616/60600]
Training loss: 2.380399  [3216/60600]
Training loss: 1.970900  [4816/60600]
Training loss: 1.712626  [6416/60600]
Training loss: 1.688659  [8016/60600]
Training loss: 1.386035  [9616/60600]
Training loss: 1.906132  [11216/60600]
Training loss: 2.857627  [12816/60600]
Training loss: 2.219126  [14416/60600]
Training loss: 1.623642  [16016/60600]
Training loss: 2.273608  [17616/60600]
Training loss: 2.109042  [19216/60600]
Training loss: 2.479257  [20816/60600]
Training loss: 2.116819  [22416/60600]
Training loss: 1.664616  [24016/60600]
Training loss: 2.246059  [25616/60600]
Training loss: 2.122251  [27216/60600]
Training loss: 1.679764  [28816/60600]
Training loss: 2.321577  [30416/60600]
Training loss: 2.281405  [32016/60600]
Training loss: 2.475323  [33616/60600]
Training loss: 1.561637  [35216/60600]
Training loss: 1.735552  [36816/60600]
Training loss: 2.125652  [38416/60600]
Training loss: 1.982105  [40016/60600]
Training loss: 2.089816  [41616/60600]
Training loss: 1.452616  [43216/60600]
Training loss: 1.560632  [44816/60600]
Training loss: 2.349360  [46416/60600]
Training loss: 2.241882  [48016/60600]
Training loss: 2.520593  [49616/60600]
Training loss: 1.300576  [51216/60600]
Training loss: 2.695659  [52816/60600]
Training loss: 2.042463  [54416/60600]
Training loss: 1.572297  [56016/60600]
Training loss: 2.356349  [57616/60600]
Training loss: 2.490773  [59216/60600]
Training accuracy: 53.98 %
Validation loss: 1.925545
Validation accuracy: 52.50% 

Epoch 49
-------------------------------
Training loss: 1.721923  [16/60600]
Training loss: 2.004076  [1616/60600]
Training loss: 1.567453  [3216/60600]
Training loss: 1.423476  [4816/60600]
Training loss: 1.277731  [6416/60600]
Training loss: 2.116508  [8016/60600]
Training loss: 2.282644  [9616/60600]
Training loss: 1.509197  [11216/60600]
Training loss: 2.484421  [12816/60600]
Training loss: 1.858496  [14416/60600]
Training loss: 1.808976  [16016/60600]
Training loss: 2.279986  [17616/60600]
Training loss: 1.373675  [19216/60600]
Training loss: 1.843301  [20816/60600]
Training loss: 2.716016  [22416/60600]
Training loss: 1.890223  [24016/60600]
Training loss: 2.481972  [25616/60600]
Training loss: 1.278005  [27216/60600]
Training loss: 1.876046  [28816/60600]
Training loss: 1.633332  [30416/60600]
Training loss: 3.006078  [32016/60600]
Training loss: 2.108532  [33616/60600]
Training loss: 2.018412  [35216/60600]
Training loss: 1.783479  [36816/60600]
Training loss: 1.718769  [38416/60600]
Training loss: 2.329939  [40016/60600]
Training loss: 1.604815  [41616/60600]
Training loss: 1.480085  [43216/60600]
Training loss: 1.619337  [44816/60600]
Training loss: 1.590601  [46416/60600]
Training loss: 1.726433  [48016/60600]
Training loss: 1.665596  [49616/60600]
Training loss: 1.219366  [51216/60600]
Training loss: 1.400926  [52816/60600]
Training loss: 2.111862  [54416/60600]
Training loss: 1.268797  [56016/60600]
Training loss: 2.259644  [57616/60600]
Training loss: 1.760274  [59216/60600]
Training accuracy: 54.17 %
Validation loss: 1.933068
Validation accuracy: 52.28% 

Early stopping
Done!

Elapsed time: 42404.453067302704 seconds

Current time: 08:56:58
                         precision    recall  f1-score   support

              apple_pie       0.39      0.21      0.27       250
         baby_back_ribs       0.53      0.59      0.56       250
                baklava       0.51      0.58      0.54       250
         beef_carpaccio       0.49      0.63      0.55       250
           beef_tartare       0.50      0.33      0.40       250
             beet_salad       0.40      0.45      0.42       250
               beignets       0.68      0.72      0.70       250
               bibimbap       0.66      0.77      0.71       250
          bread_pudding       0.36      0.32      0.34       250
      breakfast_burrito       0.44      0.42      0.43       250
             bruschetta       0.41      0.40      0.41       250
           caesar_salad       0.45      0.58      0.50       250
                cannoli       0.62      0.61      0.61       250
          caprese_salad       0.52      0.52      0.52       250
            carrot_cake       0.57      0.55      0.56       250
                ceviche       0.33      0.26      0.29       250
             cheesecake       0.52      0.51      0.52       250
           cheese_plate       0.46      0.43      0.45       250
          chicken_curry       0.51      0.37      0.43       250
     chicken_quesadilla       0.50      0.56      0.53       250
          chicken_wings       0.64      0.66      0.65       250
         chocolate_cake       0.52      0.47      0.49       250
       chocolate_mousse       0.44      0.40      0.42       250
                churros       0.62      0.74      0.68       250
           clam_chowder       0.65      0.75      0.70       250
          club_sandwich       0.53      0.65      0.58       250
             crab_cakes       0.40      0.30      0.34       250
           creme_brulee       0.63      0.71      0.67       250
          croque_madame       0.64      0.58      0.61       250
              cup_cakes       0.66      0.79      0.72       250
           deviled_eggs       0.66      0.72      0.69       250
                 donuts       0.69      0.63      0.66       250
              dumplings       0.77      0.83      0.80       250
                edamame       0.90      0.95      0.92       250
          eggs_benedict       0.66      0.60      0.63       250
              escargots       0.66      0.72      0.69       250
                falafel       0.53      0.46      0.49       250
           filet_mignon       0.41      0.31      0.35       250
         fish_and_chips       0.71      0.56      0.63       250
              foie_gras       0.35      0.30      0.32       250
           french_fries       0.62      0.80      0.70       250
      french_onion_soup       0.63      0.68      0.66       250
           french_toast       0.50      0.38      0.44       250
         fried_calamari       0.53      0.63      0.57       250
             fried_rice       0.52      0.68      0.59       250
          frozen_yogurt       0.70      0.78      0.74       250
           garlic_bread       0.61      0.48      0.54       250
                gnocchi       0.42      0.41      0.41       250
            greek_salad       0.50      0.61      0.55       250
grilled_cheese_sandwich       0.48      0.36      0.41       250
         grilled_salmon       0.44      0.38      0.41       250
              guacamole       0.67      0.74      0.71       250
                  gyoza       0.64      0.65      0.64       250
              hamburger       0.63      0.55      0.59       250
      hot_and_sour_soup       0.73      0.85      0.79       250
                hot_dog       0.68      0.70      0.69       250
       huevos_rancheros       0.43      0.33      0.37       250
                 hummus       0.43      0.38      0.41       250
              ice_cream       0.67      0.58      0.62       250
                lasagna       0.45      0.47      0.46       250
         lobster_bisque       0.65      0.75      0.69       250
  lobster_roll_sandwich       0.64      0.56      0.60       250
    macaroni_and_cheese       0.59      0.52      0.55       250
               macarons       0.81      0.88      0.84       250
              miso_soup       0.75      0.80      0.77       250
                mussels       0.73      0.80      0.76       250
                 nachos       0.45      0.46      0.46       250
               omelette       0.47      0.38      0.42       250
            onion_rings       0.79      0.72      0.75       250
                oysters       0.71      0.85      0.78       250
               pad_thai       0.58      0.78      0.66       250
                 paella       0.58      0.50      0.54       250
               pancakes       0.58      0.58      0.58       250
            panna_cotta       0.43      0.54      0.48       250
            peking_duck       0.57      0.60      0.58       250
                    pho       0.76      0.81      0.78       250
                  pizza       0.64      0.82      0.72       250
              pork_chop       0.37      0.34      0.35       250
                poutine       0.70      0.69      0.70       250
              prime_rib       0.53      0.66      0.59       250
   pulled_pork_sandwich       0.62      0.39      0.48       250
                  ramen       0.55      0.62      0.58       250
                ravioli       0.43      0.32      0.37       250
        red_velvet_cake       0.61      0.73      0.67       250
                risotto       0.46      0.44      0.45       250
                 samosa       0.53      0.56      0.54       250
                sashimi       0.63      0.78      0.70       250
               scallops       0.38      0.35      0.37       250
          seaweed_salad       0.64      0.78      0.70       250
       shrimp_and_grits       0.48      0.46      0.47       250
    spaghetti_bolognese       0.70      0.66      0.68       250
    spaghetti_carbonara       0.78      0.79      0.78       250
           spring_rolls       0.62      0.64      0.63       250
                  steak       0.44      0.23      0.30       250
   strawberry_shortcake       0.53      0.63      0.58       250
                  sushi       0.55      0.56      0.55       250
                  tacos       0.38      0.31      0.34       250
               takoyaki       0.63      0.55      0.59       250
               tiramisu       0.41      0.46      0.44       250
           tuna_tartare       0.50      0.26      0.34       250
                waffles       0.59      0.69      0.63       250

               accuracy                           0.57     25250
              macro avg       0.56      0.57      0.56     25250
           weighted avg       0.56      0.57      0.56     25250

Test accuracy: 0.5706534653465346
The metadata of the previous execution is...
{'training_images_percentage': 1, 'epochs': 55, 'learning_rate': 0.001, 'batch_size': 16, 'data_augmentation': 'no', 'subject_driven_technique': 'stable-diffusion-prompt', 'number_of_samples': 5, 'images_to_generate': 5, 'path_to_dataset': '../../../../../../work3/s226536/datasets/food-101', 'DATA_DIR': '../../../../../../work3/s226536/datasets'}

Preparing next execution...
Finished preparing next execution...
