Tue Mar 21 15:45:30 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100 80GB PCIe           On | 00000000:65:00.0 Off |                    0 |
| N/A   28C    P0               42W / 300W|      0MiB / 81920MiB |      0%   E. Process |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Using cuda device
PetsModel(
  (network): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=37, bias=True)
  )
)
Epoch 1
-------------------------------
Training loss: 3.883330  [64/3680]
Training accuracy: 6.63 %
Test loss: 3.620347
Test accuracy: 6.92% 

3.41105580329895
0.06630434782608696
3.620346895579634
0.06922867266285092
Epoch 2
-------------------------------
Training loss: 3.601108  [64/3680]
Training accuracy: 11.14 %
Test loss: 3.425254
Test accuracy: 11.77% 

3.331263542175293
0.11141304347826086
3.4252540736362853
0.11774325429272281
Epoch 3
-------------------------------
Training loss: 3.463036  [64/3680]
Training accuracy: 17.80 %
Test loss: 3.261114
Test accuracy: 17.50% 

3.1346356868743896
0.17798913043478262
3.26111350799429
0.1749795584627964
Epoch 4
-------------------------------
Training loss: 3.225970  [64/3680]
Training accuracy: 25.68 %
Test loss: 3.110666
Test accuracy: 23.79% 

3.1172738075256348
0.25679347826086957
3.110666065380491
0.23793949304987735
Epoch 5
-------------------------------
Training loss: 3.101728  [64/3680]
Training accuracy: 32.80 %
Test loss: 2.967650
Test accuracy: 31.02% 

2.9957942962646484
0.3279891304347826
2.9676500147786635
0.3101662578359226
Epoch 6
-------------------------------
Training loss: 2.956514  [64/3680]
Training accuracy: 39.92 %
Test loss: 2.832738
Test accuracy: 38.57% 

2.788609027862549
0.39918478260869567
2.832737819901828
0.3856636685745435
Epoch 7
-------------------------------
Training loss: 2.767251  [64/3680]
Training accuracy: 47.47 %
Test loss: 2.703460
Test accuracy: 44.70% 

2.835010051727295
0.4747282608695652
2.7034600233209543
0.4469882801853366
Epoch 8
-------------------------------
Training loss: 2.639783  [64/3680]
Training accuracy: 53.23 %
Test loss: 2.581031
Test accuracy: 51.05% 

2.572997808456421
0.5323369565217392
2.581030516788877
0.5104933224311802
Epoch 9
-------------------------------
Training loss: 2.615695  [64/3680]
Training accuracy: 58.42 %
Test loss: 2.463119
Test accuracy: 55.68% 

2.5889298915863037
0.5842391304347826
2.46311944517596
0.5568274734260016
Epoch 10
-------------------------------
Training loss: 2.377048  [64/3680]
Training accuracy: 61.77 %
Test loss: 2.360198
Test accuracy: 60.02% 

2.205939292907715
0.6176630434782608
2.3601975153232444
0.6001635322976288
Epoch 11
-------------------------------
Training loss: 2.418897  [64/3680]
Training accuracy: 65.38 %
Test loss: 2.260132
Test accuracy: 63.37% 

2.2980387210845947
0.653804347826087
2.2601316180722466
0.633687653311529
Epoch 12
-------------------------------
Training loss: 2.248351  [64/3680]
Training accuracy: 67.64 %
Test loss: 2.164292
Test accuracy: 65.99% 

2.2889318466186523
0.6763586956521739
2.1642924608855414
0.659852820932134
Epoch 13
-------------------------------
Training loss: 2.057675  [64/3680]
Training accuracy: 71.09 %
Test loss: 2.065543
Test accuracy: 68.71% 

1.9767504930496216
0.7108695652173913
2.0655434912648696
0.6871082038702644
Epoch 14
-------------------------------
Training loss: 2.040283  [64/3680]
Training accuracy: 72.53 %
Test loss: 1.986905
Test accuracy: 70.81% 

2.346581220626831
0.7252717391304347
1.9869050404121136
0.7080948487326247
Epoch 15
-------------------------------
Training loss: 2.075508  [64/3680]
Training accuracy: 74.32 %
Test loss: 1.915192
Test accuracy: 72.55% 

1.9804824590682983
0.7432065217391305
1.9151923738676926
0.725538293813028
Epoch 16
-------------------------------
Training loss: 1.953038  [64/3680]
Training accuracy: 75.62 %
Test loss: 1.827781
Test accuracy: 73.83% 

1.8676012754440308
0.75625
1.8277814141635238
0.7383483237939493
Epoch 17
-------------------------------
Training loss: 1.598202  [64/3680]
Training accuracy: 77.12 %
Test loss: 1.754854
Test accuracy: 74.68% 

1.936905860900879
0.7711956521739131
1.7548537665400012
0.7467974925047697
Epoch 18
-------------------------------
Training loss: 1.786974  [64/3680]
Training accuracy: 78.32 %
Test loss: 1.699817
Test accuracy: 75.93% 

1.7101558446884155
0.7831521739130435
1.699817336838821
0.7593349686563097
Epoch 19
-------------------------------
Training loss: 1.575480  [64/3680]
Training accuracy: 79.16 %
Test loss: 1.630421
Test accuracy: 76.75% 

1.583992600440979
0.7915760869565217
1.6304207999130775
0.7675115835377487
Epoch 20
-------------------------------
Training loss: 1.643775  [64/3680]
Training accuracy: 80.46 %
Test loss: 1.576151
Test accuracy: 77.95% 

1.7186543941497803
0.8046195652173913
1.576151114085625
0.779503952030526
Epoch 21
-------------------------------
Training loss: 1.533839  [64/3680]
Training accuracy: 81.58 %
Test loss: 1.526108
Test accuracy: 78.74% 

1.5359115600585938
0.8157608695652174
1.5261078600225777
0.7874080130825838
Epoch 22
-------------------------------
Training loss: 1.632473  [64/3680]
Training accuracy: 81.44 %
Test loss: 1.477161
Test accuracy: 79.37% 

1.6762360334396362
0.8144021739130435
1.4771612122141082
0.7936767511583538
Epoch 23
-------------------------------
Training loss: 1.481270  [64/3680]
Training accuracy: 82.53 %
Test loss: 1.423897
Test accuracy: 79.91% 

1.3033545017242432
0.8252717391304348
1.423897317771254
0.7991278277459798
Epoch 24
-------------------------------
Training loss: 1.273440  [64/3680]
Training accuracy: 83.12 %
Test loss: 1.381589
Test accuracy: 80.92% 

1.4753779172897339
0.83125
1.3815890201206864
0.809212319433088
Epoch 25
-------------------------------
Training loss: 1.281967  [64/3680]
Training accuracy: 84.02 %
Test loss: 1.349043
Test accuracy: 81.00% 

1.3654841184616089
0.8402173913043478
1.349043237751928
0.8100299809212319
Epoch 26
-------------------------------
Training loss: 1.306128  [64/3680]
Training accuracy: 84.32 %
Test loss: 1.309235
Test accuracy: 81.36% 

1.2574950456619263
0.8432065217391305
1.3092346458599484
0.8135731807031888
Epoch 27
-------------------------------
Training loss: 1.106959  [64/3680]
Training accuracy: 84.73 %
Test loss: 1.275388
Test accuracy: 82.09% 

1.2258328199386597
0.8472826086956522
1.275388450458132
0.820932134096484
Epoch 28
-------------------------------
Training loss: 1.156053  [64/3680]
Training accuracy: 85.87 %
Test loss: 1.236255
Test accuracy: 82.50% 

1.2522021532058716
0.8586956521739131
1.2362551935787858
0.8250204415372036
Epoch 29
-------------------------------
Training loss: 1.241615  [64/3680]
Training accuracy: 85.87 %
Test loss: 1.195927
Test accuracy: 82.86% 

1.1340153217315674
0.8586956521739131
1.19592674539007
0.8285636413191605
Epoch 30
-------------------------------
Training loss: 1.242317  [64/3680]
Training accuracy: 85.92 %
Test loss: 1.183912
Test accuracy: 82.77% 

0.9883255958557129
0.8592391304347826
1.1839123553243176
0.8277459798310166
Epoch 31
-------------------------------
Training loss: 1.065921  [64/3680]
Training accuracy: 86.25 %
Test loss: 1.154710
Test accuracy: 83.05% 

1.0465922355651855
0.8625
1.1547101026978985
0.8304715181248297
Epoch 32
-------------------------------
Training loss: 1.179227  [64/3680]
Training accuracy: 86.03 %
Test loss: 1.128252
Test accuracy: 83.24% 

1.1765658855438232
0.8603260869565217
1.1282516769294082
0.8323793949304987
Epoch 33
-------------------------------
Training loss: 1.123149  [64/3680]
Training accuracy: 86.88 %
Test loss: 1.098055
Test accuracy: 83.67% 

1.4329332113265991
0.86875
1.0980554058634002
0.8367402562005997
Epoch 34
-------------------------------
Training loss: 0.968530  [64/3680]
Training accuracy: 86.77 %
Test loss: 1.069819
Test accuracy: 83.65% 

1.1790677309036255
0.8676630434782608
1.0698185511704148
0.8364677023712184
Epoch 35
-------------------------------
Training loss: 1.019283  [64/3680]
Training accuracy: 86.68 %
Test loss: 1.053226
Test accuracy: 84.08% 

0.8462691307067871
0.8668478260869565
1.0532255614625996
0.8408285636413192
Epoch 36
-------------------------------
Training loss: 1.115506  [64/3680]
Training accuracy: 87.31 %
Test loss: 1.029219
Test accuracy: 84.68% 

1.1123855113983154
0.8730978260869565
1.029218799081342
0.8468247478877078
Epoch 37
-------------------------------
Training loss: 0.834286  [64/3680]
Training accuracy: 87.96 %
Test loss: 1.010009
Test accuracy: 84.41% 

0.9296984076499939
0.8796195652173913
1.010008809895351
0.8440992095938948
Epoch 38
-------------------------------
Training loss: 1.079685  [64/3680]
Training accuracy: 88.21 %
Test loss: 0.997936
Test accuracy: 84.52% 

1.034839391708374
0.8820652173913044
0.9979363536012584
0.84518942491142
Epoch 39
-------------------------------
Training loss: 0.747558  [64/3680]
Training accuracy: 88.18 %
Test loss: 0.974912
Test accuracy: 84.46% 

0.9574072957038879
0.8817934782608695
0.9749124317333616
0.8446443172526574
Epoch 40
-------------------------------
Training loss: 0.878121  [64/3680]
Training accuracy: 88.21 %
Test loss: 0.957642
Test accuracy: 85.01% 

1.1440030336380005
0.8820652173913044
0.9576418780047318
0.8500953938402834
Epoch 41
-------------------------------
Training loss: 0.884242  [64/3680]
Training accuracy: 88.67 %
Test loss: 0.939436
Test accuracy: 85.20% 

0.8231463432312012
0.8866847826086957
0.9394356639220797
0.8520032706459526
Epoch 42
-------------------------------
Training loss: 0.894153  [64/3680]
Training accuracy: 88.97 %
Test loss: 0.926843
Test accuracy: 85.28% 

0.9859360456466675
0.8896739130434783
0.9268434479318816
0.8528209321340965
Epoch 43
-------------------------------
Training loss: 1.030247  [64/3680]
Training accuracy: 89.13 %
Test loss: 0.899191
Test accuracy: 85.64% 

0.8367775082588196
0.8913043478260869
0.8991905656354181
0.8563641319160534
Epoch 44
-------------------------------
Training loss: 0.911308  [64/3680]
Training accuracy: 89.18 %
Test loss: 0.892647
Test accuracy: 85.39% 

0.8900032639503479
0.8918478260869566
0.8926470618823479
0.8539111474516217
Epoch 45
-------------------------------
Training loss: 0.779666  [64/3680]
Training accuracy: 89.46 %
Test loss: 0.885292
Test accuracy: 85.45% 

0.7891139984130859
0.8945652173913043
0.8852915311681813
0.8544562551103843
Epoch 46
-------------------------------
Training loss: 0.818326  [64/3680]
Training accuracy: 89.54 %
Test loss: 0.866920
Test accuracy: 85.66% 

1.0033104419708252
0.8953804347826086
0.8669197600463341
0.8566366857454347
Epoch 47
-------------------------------
Training loss: 0.827523  [64/3680]
Training accuracy: 89.21 %
Test loss: 0.852790
Test accuracy: 85.53% 

0.8634525537490845
0.8921195652173913
0.8527902868287317
0.8552739165985282
Epoch 48
-------------------------------
Training loss: 0.725440  [64/3680]
Training accuracy: 89.38 %
Test loss: 0.846905
Test accuracy: 85.94% 

0.8345094919204712
0.89375
0.8469047823856617
0.8593622240392478
Epoch 49
-------------------------------
Training loss: 0.736323  [64/3680]
Training accuracy: 89.59 %
Test loss: 0.826435
Test accuracy: 85.96% 

0.6476697325706482
0.8959239130434783
0.8264346739341473
0.8596347778686291
Epoch 50
-------------------------------
Training loss: 0.706662  [64/3680]
Training accuracy: 90.05 %
Test loss: 0.822950
Test accuracy: 85.96% 

0.8416894674301147
0.9005434782608696
0.8229502665585485
0.8596347778686291
Epoch 51
-------------------------------
Training loss: 0.800095  [64/3680]
Training accuracy: 90.14 %
Test loss: 0.810128
Test accuracy: 85.83% 

0.91004478931427
0.9013586956521739
0.8101283794847028
0.8582720087217225
Epoch 52
-------------------------------
Training loss: 0.746880  [64/3680]
Training accuracy: 90.16 %
Test loss: 0.791163
Test accuracy: 86.29% 

0.8105671405792236
0.9016304347826087
0.7911633119500917
0.8629054238212047
Epoch 53
-------------------------------
Training loss: 0.705720  [64/3680]
Training accuracy: 90.05 %
Test loss: 0.784434
Test accuracy: 86.35% 

0.7759617567062378
0.9005434782608696
0.7844343730087938
0.8634505314799673
Epoch 54
-------------------------------
Training loss: 0.708266  [64/3680]
Training accuracy: 89.70 %
Test loss: 0.779759
Test accuracy: 86.15% 

0.7523427605628967
0.8970108695652174
0.779758895265645
0.8615426546742981
Epoch 55
-------------------------------
Training loss: 0.772560  [64/3680]
Training accuracy: 90.30 %
Test loss: 0.780734
Test accuracy: 86.21% 

0.88262540102005
0.9029891304347826
0.7807344938146656
0.8620877623330607
Done!
[3.41105580329895, 3.331263542175293, 3.1346356868743896, 3.1172738075256348, 2.9957942962646484, 2.788609027862549, 2.835010051727295, 2.572997808456421, 2.5889298915863037, 2.205939292907715, 2.2980387210845947, 2.2889318466186523, 1.9767504930496216, 2.346581220626831, 1.9804824590682983, 1.8676012754440308, 1.936905860900879, 1.7101558446884155, 1.583992600440979, 1.7186543941497803, 1.5359115600585938, 1.6762360334396362, 1.3033545017242432, 1.4753779172897339, 1.3654841184616089, 1.2574950456619263, 1.2258328199386597, 1.2522021532058716, 1.1340153217315674, 0.9883255958557129, 1.0465922355651855, 1.1765658855438232, 1.4329332113265991, 1.1790677309036255, 0.8462691307067871, 1.1123855113983154, 0.9296984076499939, 1.034839391708374, 0.9574072957038879, 1.1440030336380005, 0.8231463432312012, 0.9859360456466675, 0.8367775082588196, 0.8900032639503479, 0.7891139984130859, 1.0033104419708252, 0.8634525537490845, 0.8345094919204712, 0.6476697325706482, 0.8416894674301147, 0.91004478931427, 0.8105671405792236, 0.7759617567062378, 0.7523427605628967, 0.88262540102005]
[3.620346895579634, 3.4252540736362853, 3.26111350799429, 3.110666065380491, 2.9676500147786635, 2.832737819901828, 2.7034600233209543, 2.581030516788877, 2.46311944517596, 2.3601975153232444, 2.2601316180722466, 2.1642924608855414, 2.0655434912648696, 1.9869050404121136, 1.9151923738676926, 1.8277814141635238, 1.7548537665400012, 1.699817336838821, 1.6304207999130775, 1.576151114085625, 1.5261078600225777, 1.4771612122141082, 1.423897317771254, 1.3815890201206864, 1.349043237751928, 1.3092346458599484, 1.275388450458132, 1.2362551935787858, 1.19592674539007, 1.1839123553243176, 1.1547101026978985, 1.1282516769294082, 1.0980554058634002, 1.0698185511704148, 1.0532255614625996, 1.029218799081342, 1.010008809895351, 0.9979363536012584, 0.9749124317333616, 0.9576418780047318, 0.9394356639220797, 0.9268434479318816, 0.8991905656354181, 0.8926470618823479, 0.8852915311681813, 0.8669197600463341, 0.8527902868287317, 0.8469047823856617, 0.8264346739341473, 0.8229502665585485, 0.8101283794847028, 0.7911633119500917, 0.7844343730087938, 0.779758895265645, 0.7807344938146656]
[0.06630434782608696, 0.11141304347826086, 0.17798913043478262, 0.25679347826086957, 0.3279891304347826, 0.39918478260869567, 0.4747282608695652, 0.5323369565217392, 0.5842391304347826, 0.6176630434782608, 0.653804347826087, 0.6763586956521739, 0.7108695652173913, 0.7252717391304347, 0.7432065217391305, 0.75625, 0.7711956521739131, 0.7831521739130435, 0.7915760869565217, 0.8046195652173913, 0.8157608695652174, 0.8144021739130435, 0.8252717391304348, 0.83125, 0.8402173913043478, 0.8432065217391305, 0.8472826086956522, 0.8586956521739131, 0.8586956521739131, 0.8592391304347826, 0.8625, 0.8603260869565217, 0.86875, 0.8676630434782608, 0.8668478260869565, 0.8730978260869565, 0.8796195652173913, 0.8820652173913044, 0.8817934782608695, 0.8820652173913044, 0.8866847826086957, 0.8896739130434783, 0.8913043478260869, 0.8918478260869566, 0.8945652173913043, 0.8953804347826086, 0.8921195652173913, 0.89375, 0.8959239130434783, 0.9005434782608696, 0.9013586956521739, 0.9016304347826087, 0.9005434782608696, 0.8970108695652174, 0.9029891304347826]
[0.06922867266285092, 0.11774325429272281, 0.1749795584627964, 0.23793949304987735, 0.3101662578359226, 0.3856636685745435, 0.4469882801853366, 0.5104933224311802, 0.5568274734260016, 0.6001635322976288, 0.633687653311529, 0.659852820932134, 0.6871082038702644, 0.7080948487326247, 0.725538293813028, 0.7383483237939493, 0.7467974925047697, 0.7593349686563097, 0.7675115835377487, 0.779503952030526, 0.7874080130825838, 0.7936767511583538, 0.7991278277459798, 0.809212319433088, 0.8100299809212319, 0.8135731807031888, 0.820932134096484, 0.8250204415372036, 0.8285636413191605, 0.8277459798310166, 0.8304715181248297, 0.8323793949304987, 0.8367402562005997, 0.8364677023712184, 0.8408285636413192, 0.8468247478877078, 0.8440992095938948, 0.84518942491142, 0.8446443172526574, 0.8500953938402834, 0.8520032706459526, 0.8528209321340965, 0.8563641319160534, 0.8539111474516217, 0.8544562551103843, 0.8566366857454347, 0.8552739165985282, 0.8593622240392478, 0.8596347778686291, 0.8596347778686291, 0.8582720087217225, 0.8629054238212047, 0.8634505314799673, 0.8615426546742981, 0.8620877623330607]
