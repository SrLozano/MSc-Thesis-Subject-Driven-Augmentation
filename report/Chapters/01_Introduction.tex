\chapter{Introduction}

Text-based image generation models have reached a point of maturity where they are capable of generating high-fidelity photorealistic images \cite{ho2020denoising, dhariwal2021diffusion}. These images have reached a level where they are usable in real projects and even indistinguishable from an actual image by most of the public \cite{newyorktimesbelieve}. In addition, the availability of text-to-image models has been increased by private companies, educational institutions, and the open-source community. Gigantic models such as Stable Diffusion are available in a completely accessible way for anyone wanting to try or experiment with it. On the other hand, this growing availability allows researchers worldwide to develop methods that enable more effective control of generative models. In this line, the work of Textual inversion \cite{gal2022image}, Dreambooth \cite{ruiz2022dreambooth}, and ControlNet \cite{zhang2023adding} stand out. These methods allow subject-driven generation, which consists of reconstructing a subject in different contexts while maintaining its fundamental characteristics; and modifying existing images with high fidelity.

Therefore, it is logical to ask the question: to what extent does this set of tools allow the use of synthetic images in real tasks? Thus, this thesis focuses on solving this question from the deep learning perspective. Therefore, to what extent can images generated by text-to-image models improve the performance of computer vision models? To address this question, we have developed an experimental framework to test the synthetic images generated by the Stable Diffusion model on several classical computer vision tasks.

First, we take a well-studied dataset such as the Oxford-IIIT Pet dataset \cite{Parkhi2012CatsAD}. Using subject-driven generation techniques, we create a pipeline in which synthetic images are used to augment the real images of the dataset in a classification task. Furthermore, we compare the results with classical data augmentation techniques and automated augmentation policies. We also study the effect of the size of the proportion of real versus synthetic images by fixing the latter's size. Secondly, we test the impact of the size of the proportion of synthetic images compared to real ones, but this time leaving the number of real ones fixed. Thirdly, we experiment with training a computer vision model with only generated images. Fourth, we combine the generative data augmentation approaches used with strategies based on automated augmentation policies to inspect the consequences. Fifth, we add control over the generated images with ControlNet. Sixth, we use the additional control provided by ControlNet to increase the dataset size in a segmentation task. Finally, we reaffirm our findings with the Food-101 dataset \cite{bossard14}.

Our extensive experiments show that subject-driven augmentation is a competitive data augmentation technique under specific characteristics. In particular, subject-driven augmentation is really beneficial on datasets with very few training images per class. Thus, considering a Resnet34 network on the Oxford-IIIT Pet dataset using less than 10 real images per class, we found performance \footnote{The model performance metric considered is accuracy.} improvements of up to 19.11\%. Moreover, this result is especially significant when we consider that classical data augmentation techniques are unable to improve the baseline. On the other hand, we show that adding synthetic images to a small dataset only makes sense to a certain extent. Again, with a Resnet34 network on the Oxford-IIIT Pet dataset using only 5 real images per class, we show that generating 100\% synthetic images improves the baseline by 18.93\%. Alternatively, by adding 1000\% of synthetic images, the baseline improvement only rises up to 19.11\%. On the other hand, we also experimented with no real images at all. In this case, we show that competitive results can be obtained using only synthetic images in the training of a computer vision task. We also show that adding conditional control with ControlNet can improve the results. Thus, we obtain up to 23.47\% improvement over the baseline when using 5\% real images and 2000\% synthetic images in the Oxford-IIIT Pet dataset with a Resnet34 network. Finally, we show how this approach can be employed in different tasks, such as segmentation or in other datasets, such as Food-101.