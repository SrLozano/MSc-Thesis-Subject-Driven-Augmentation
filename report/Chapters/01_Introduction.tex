\chapter{Introduction}

Text-based image generation models have reached a point of maturity where they are capable of generating high-fidelity photorealistic images \cite{ho2020denoising, dhariwal2021diffusion}. These images have reached a level where they are usable in real projects and even indistinguishable from an actual image by most of the public \cite{newyorktimesbelieve}. In addition, the availability of text-to-image models has been increased by private companies, educational institutions, and the open-source community. Gigantic models such as Stable Diffusion are available in a completely accessible way for anyone wanting to try or experiment with it. On the other hand, this growing availability allows researchers worldwide to develop methods that enable more effective control of generative models. In this line, the work of Textual inversion \cite{gal2022image}, Dreambooth \cite{ruiz2022dreambooth}, and ControlNet \cite{zhang2023adding} stand out. These methods allow subject-driven generation, which consists of reconstructing a subject in different contexts while maintaining its fundamental characteristics; and modifying existing images with high fidelity.

Therefore, it is logical to ask the question: to what extent does this set of tools allow the use of synthetic images in real tasks? Thus, this thesis focuses on solving this question from the deep learning perspective. Therefore, to what extent can images generated by text-to-image models improve the performance of computer vision models? To address this question, we have developed an experimental framework to test the synthetic images generated by the Stable Diffusion model on several classical computer vision tasks.

First, we take a well-studied dataset such as the Oxford-IIIT Pet dataset \cite{Parkhi2012CatsAD}. Using subject-driven generation techniques, we create a pipeline in which synthetic images are used to augment the real images of the dataset in a classification task. Furthermore, we compare the results with classical data augmentation techniques and automated augmentation policies. We also study the effect of the size of the proportion of real versus synthetic images by fixing the latter's size. Secondly, we test the impact of the size of the proportion of synthetic images compared to real ones, but this time leaving the number of real ones fixed. Thirdly, we experiment with training a computer vision model with only generated images. Fourth, we combine the generative data augmentation approaches used with strategies based on automated augmentation policies to inspect the consequences. Fifth, we add control over the generated images with ControlNet. Sixth, we use the additional control provided by ControlNet to increase the dataset size in a segmentation task. Finally, we reaffirm our findings with the Food-101 dataset \cite{bossard14}.

PARAGRAPH WITH RESULTS IN HERE