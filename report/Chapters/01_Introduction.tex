\chapter{Introduction}

Text-based image generation models have reached a point of maturity where they are capable of generating high-fidelity photorealistic images \cite{ho2020denoising, dhariwal2021diffusion}. These images have attained a degree of quality that renders them suitable for practical implementation and are often indiscernible from genuine photographs to the majority of observers \cite{newyorktimesbelieve}. In addition, the availability of text-to-image models has been increased by private companies, educational institutions, and the open-source community. Gigantic models such as Stable Diffusion are available in a completely accessible way for anyone wanting to try or experiment with it. 

The increasing accessibility of these resources empowers researchers worldwide to devise novel techniques that facilitate enhanced manipulation and control of generative models. In this line, the work of Textual inversion \cite{gal2022image}, Dreambooth \cite{ruiz2023dreambooth}, and ControlNet \cite{zhang2023adding} stand out. These first two methods allow \textit{subject-driven generation}, which consists of reconstructing a subject in different contexts while maintaining its fundamental characteristics and details. In particular, Dreambooth takes a few images of a subject and returns a personalised text-to-image model by fine-tuning. Then, a unique identifier refers to the subject. Similarly, Textual inversion takes a reduced set of images and finds an embedding token for a new token while keeping the model intact. Lastly, ControlNet introduces conditional control to the generation process, enabling the precise modification of existing images while preserving high fidelity.

In addition to all these new possibilities, one of the foremost challenges in deep learning emerges. The utilisation of vast quantities of data and the necessity for extensively annotated datasets pose significant concerns. The creation and upkeep of these massive datasets incur substantial expenses and remain beyond the reach of most researchers \cite{yang2022image}. Consequently, the scientific community has primarily directed its efforts towards optimising deep learning architectures rather than developing methodologies to mitigate the cost of acquiring and maintaining large-scale datasets \cite{ghiasi2021simple}.

Hence, it is logical to ask the question: to what extent does this set of tools allow the use of synthetic images in real tasks while allowing the cost reduction of creating and maintaining datasets? Thus, this thesis focuses on solving this question from the deep learning perspective. Therefore, to what extent can images generated by text-to-image systems improve the performance of computer vision models? To address this question, we have developed an experimental framework to test the synthetic images generated by the Stable Diffusion model on several classical computer vision tasks. Concretely, we approach the issue through the lens of data augmentation, with a specific focus on its applicability to classification and segmentation problems.

First, we take a well-studied dataset such as the Oxford-IIIT Pet dataset \cite{Parkhi2012CatsAD}. Using \textit{subject-driven generation} techniques, we create a pipeline in which synthetic images are used to augment the real images of the dataset in a classification task. Furthermore, we compare the results with classical data augmentation techniques and automated augmentation policies. We also study the effect of the size of the proportion of real versus synthetic images by fixing the latter's size. Secondly, we test the impact of the size of the proportion of synthetic images compared to real ones, but this time leaving the number of real ones fixed. Thirdly, we experiment with training a computer vision model with only generated images. Fourth, we combine the \textit{generative data augmentation} approaches used with strategies based on automated augmentation policies to inspect the consequences. Fifth, we add control over the generated images with ControlNet to improve image quality. Sixth, we use the additional control provided by ControlNet to increase the dataset size in a segmentation task. Finally, we reaffirm our findings with the Food-101 dataset \cite{bossard14}.

Our extensive experiments show that \textit{subject-driven augmentation} is a competitive data augmentation technique under specific characteristics. In particular, \textit{subject-driven augmentation} is really beneficial on datasets with very few training images per class. Thus, considering a Resnet34 network on the Oxford-IIIT Pet dataset using less than 10 real images per class, we found classification performance \footnote{The model performance metric considered is accuracy.} improvements of up to 19.11\%. Moreover, this result is especially significant when we consider that classical data augmentation techniques are unable to improve the no-augmentation baseline. 

On the other hand, we show that adding synthetic images to a small dataset only makes sense to a certain extent. Again, with a Resnet34 network on the Oxford-IIIT Pet dataset using only 5 real images per class, we show that generating 100\% synthetic images improves the no-augmentation baseline by 18.93\%. Alternatively, by adding 1000\% of synthetic images, the baseline improvement only rises up to 19.11\%. 

Alternatively, we also experimented with no real images at all. In this case, we show that competitive results can be obtained using only synthetic images in the training of a computer vision task. We also show that adding conditional control with ControlNet can improve the results. Thus, we obtain up to 23.47\% improvement over the no-augmentation baseline when using 5\% real images and 2000\% synthetic images in the Oxford-IIIT Pet dataset with a Resnet34 network. 

Finally, we show how this approach can be employed in different tasks, such as segmentation or in other datasets, such as Food-101.

Our findings yield clear implications within the realm of computer vision. First, \textbf{subject-driven augmentation techniques are a competitive approach}. Second, these data augmentation techniques are \textbf{especially advantageous in sparse datasets}. Lastly, despite advancements, synthetic images exhibit limitations in faithfully representing reality, indicating \textbf{ample potential for further enhancement}. 

Hence, we envision a future trend towards \textit{few-shot} and \textit{zero-shot} learning. These approaches facilitate the training of models capable of accurate generalisation and prediction even with minimal or zero real training images. With text-to-image models that generate high-quality synthetic images, reducing or eliminating the expenses associated with creating extensive datasets required for deep learning model training becomes feasible. Thereby, the capabilities of computer vision models will be enhanced.