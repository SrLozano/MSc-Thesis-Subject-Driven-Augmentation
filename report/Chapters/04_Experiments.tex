\chapter{Experiments} \label{sec:experiments}

This section aims to empirically test how effective subject-driven augmentation is in improving the performance of computer vision models. Our approach consists of testing how competitive this data augmentation technique is on real tasks compared to other well-known methods such as Autoaugment or RandAugment. In addition, we study the behaviour concerning the ratio of real images to synthetic images and check how much information can be obtained using only synthetic images. On the other hand, we test control techniques to try to improve the results. With these, we demonstrate whether the proposed augmentation approach can enhance the performance of segmentation models. Finally, we test in other domains to see if the process is generalisable to other datasets.

Our results support the hypothesis that subject-driven augmentation is a competitive data augmentation technique in real tasks. In particular, we show that it is especially significant when training data is sparse. Thus, we observe accuracy increases of up to 19.11\% in classification tasks using the Oxford-IIIT Pet dataset. However, we show that adding synthetic images to a small dataset only makes sense to a certain extent, especially when sufficient real training images are available. Furthermore, we show that competitive results can be obtained using only synthetic images in training a computer vision task. Finally, we demonstrate the versatility of this approach by showing its application in various tasks, including segmentation, as well as its potential on alternative datasets such as Food-101.

\section{Experiments overview} \label{sec:experimentsO}

The first step in testing the capabilities of subject-driven augmentation is to know the limitations of the selected subject-driven techniques. If we consider Dreambooth and Textual inversion, we will realise that their main input element is images of a specific subject. Two fundamental questions arise at this point. Firstly, how many images are to be used? Secondly, is it feasible to apply Dreambooth and Textual inversion to different subjects of the same class?

For this reason, we initially set up the experiment \textbf{01-number-of-images}. In this experiment, we want to test the effect of the number of images used in applying subject-driven generation techniques on the quality of the images generated. Dreambooth and Textual inversion authors propose using between 3 and 5 images \cite{ruiz2023dreambooth, gal2022image}. However, since the proposed pipeline will push these techniques to their limits by selecting images of subjects that do not necessarily have to be the same, it is interesting to see how flexible they are. Therefore, the \textit{01-number-of-images} experiment proposes to test 1, 2, 3 and 5 images. These tests allow the flexibility of Dreambooth and Textual inversion to be tested to establish an appropriate number of images.

On the other hand, the question remains whether it is feasible to push these methods to the limit with images of subjects that, although of the same class, are different. Thus, we define the experiment \textbf{02-different-subjects}. It aims to test how flexible Dreambooth and Textual inversion are when provided with several images of different subjects sharing the same class. To do so, we take the domain of dog breeds and employ subject-driven generation methods on sets of images that mix dog breeds. Furthermore, we perform the experiment incrementally to maximise the information we can extract from the performance of Dreambooth and Textual inversion. Initially, we take dogs with a common breed, and successively, we introduce dogs of increasingly different breeds. Specifically, we start with a Golden Retriever and successively add subjects of the following breeds: German Shepherd, Siberian Husky, Bulldog and Welsh Corgi.

At this point, the experiments \textit{01-number-of-images} and \textit{02-different-subjects} give us an insight into the possibilities of subject-driven generation techniques. Therefore, we can move on to experimenting with the entire pipeline. In experiment \textbf{003-training-percentage}, we intend to compare it with other data augmentation techniques. The selected task consists of classification on the Oxford-IIIT Pet dataset. The selected approaches are as follows.

\begin{itemize}
    \item \textbf{No-augmentation baseline}: Common and comparative starting point for assessing the performance of other approaches. It helps to establish the minimum expected level of performance. It is the vanilla classification task, i.e. without additional data augmentation or modification.
    \item \textbf{Custom data augmentation}: The training set is extended with classical transformations such as horizontal flips, rotations, and brightness and contrast adjustments, among others. In particular, the following transformations are used.
    \begin{itemize}
        \item \textit{RandomHorizontalFlip(p=0.5)}: This transform randomly flips the input image horizontally with a probability of 50\%.
        \item \textit{ColorJitter(brightness=0.3, contrast=0.1, saturation=0.2, hue=0.1)}: This transform randomly adjusts the brightness, contrast, saturation and hue values of the input image. The specified parameters control the magnitude of the adjustment.
        \item \textit{GaussianBlur(kernel\_size=3, sigma=(0.1, 2.0))}: This transform applies a Gaussian blur to the input image. The parameter \textit{kernel\_size} defines the size of the kernel used for blurring, while \textit{sigma} controls the standard deviation of the Gaussian distribution used to generate the blur.
        \item \textit{RandomRotation(10)}: This transformation randomly rotates the input image by a randomly selected angle in the range of -10 to 10 degrees.
    \end{itemize}
    \item \textbf{AutoAugment}: An automated augmentation policy developed by Google Brain \cite{cubuk2018autoaugment} is used for data augmentation.
    \item \textbf{RandAungment}: An improved automated augmentation policy developed by Google Brain \cite{cubuk2020randaugment} is used for data augmentation.
    \item \textbf{Dreambooth}: The subject-driven augmentation pipeline based on Dreambooth is used, as described in section \ref{sec: sdAugmentation}. 
    \item \textbf{Textual inversion}: The subject-driven augmentation pipeline based on Textual inversion, as described in section \ref{sec: sdAugmentation}, is used.
    \item \textbf{Stable Diffusion prompt}: The subject-driven augmentation pipeline based on class names, as described in section \ref{sec: cnbAugmentation}, is used.
\end{itemize}

However, experiment \textit{03-training-percentage} continues beyond there and compares these approaches by varying the percentage of real data used in the training set. In this way, we can check what effect the size of the dataset has on the effectiveness of one or the other technique. Finally, it is important to highlight that subject-driven approaches use 50 synthetic images. Nonetheless, for specific implementation details, please refer to section \ref{sec: implemantationD}.

Once it is known how the selected techniques perform when the size of the training set is varied, it is logical to think that the next step is to vary the number of images generated. Along these lines, in the \textbf{004-generation-percentage experiment}, we vary the percentage of images generated while leaving the size of the actual training set fixed. In this way, we can test how many synthetic images perform better with respect to the accuracy of the task.

After completing experiments \textit{03-training-percentage} and \textit{04-generation-percentage}, to what extent are real images necessary to obtain competitive results? To address this question, we set up experiment \textbf{005-all-generated}. In it, we only train the classification model with synthetic images, and the objective is to determine the quality of the information in the images. That is, to what extent can the text-to-image model generate images with valid information that a classification model can subsequently extract? In this way, it can be considered a case of transfer learning in which a larger model transfers information to a smaller model. In this case, through images.

However, the approach projected in the \textit{05-all-generated} experiment has a fundamental problem. Dreambooth and Textual inversion need real images as inputs. Therefore, the premise of not using any real images is not being fulfilled. Conversely, the Stable Diffusion prompt approach (based on generating images using only class names, figure \ref{fig:classaug} does not require any input images. Therefore, its results are valid.

Experiment \textbf{06-controlnet} adds conditional control to the images generated by the Stable Diffusion prompt approach. Figure \ref{fig:condaug} shows the pipeline used. Its purpose is to test whether the quality of the synthetic images can be improved by adding control.

Another interesting question is whether classical data augmentation techniques are capable of being used in combination with the subject-driven approach. Thus, the \textbf{07-combinations} experiment seeks to merge the best subject-driven configurations found with classical techniques such as RandAugment. The combination could improve the results of both techniques separately. 

So far, we have only considered a classification task on the Oxford-IIIT Pet dataset. Thus, the \textbf{08-segmentation} experiment moves the subject-driven approach to a segmentation task on the same dataset. It employs conditional control, as does experiment \textit{06-controlnet}. On the other hand, experiment \textbf{09-food-101} seeks to test another dataset to demonstrate the versatility of subject-driven augmentation.

In summary, we conducted the following experiments in this paper to learn about the strengths and weaknesses of subject-driven augmentations.

\begin{itemize}
    \item \textbf{01-number-of-images}: It takes Dreambooth and Textual inversion to study the effect of the number of real images used as input. This is interesting as it allows us to explore the limits of these techniques. This is essential since the proposed augmentation pipeline requires these methods to provide great flexibility.
    \item \textbf{02-different-subjects}: The objective is to examine whether it is feasible to push these methods to the limit with images of subjects that, although of the same class, are different. Thus, we take the domain of dog breeds and employ subject-driven generation methods on sets of images that mix dog breeds incrementally by considering less and less similar breeds.
    \item \textbf{03-training-percentage}: This experiment goes on to test the entire pipeline. In this way, we intend to compare it with other data augmentation techniques. Specifically, we define tests with a baseline, classical data augmentation techniques, automated augmentation policies such as AutoAugment and RandAugment, and the subject-driven augmentation pipeline defined with Dreambooth, Textual inversion and Stable Diffusion prompt. In addition, this experiment compares these same approaches by varying the percentage of real data used in the training set. The number of synthetic images is set to 50.
    \item \textbf{04-generation-percentage}: It takes \textit{03-training-percentage} and varies the percentage of images generated while leaving the size of the actual training set fixed.
    \item \textbf{05-all-generated}: The purpose is to evaluate whether a computer vision model can only be trained with synthetic images and obtain competitive results. Nonetheless, it should be noted that both Dreambooth and Textual inversion need real images to personalise the text-to-image model. Therefore, their results should be interpreted with caution. However, the Stable Diffusion prompt approach can be run with no real images, only with class names.
    \item \textbf{06-controlnet}: It aims to test whether the quality of synthetic images can be improved by adding conditional control. It considers the Stable Diffusion prompt approach. 
    \item \textbf{07-combinations}: It merges the best subject-driven configurations found with classical techniques such as RandAugment. The idea is that the combination could improve the results of both techniques separately.
    \item \textbf{08-segmentation}: It moves the subject-driven approach to a segmentation task. 
    \item \textbf{09-food-101}: Considering an utterly different dataset, it aims to see how versatile subject-driven augmentation is.
\end{itemize}

\section{Implementation details} \label{sec: implemantationD}

Next, the aspects of the code implementation carried out to execute the experiments defined in \ref{sec:experimentsO} are detailed. Thus, the details concerning the datasets, the neural networks, the subject-driven techniques and the hardware and execution environment are explained. The aim is to make the detailed analysis in this work as rigorous and thorough as possible. And, consequently, to allow replicability so anyone can certify the results obtained.

\subsection{Datasets}

The primary dataset chosen for the present work is Oxford-IIIT Pet \cite{Parkhi2012CatsAD}, a collection of 7,349 images of cats and dogs of 37 different breeds, of which 25 are dogs, and 12 are cats. The dataset contains about 200 images for each breed. We divide these images randomly into 100 for training, 50 for validation and 50 for testing. Each image is labelled with the breed and a pixel-level segmentation marking the body. The segmentation consists of a trimap with regions representing the pet's body, the background and ambiguous areas (including the boundary of the pet's body and accessories such as collars). Figure \ref{fig:oxfordiiitPetC} shows examples of each of the Oxford-IIIT Pet classes. Note the diversity of the images, with a high variability of colour, subject arrangement or backgrounds. On the other hand, Figure \ref{fig:oxfordiiitPetS} shows the pixel-level annotations used in the segmentation task. 

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Pictures/oxfordiiitPetC.png} 
    \caption{\textbf{Examples of each of the 37 Oxford-IIIT Pet classes} \cite{Parkhi2012CatsAD}. Note the significant variability found in the images, from changes in lighting and size to layout and scenery. This fact and the fact that it is a fine-grained dataset make it ideal for testing whether subject-driven augmentation is a competitive strategy.}
    \label{fig:oxfordiiitPetC}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Pictures/oxfordiiitPetS.png} 
    \caption{\textbf{Oxford-IIIT Pet annotations for segmentation}. Green is for the background region, yellow is for the ambiguous region, and purple is for the subject. }
    \label{fig:oxfordiiitPetS}
\end{figure}

We have chosen this dataset as the primary dataset for our analysis because it is \textit{fine-grained}. This type of dataset contains many categories or classes with subtle distinctions between them. Unlike \textit{coarse-grained} datasets with broader categories, \textit{fine-grained} datasets focus on capturing fine details and subtle variations within a specific domain. Thus, given that the present work focuses on subject-driven augmentation techniques, such datasets allow us better discriminate the strengths and weaknesses of these techniques. In addition, the fact that this dataset contains both classification and segmentation annotations is also helpful.

On the other hand, the dataset used in experiment \textit{09-food-101} to show the versatility of the subject-driven augmentation technique is Food-101 \cite{bossard14}. This \textit{fine-grained} dataset contains 101,000 images of 101 different food categories. We divide these images into 600 for training, 150 for validation and 250 for testing. It should be noted that both the training and validation images contain noise that the authors have not purposely cleaned to reflect that the real data is imperfect and contains large variability. Figure \ref{fig:food101} shows some images from this dataset showing the significant variability of the existing food types.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Pictures/food101.png} 
    \caption{\textbf{Examples of 100 of the 101 Food-101 categories} \cite{bossard14}. Notice the significant variability of the existing food types and the fact that it is a \textit{fine-grained} dataset.}
    \label{fig:food101}
\end{figure}

\subsection{Networks}
Concerning the deep network with which the experiments are carried out, the two proposed scenarios of classification and segmentation must be considered. For the first task, we take \textit{ResNet34}, a variant of the \textit{ResNet} architecture \cite{he2016deep} with 34 layers. This network is considered medium-sized, being smaller, for example, than \textit{ResNet50}. \textit{ResNet} stands for \textit{Residual Network}, a reference to the residual connections that this architecture proposes to the problem of vanishing gradients. This problem occurs during the training of neural networks with methods based on gradient descent and backpropagation. The residual connections solution allows information to flow directly through the network layers, thus enabling the training of deeper networks. 

Returning to the main problem of this work, we employ \textit{Resnet34} pre-trained on \textit{ImageNet-1k} and apply feature extraction with a fully connected classifier suitable for the 37 Oxford-IIIT Pet classes. The approach works because, even though the network is pre-trained, feature extraction allows us to exploit the meaningful features of the training images optimally. In this way, we can determine which data augmentation technique is the one that succeeds in making the extracted features as informative about the task as possible.

Finally, in the network training, \textit{Cross-Entropy} is used as a loss function and \textit{stochastic gradient descent - SGD} as an optimiser. Additionally, an early stopping with a patience of 5, a learning rate of $10^{-3}$ and a batch size of 16 are used.

On the other hand, for the segmentation task, we use the \textit{DeepLabV3} model \cite{chen2017rethinking} with \textit{ResNet101} as a backbone. The semantic segmentation architecture is based on the intensive use of \textit{atrous convolutions}. This type of convolution allows the expansion of the receptive field of a convolutional network without increasing the number of parameters. 

Returning to our segmentation task, we take a parallel approach to the classification task by performing feature extraction. In this case, the last layer of the \textit{DeepLabV3} model is modified so that the segmentation takes place in 3 values (subject, background and ambiguous region). \textit{Cross-Entropy} is used as a loss function, and \textit{Adam} as an optimiser. Additionally and analogously to the classification case, an early stopping with a patience of 5, a learning rate of $10^{-3}$ and a batch size of 16 are used.

\subsection{Subject-driven techniques and text-to-image model}

Stable Diffusion in its \textit{stable-diffusion-v1-5} version is used as a text-to-image model. The number of input images is 5, as can be seen from the results shown in \ref{sec: exp-01} and \ref{sec: exp-02}. The resolution of the generated synthetic images is 512x512. The rest of the model's parameters when generating the images are shown in table \ref{table:TableSDhyperparameters} and are derived from the analysis performed in \cite{stablediffusiondiffusers}.

\begin{table}[ht]
\centering
\begin{tabular}{|p{0.24\linewidth}|p{0.57\linewidth}|p{0.10\linewidth}|}
\hline
\rowcolor[HTML]{AEAAAA} 
\textbf{Hyperparameter} & \textbf{Description} & \textbf{Value} \\ \hline
\textit{num\_inference\_steps} & The bigger, the better the results are. However, also the longer the generation   takes & 50 \\ \hline
\textit{guidance\_scale} & It enhances the compliance with the conditional signal that directs the creation   (text). It compels the generation to align with the given prompt more closely, possibly sacrificing image quality or variety in the process. Also known as classifier-free guidance & 7.5 \\ \hline
\end{tabular}
\caption{\textbf{Stable Diffusion hyperparameters}.}
\label{table:TableSDhyperparameters}
\end{table}

Table \ref{table:TableDreamboothhyperparameters} lists the hyperparameters with which Dreambooth is run. The choice of these is derived from the analysis of the Hugging Face blog \cite{dreamboothdiffusers}.

\begin{table}[ht]
\centering
\begin{tabular}{|l|p{0.48\linewidth}|l|}
\hline
\rowcolor[HTML]{AEAAAA} 
\textbf{Hyperparameter} & \textbf{Description} & \textbf{Value} \\ \hline
\textit{instance\_prompt} & Identifier specifying the instance & \textless{}funny-ret\textgreater{} \\ \hline
\textit{resolution} & Resolution for input images. All of them will be resized to that value & 512 \\ \hline
\textit{train\_batch\_size} & Batch   size for the training data loader & 1 \\ \hline
\textit{gradient\_accumulation\_steps} & Number   of updates steps to accumulate before performing a backward or update pass & 1 \\ \hline
\textit{learning\_rate} & Initial learning rate & $5\cdot10^{-6}$ \\ \hline
\textit{lr\_scheduler} & Scheduler type to use & constant \\ \hline
\textit{lr\_warmup\_steps} & Number of steps for the warmup in the \textit{lr\_scheduler} & 0 \\ \hline
\textit{max\_train\_steps} & Total number of training steps to perform & 400 \\ \hline
\end{tabular}
\caption{\textbf{Dreambooth hyperparameters}.}
\label{table:TableDreamboothhyperparameters}
\end{table}

Table \ref{table:TableTextinversionhyperparameters} contains the hyperparameters with which Textual inversion is executed. Note that \textit{placeholder\_token} refers to the token used as a placeholder for the concept, \textit{initializer\_token} to the token used as the initialiser word and \textit{learnable\_property} as a choice between object or style.

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor[HTML]{AEAAAA} 
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
\textit{learnable\_property} & object \\ \hline
\textit{placeholder\_token} & \textless{}funny-ret\textgreater{} \\ \hline
\textit{initializer\_token} & animal \\ \hline
\textit{resolution} & 512 \\ \hline
\textit{train\_batch\_size} & 1 \\ \hline
\textit{gradient\_accumulation\_steps} & 4 \\ \hline
\textit{learning\_rate} & 5Â·10-4 \\ \hline
\textit{lr\_scheduler} & constant \\ \hline
\textit{lr\_warmup\_steps} & 0 \\ \hline
\end{tabular}
\caption{\textbf{Textual inversion hyperparameters}.}
\label{table:TableTextinversionhyperparameters}
\end{table}

Finally, it is important to note that sometimes images generated by the text-to-image model are black and contain no information. This is because the Stable Diffusion model includes an NSFW content filter that is very easily activated. Therefore, the implementation made in this work does not consider these images. They are removed, and others are generated in their place.

\subsection{Hardware and environment}

The execution environment uses the resources provided by the \textit{Technical University of Denmark - DTU} through the \textit{high-performance cluster - HPC} belonging to the \textit{DTU Computing Center - DCC}. The use of these advanced computing resources is because the tasks proposed in this work require significant amounts of processing power and memory. For more details about the hardware used, please refer to Appendix \ref{APHardware}. 

On the other hand, as for the software used. The programming language chosen to create the code is \textit{Python}, in its version 3.8.13. The deep learning library, \textit{Pytorch} in its version 2.0.1. Furthermore, as for the library containing the state-of-the-art pre-trained diffusion models as well as the subject-driven and conditional control techniques, \textit{Hugging Face diffusers} has been used in version 0.16.1. For further details and a detailed list of all the libraries and software tools used, please visit Appendix \ref{APSoftware}.

\section{Results} \label{sec: results} 

This section shows the results obtained throughout the experiments defined in \ref{sec:experimentsO}. Furthermore, we provide accompanying analyses and observations of significance to enable the interpretation of these results. Appendix \ref{rigour} contains details on how we ensure the rigour and reproducibility of the experiments.

\subsection{Influence of the number of images on subject-driven generation} \label{sec: exp-01}

Experiment \textit{01-number-of-images} studies the effect of the number of images used as input in Dreambooth and Textual inversion. Image \ref{fig:exp1} summarises the results of the experiment. It distinguishes the images used as input on the left and the resulting images on the right. Within the synthetic images, we provide the results of 2 prompts and 4 different values of images used as input for both Dreambooth and Textual inversion.   

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Pictures/exp1.png} 
    \caption{\textbf{Experiment 01-number-of-images}. The input images are shown on the left and the resulting images on the right. Synthetic images with two prompts and four input images variations for Dreambooth and Textual inversion are included.}
    \label{fig:exp1}
\end{figure}

Analysing these results is complex as it is an evaluation that does not rely on any easily measurable metric. However, the results suggest \textbf{better quality synthetic images are obtained using 5 real images} as input. To reach this conclusion, we looked at the generalisation ability of the subject in different contexts or positions. Thus, in the case of Textual inversion, it is clear that only in the case of 5 images the main characteristics of the creature are maintained. On the other hand, in the case of Dreambooth, we observe how the generalisation of the entity starts to be correct from the 2 input images. At this point, Dreambooth obtains more faithful results than Textual inversion.

In any case, we confirm that using a single image to execute these subject-driven techniques is unfeasible. This fact has important implications for our work. When using more than 1 image in a data augmentation use case with a real dataset, these pictures are taken from the same class. Nevertheless, it is not assured that they are the same subject. In fact, in most cases, this will not be the case. This use case is not the primary use case of either Dreambooth or Textual inversion, and, therefore, we must ensure that, even with images of different subjects of the same class, these approximations work. In this line, special attention should be paid to experiment \textit{02-different-subjects}.

\subsection{Subject-driven generation with diverse subjects} \label{sec: exp-02}

Experiment \textit{02-different-subjects} studies how subject-driven techniques behave when the subjects of the input images are different. For this purpose, the dogs' domain is taken, and incrementally, Dreambooth and Textual inversion are used with increasingly different dogs. Image \ref{fig:exp2} summarises the results of the experiment. It distinguishes the images used as input at the top and the synthetic images for each technique at the bottom. In addition, for each technique, the synthetic images are divided into 4 subsets, each corresponding to a different subset of input images. The aim is to observe the differences in the images generated by the personalised text-to-image model with increasingly less similar inputs. Thus, the subsets of images used as input are:

\begin{itemize}
    \item \textbf{subset 1}: \textit{golden\_1, golden\_2, golden\_3.}
    \item \textbf{subset 2}: \textit{golden\_1, golden\_2, golden\_3, german\_1, german\_2.}
    \item \textbf{subset 3}: \textit{golden\_1, golden\_2, golden\_3, german\_1, german\_2, siberian\_1, siberian\_2.}
    \item \textbf{subset 4}: \textit{bulldog\_1, corgi\_1, german\_1, golden\_1, siberian\_1.}
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.90\textwidth]{Pictures/exp2.png} 
    \caption{\textbf{Experiment 02-different-subjects}. The images used as input are at the top. The synthetic images for each technique are at the bottom. The experiment shows the differences in the images generated by the personalised text-to-image model with increasingly less similar inputs.}
    \label{fig:exp2}
\end{figure}

Analogous to the \textit{01-number-of-images} experiment, evaluating the images without an objective metric is problematic. However, the results obtained leave no doubt as to their quality. In all cases, subjects with dog-like characteristics are distinguishable in the synthetic images. It is especially noteworthy that, even with such different breeds as in \textit{subset\_4}, the images clearly show a being with dog characteristics. Although these images are readily identifiable as fake by the human eye, they contain very relevant information about what a dog is. Thus, with an appropriate architecture, they could be used to train a computer vision model. On the other hand, we would like to emphasise the magnificent results obtained with \textit{subset\_1}. With different subjects of the same breed (a fine-grained dataset if we make a parallelism with a computer vision task), Dreambooth and Textual inversion generate images that start to be difficult to distinguish from authentic images.

In summary, the \textit{02-different-subjects} experiment leaves no doubt that \textbf{it is possible to use subject-driven techniques with different subjects of the same class}. Therefore, these results support the idea that subject-driven augmentation techniques are an approach that should be considered in the training of complex computer vision tasks. Thus, we defined experiment \textit{03-training-percentage} to test subject-driven augmentation techniques on a real task.

\subsection{Comparative analysis of subject-driven and classical data augmentation} \label{sec: exp-03}

Experiment \textit{03-training-percentage} evaluates the entire subject-driven augmentation pipeline and compares it to other data augmentation techniques in a real task. This includes comparisons with the no-augmentation baseline, classical techniques and automated policies (AutoAugment and RandAugment). For subject-driven techniques, it includes Dreambooth, Textual inversion and Stable Diffusion prompt. In addition, the experiment examines the impact of varying the size of the real dataset. For this purpose, the percentage of data indicates how many images have been used. We consider 100\% the use of the complete Oxford-IIIT Pet training set. On the other hand, the number of synthetic images is kept fixed at 50. Unlike the \textit{01-number-of-images} and \textit{02-different-subjects} experiments, we have a more rigorous evaluation method in this case. We use the accuracy of the trained model in the classification task and establish a baseline that does not employ any data augmentation technique. Figure \ref{fig:exp3} shows a plot of the results. 

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Pictures/dreambooth-img.png} 
    \caption{\textbf{Synthetic images generated using Dreambooth for the Oxford-IIIT Pet domain}. The quality of these images is remarkably high, with some being potentially indistinguishable from authentic images. Nevertheless, some have slight anatomical flaws and artefacts.}
    \label{fig:exp3-images-dream}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Pictures/textual-inversion-img.png} 
    \caption{\textbf{Synthetic images generated using Textual inversion for the Oxford-IIIT Pet domain}. Although some images are particularly good, overall, there are more anatomical flaws and artefacts than with Dreambooth or Stable Diffusion prompt.}
    \label{fig:exp3-images-text}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Pictures/stable_diffusion_prompt-img.png} 
    \caption{\textbf{Synthetic images generated using Stable Diffusion prompt for the Oxford-IIIT Pet domain}. The quality of these images is remarkably high, with some being potentially indistinguishable from authentic images. Nevertheless, some have slight anatomical flaws and artefacts.}
    \label{fig:exp3-images-sd}
\end{figure}

Figures \ref{fig:exp3-images-dream}, \ref{fig:exp3-images-text} and \ref{fig:exp3-images-sd} show a random selection of synthetic images obtained by the subject-driven techniques. When evaluating the images generated, it is not easy to give a proper verdict without objective metrics. However, the images generated by Dreambooth and Stable Diffusion prompt are of very high quality. Although they indeed have flaws, in general terms, they seem adequate and faithful to reality. Especially in the case of the Stable Diffusion prompt, since it is a very present domain in the text-to-image model, the results are particularly good. This is likely not the case for other, less common domains. Finally, the images produced by Textual inversion show, in general terms, more defects, especially anatomical ones.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Pictures/experiment_003.pdf}
    \caption{\textbf{Experiment 03-training-percentage}. The experiment looks at how varying the size of the real dataset affects classic and subject-driven techniques. The percentage of data used is indicated on a logarithmic scale on the x-axis. We find that subject-driven augmentation techniques are a promising approach for small dataset sizes.}
    \label{fig:exp3}
\end{figure}

The data indicate that subject-driven augmentation techniques significantly improve model performance when the percentage of real data is 10\% or less. In this case, we observe significant increases in accuracy. For example, with 5\% real data, Textual inversion achieves a performance increase of 19.11\%. On the other hand, as we increase the number of real images to 10\%, the improvement achieved by Textual inversion is 3.37\%. From this point on, Textual inversion, despite being the most promising subject-driven approach, does not bring any new accuracy improvements that can be considered relevant. On the other hand, Dreambooth is the worst performer among the subject-driven techniques. This approach brings improvements of 16.06\% and 0.5\% when using 5\% and 10\% of the real data, respectively. Stable Diffusion prompt shows similar results to Textual inversion.

If we now look at the classical techniques (Custom data augmentation, AutoAugment and RandAugment), we can see how they worsen the accuracy when the dataset size is small. It is not until 50\% of the training set is present that they achieve performances that improve the no-augmentation baseline. Among them, RandAugment is the best performer, with accuracy increases of only 1.12\% when 100\% of the training set is used. In this case, all classical techniques can improve the result.

In summary, the data clearly show that subject-driven augmentation techniques are a promising approach when the dataset size is small. In this case, the training set contains 100 images per class when complete. Our results show that, \textbf{for cases where data is scarce or very costly to obtain, one can take advantage of the capabilities and world knowledge of text-to-image models to increase the accuracy of classification models} for computer vision tasks by more than 19\%. These findings are especially relevant since classical techniques fail miserably on small datasets.

\subsection{Impact of the percentage of generated data} \label{sec: exp-04}

Experiment \textit{04-generation-percentage} takes experiment \textit{03-training-percentage} and varies the percentage of images generated while leaving the size of the actual training set fixed. We consider two different scenarios. In the first one, we take 100\% of the Oxford-IIIT Pet training set and in the second one, only 5\%. Figure \ref{fig:exp4} shows a plot of the results. 

\begin{figure}[th]
    \centering
    \includegraphics[width=1\textwidth]{Pictures/experiment_004.pdf}
    \caption{\textbf{Experiment 04-generation-percentage}. The experiment involves manipulating the proportion of generated data (measured on a logarithmic scale). Our findings indicate that in large datasets, the techniques do not yield a substantial improvement in system performance. However, in smaller ones, they do demonstrate an increase.}
    \label{fig:exp4}
\end{figure}

The data obtained show that when 100\% of the real training data is used (i.e. sufficient training data is available), it does not make sense to use subject-driven augmentation techniques. These approaches are not able to improve the no-augmentation baseline performance in a significant way. Moreover, the 3 techniques considered show a clear tendency to worsen their results as the number of synthetic images increases. 

On the other hand, using the subject-driven techniques makes much sense when using 5\% of the real training data (i.e., very little training data). By adding only a few images, the accuracy increases substantially. Adding only 100\% new synthetic images (which would imply doubling the number of images from 5 to 10), up to 18.93\% is achieved with Stable Diffusion prompt and 11\% with Textual inversion. In this scenario, increasing the number of synthetic images can improve the results, but only to a certain extent. By adding 1000\% synthetic images, Textual inversion offers a performance improvement of 19.11\%.

In summary, when there are enough training images, subject-driven augmentation techniques are not able to increase the performance of the system significantly, no matter how many synthetic images are added. In contrast, when the dataset is small, these approaches substantially improve the results.

\subsection{Feasibility of solely training models on synthetic images} \label{sec: exp-05}

Experiment \textit{05-all-generated} studies how necessary real images are in the training of computer vision models. For this purpose, we take Textual inversion, Dreambooth and Stable Diffusion prompt and use them to generate synthetic images to train the classification model for Oxford-IIIT Pet. The idea is to know to what extent the information contained in the synthetic images is faithful to reality and allows, without the help of real images, to obtain competitive results. It is crucial to note that, although useful, the results should be taken cautiously in the case of Textual inversion and Dreambooth since these techniques use a few real images to personalise the text-to-image model. In contrast, Stable Diffusion prompt does not use any image from the dataset, and therefore its results are more faithful to the idea of the experiment. The data collected are shown in Figure \ref{fig:exp5}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Pictures/experiment_005.pdf}
    \caption{\textbf{Experiment 05-all-generated}. The experiment uses only synthetic images to train the classification model. The number of synthetic images per class is measured on a logarithmic scale. Our findings indicate that there is still a gap between models trained with synthetic images and those trained with real ones.}
    \label{fig:exp5}
\end{figure}

The results indicate that, although decent results can be obtained, the synthetic images are not faithful enough to reality to obtain competitive results. Thus, we observe that the best-performing case, Textual inversion with 400 images per class, only obtains an accuracy of 0.72, which is 18.04\% worse than the baseline using the full real dataset. On the other hand, if we compare this result with using only 5 real images per class, we are looking at a performance improvement of 15.44\%. This result indicates that the synthetic images contain valid and usable information about reality. However, this information is still far from being indistinguishable from the information provided by real images.

The data also show that the more images are added, the better. However, a reduction in the trend can be seen from 100 images per class, reaching a maximum of around 400-1000 images per class. On the other hand, Textual inversion obtains the best values and, therefore, its images contain features that are more faithful to the real images and more usable by the associated classification model.

In summary, we show that \textbf{synthetic images contain usable information} in computer vision tasks. However, these images contain less information than real images. Thus, we show that \textbf{there is still a gap between models trained with synthetic images and those trained with real images}. Moreover, by including the Stable Diffusion prompt approach, we endow the present experiment with rigour since it does not personalise the text-to-image model and, therefore, does not use any real information about the dataset used.

\subsection{Effect of adding conditional control} \label{sec: exp-06}

Experiment \textit{06-controlnet} examines whether it is possible to improve the quality of synthetic images by adding conditional control. It takes the Stable Diffusion prompt approach and uses ControlNet pre-trained with Canny edge detections. The tests consist of 3 cases where the number of real images is taken as 100\%, 50\% and 5\% while varying the percentage of synthetic images. The collected data are shown in Figure \ref{fig:exp6}. 

Figure \ref{fig:exp6-images-cont} shows a sample of synthetic images achieved by ControlNet. Generally, the images enhanced by ControlNet exhibit fewer anatomical defects than their non-ControlNet counterparts. However, the overall realism of these images is diminished. It is evident that the distinct identifying features of each breed are accentuated, which can prove advantageous for classification purposes. Nonetheless, these images are unlikely to deceive a human observer.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Pictures/controlnet-img.png} 
    \caption{\textbf{Synthetic images generated using ControlNet for the Oxford-IIIT Pet domain}. For each pair of images, the original image is on the left side and the variation obtained by ControlNet is on the right side. These images have fewer anatomical defects as they follow the anatomical annotation of the real image. However, images are of poorer overall quality.}
    \label{fig:exp6-images-cont}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Pictures/experiment_006.pdf}
    \caption{\textbf{Experiment 06-controlnet}. The experiment uses conditional control. Textual inversion is added for comparison purposes. The number of synthetic images per class is measured on a logarithmic scale. Our findings show that conditional control can improve the fidelity of synthetic images when small sets of real images are considered.}
    \label{fig:exp6}
\end{figure}

The results indicate that for the cases with 100\% and 50\% real images, the accuracy does not improve with respect to the no-augmentation baseline when conditional control is used. On the other hand, with 5\% real images and 2000\% synthetic images, the accuracy is 0.77. This improvement represents an increase of 23.47\% over the no-augmentation baseline. Thus, this increase surpasses the best obtained so far, 19.11\% in the case of Textual inversion, with 5\% real images and 1000\% synthetic.

In summary, our findings show that \textbf{conditional control can improve the fidelity of synthetic images}. Thus, when used in a task associated with a small training set, we observe performance improvements of up to 23.47\%.

\subsection{Combination of subject-driven and classical data augmentation techniques} \label{sec: exp-07}

Experiment \textit{07-combinations} investigates the feasibility of integrating subject-driven approaches with classical data augmentation techniques. Consequently, we merge the most favourable subject-driven configurations with the top-performing classical technique, RandAugment. The conducted tests and their corresponding results are presented in Table \ref{table:TableExp07}. The column \textit{Variation} denotes the percentage difference in test accuracy before and after the inclusion of RandAugment.

\begin{table}[ht]
\centering
\begin{tabular}{|p{0.45\linewidth}|l|l|l|}
\hline
\cellcolor[HTML]{BFBFBF}\textbf{Description} & \cellcolor[HTML]{BFBFBF}\textbf{Real   data} & \cellcolor[HTML]{BFBFBF}\textbf{Synthetic   data} & \cellcolor[HTML]{BFBFBF}\textbf{Variation} \\ \hline
Textual inversion + RandAugment & 100\% & 100\% & -1.43\% \\ \hline
Textual inversion + RandAugment & 100\% & 50\% & -1.24\% \\ \hline
Textual inversion + RandAugment & 100\% & 10\% & 0.61\% \\ \hline
Textual inversion + RandAugment & 100\% & 5\% & -1.48\% \\ \hline
Dreambooth + RandAugment & 100\% & 5\% & 0.12\% \\ \hline
Stable Diffusion prompt + RandAugment & 100\% & 10\% & -0.18\% \\ \hline
Stable Diffusion prompt + RandAugment & 100\% & 5\% & -0.25\% \\ \hline
ControlNet + RandAugment & 100\% & 100\% & -0.25\% \\ \hline
ControlNet + RandAugment & 100\% & 5\% & -1.53\% \\ \hline
Textual inversion + RandAugment & 5\% & 1000\% & -1.83\% \\ \hline
Stable Diffusion prompt + RandAugment & 5\% & 200\% & 1.83\% \\ \hline
ControlNet + RandAugment & 5\% & 2000\% & -7.23\% \\ \hline
\end{tabular}
\caption{\textbf{Experiment 07-combinations}. The \textit{Variation} column indicates the percentage variation of the test accuracy before and after adding RandAugment. Our findings show that combining subject-driven augmentation with classical techniques does not improve the results.}
\label{table:TableExp07}
\end{table}

The findings unequivocally demonstrate that \textbf{integrating subject-driven augmentation with classical techniques does not yield improvements} in the results. In 75\% of instances, the results exhibited deterioration upon the inclusion of RandAugment. Moreover, among the remaining 25\% of cases that exhibited improvement, the majority experienced less than 1\% marginal enhancements, rendering them statistically insignificant. The observed detrimental effect of combining these techniques might be attributed to the introduction of excessive data variability that surpasses the model's capacity for generalisation.

\subsection{Subject-driven augmentation on a segmentation task} \label{sec: exp-08}

Experiment \textit{08-segmentation} investigates the feasibility of employing subject-driven augmentation in a segmentation task. Specifically, we utilise the Stable Diffusion prompt approach, incorporating conditional control mechanisms based on segmentation maps. This experiment bears similarity to the \textit{06-controlnet} experiment, with the distinction lying in the task being changed from classification to segmentation and the replacement of Canny edge annotations with segmentation maps as the control element. The Jaccard score is selected as the reference metric for evaluating the performance of this task. Additionally, we consider two scenarios: one utilising 100\% of the Oxford-IIIT Pet training set and another utilising only 5\%. Figure \ref{fig:exp8} depicts a graphical representation of the data gathered from the conducted tests.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Pictures/experiment_008.pdf}
    \caption{\textbf{Experiment 08-segmentation}. This experiment translates the subject-driven augmentation approach to a segmentation task. The
number of synthetic images per class is measured on a logarithmic scale. Our findings show that despite weak improvements, subject-driven augmentation techniques are a valid approach in segmentation tasks.}
    \label{fig:exp8}
\end{figure}

The findings from the experiment indicate that the data augmentation approach based on synthetic imaging has no significant effect on the performance of the associated model. When utilising 100\% of the real training set, the observed improvements are minimal, ranging from 0.42\% to 0.66\% compared to the no-augmentation baseline. These results, although consistent, possess limited strength and should not be given substantial consideration. Furthermore, increasing the number of synthetic images demonstrates a downward trend, indicating that it does not positively affect the overall outcome. Conversely, when only 5\% of the real data is used, more substantial improvements are observed, with an enhancement of 0.86\% compared to the no-augmentation baseline. However, these improvements remain modest and should be approached with caution. Additionally, a peak is observed when the synthetic image proportion reaches 1000\%, suggesting that further increasing the amount of synthetic data does not yield additional enhancements.

In summary, the results of our study suggest the possibility of enhancing the performance of segmentation models through the integration of synthetic images. However, these improvements are not statistically significant and should be interpreted carefully due to potential stochastic factors. Nevertheless, our findings highlight the \textbf{relevance of subject-driven augmentation techniques in domains beyond classification}.

\subsection{Domain change: Food-101} \label{sec: exp-09}

Experiment \textit{09-food-101} aims to demonstrate the generalizability of subject-driven augmentation across different domains. In order to achieve this objective, we substitute the Oxford-IIIT Pet dataset with the Food-101 dataset and conduct a series of tests to assess the viability of the approach proposed in this paper beyond the realm of pet-related data. To facilitate this evaluation, we employ the Stable Diffusion prompt approach, which offers faster testing. We focus on this study's two most frequently encountered scenarios: a training set comprising 100\% and 5\% of the dataset. The task involves classification, with accuracy as the reference metric. To accommodate the 101 classes in the new dataset, we continue to utilise the ResNet34 network architecture, albeit with necessary adaptations. The results obtained are presented in Table \ref{table:TableExp09}.

The no-augmentation baseline for the case where 100\% of the training set is used is 0.5688. For 5\%, the accuracy taken as the no-augmentation baseline is 0.3518.

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|l|}
\hline
\rowcolor[HTML]{BFBFBF} 
\textbf{Real data} & \textbf{Synthetic data} & \textbf{Test accuracy} \\ \hline
100\% & 5\% & 0.5706 \\ \hline
100\% & 10\% & 0.5482 \\ \hline
5\% & 100\% & 0.3803 \\ \hline
5\% & 200\% & 0.3877 \\ \hline
5\% & 400\% & 0.3586 \\ \hline
\end{tabular}
\caption{\textbf{Experiment 09-food-101}. Our findings show that subject-driven augmentation is applicable in other domains. Moreover, the results confirm the trend seen in previous experiments such as \textit{04-generation-percentage} that the approach is particularly competitive when the dataset considered has few images.}
\label{table:TableExp09}
\end{table}

The findings illustrate the \textbf{effectiveness of augmenting a dataset with synthetic images, particularly when the dataset size is small}. In the case of utilising only 5\% of the training data, a notable performance improvement of up to 10.2\% is observed. However, when the entire 100\% of the training data is employed, the performance increase is a mere 0.32\%. These results corroborate the trends identified in the \textit{04-generation-percentage} experiment, affirming the validity and competitiveness of the subject-driven approach to data augmentation, particularly in scenarios with limited image availability.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Pictures/synthetic-food101.png} 
    \caption{\textbf{Synthetic images generated for the food101 domain.} The quality of these images is remarkably high, with some being potentially indistinguishable from authentic images.}
    \label{fig:exp9-images}
\end{figure}

Figure \ref{fig:exp9-images} showcases a selection of synthetic images generated by Stable Diffusion for this experiment. The quality of these images is remarkably high, with some being potentially indistinguishable from authentic images to certain people. It is worth noting that Stable Diffusion benefits from a substantial representation of the food domain in its training set, primarily sourced from the internet. Consequently, in other domains where such comprehensive representation is lacking, the generated images may exhibit a different level of quality.