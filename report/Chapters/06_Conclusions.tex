\chapter{Conclusions} \label{sec:conclusions}

In this paper, we study the capabilities of text-to-image models in the context of synthetic imaging and their use in deep learning models. In particular, we ask to what extent synthetic images can improve the performance of computer vision models. This question holds significant relevance in light of the remarkable advancements observed in text-to-image models and the substantial expenses associated with acquiring adequately extensive and high-quality datasets.

The open availability of some models, such as Stable Diffusion, has driven the growing capabilities of text-to-image models. Their broad accessibility to the scientific community and enthusiasts has facilitated remarkable efforts in optimising and expanding their capabilities and applications. Thus, we highlight the work of Dreambooth \cite{ruiz2023dreambooth}, Textual inversion \cite{gal2022image} and ControlNet \cite{zhang2023adding}, which are examined in this study. With the support and enthusiasm of deep learning researchers, the field has witnessed a significant transformation over a span of merely 6 years, progressing from the initial text-to-image model \cite{mansimov2015generating} introduced in 2015 to the diffusion models \cite{saharia2022palette, nichol2021glide} that have been demonstrating impressive performance since 2021.

On the other hand, deep learning models consume large amounts of data and require large-scale annotated datasets. Creating and maintaining these massive datasets is costly and inaccessible for most researchers. As a result, the scientific community has focused more on optimising deep learning architectures than on methods to reduce the cost of acquiring and maintaining large datasets.

In this context, it is logical to ask whether it is possible to improve the capabilities of computer vision models by using synthetic images produced by capable and expressive text-to-image models.

Thus, our contribution is manifold.

\begin{itemize}
    \item We propose the use of the generative approach in data augmentation tasks.  
    \item We build a data augmentation pipeline based on synthetic images generated by a personalised text-to-image model for subject-driven generation.
    \item We compare the effectiveness of our approach with classical data augmentation techniques.
    \item We study the effect of the number of synthetic images in relation to the size of the original dataset.
    \item We study the feasibility of not using any real images in the training of a competitive computer vision model.
    \item We propose improving the results obtained by using conditional control to improve the quality of the images and the combination of the proposed subject-driven augmentation with classical techniques.
    \item We show that our approach applies to other tasks such as segmentation and other datasets like Food-101.
\end{itemize}

Our extensive experimental investigations demonstrate the \textbf{effectiveness of subject-driven augmentation as a competitive data augmentation technique}, particularly in datasets with a limited number of training images per class. Specifically, when employing a Resnet34 network for a classification task on the Oxford-IIIT Pet dataset with 5 real images per class, we observed performance improvements of up to 19.11\% for accuracy. This outcome is particularly noteworthy as conventional data augmentation techniques failed to enhance the baseline performance.

Additionally, we established that adding synthetic images to a small dataset yields significant benefits up to a certain threshold. By employing a Resnet34 network on the Oxford-IIIT Pet dataset with only 5 real images per class, we found that generating 100\% synthetic images improved the no-augmentation baseline performance by 18.93\%. However, increasing the proportion of synthetic images to 1000\% resulted in a marginal improvement of only 19.11\%.

Furthermore, we conducted experiments without real images, demonstrating that competitive results can be achieved by training a computer vision model solely on synthetic images. Additionally, we explored the incorporation of conditional control using ControlNet, further enhancing the outcomes. Remarkably, when utilising 5\% real images and 2000\% synthetic images in conjunction with the Oxford-IIIT Pet dataset and a Resnet34 network, we observed a substantial improvement of up to 23.47\% over the no-augmentation baseline performance.

Lastly, we showcased the versatility of this approach by successfully applying it to different tasks, such as segmentation and other datasets like Food-101.

Consequently, considering the research question we have posed, we draw the following implications. First, subject-driven augmentation techniques are a competitive approach. Second, these data augmentation techniques are especially useful in sparse datasets. And third, synthetic images are still not sufficiently faithful to reality and still have significant room for improvement. A detailed discussion of these implications can be found in section \ref{sec:discussion}.