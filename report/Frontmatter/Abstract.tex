\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

Recent advancements in text-to-image generation models have led to the development of mature systems capable of producing highly realistic and detailed images. These models have become increasingly accessible, empowering researchers to explore novel techniques for enhanced manipulation and control. Subject-driven generation emerges as a technique that allows the synthesis of new images featuring a subject within diverse contexts. Hence, to what extent can images generated by text-to-image systems improve the performance of computer vision models? This thesis addresses this question by developing an experimental framework to test the synthetic images generated by the Stable Diffusion model on classification and segmentation tasks. Concretely, we approach the issue through the lens of data augmentation. Our findings show that subject-driven augmentation is a competitive data augmentation technique, being especially relevant in small datasets where data is scarce or expensive. Nonetheless, we showcase that there is still a relevant gap between synthetic and real images. These findings underscore the potential for few-shot and zero-shot learning approaches, offering avenues to reduce or eliminate the costs associated with extensive dataset creation while augmenting the capabilities of computer vision models. Code released\footnote{\href{https://github.com/SrLozano/MSc-Thesis-Subject-Driven-Augmentation}{https://github.com/SrLozano/MSc-Thesis-Subject-Driven-Augmentation}}.

\vspace{15pt}

\textbf{Keywords}: deep learning, computer vision, text-to-image generation, data augmentation, subject-driven augmentation, Dreambooth, Textual inversion, ControlNet, Stable Diffusion, few-shot learning, zero-shot learning.